{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<CudaNdarrayType(float32, vector)>), HostFromGpu(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 1000 times took 0.266169 seconds\n",
      "Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323296]\n",
      "Used the gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780 (CNMeM is enabled)\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in xrange(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        #print 'u', shape\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "\n",
    "                         \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset=slice(None,40000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(\n",
    "    cifar10_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar10_train.num_examples, 100))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset=slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(\n",
    "    cifar10_validation, iteration_scheme=SequentialScheme(cifar10_validation.num_examples, 250))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(\n",
    "    cifar10_test, iteration_scheme=SequentialScheme(cifar10_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (100, 1, 28, 28) containing float32\n",
      " - an array of size (100, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (250, 1, 28, 28) containing float32\n",
      " - an array of size (250, 1) containing uint8\n",
      "CIFAR: \n",
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (250, 3, 32, 32) containing float32\n",
      " - an array of size (250, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"CIFAR: \"  \n",
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-6604dab46423>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-6604dab46423>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    iris_train_f = iris[:2*pop_num/3,1:]\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#print iris\n",
    "\n",
    "feats = 4\n",
    "alpha = 0.01\n",
    "pop_num = 150\n",
    "rng = np.random\n",
    "iris_f = iris['data'][:pop_num,:feats]\n",
    "iris_t = iris['target'][:pop_num]\n",
    "iris = hstack(([[x] for x in iris_t], iris_f))\n",
    "\n",
    "rng.shuffle(iris\n",
    "\n",
    "#print iris\n",
    "\n",
    "iris_train_f = iris[:2*pop_num/3,1:]\n",
    "iris_train_t = np.array(iris[:2*pop_num/3, 0], dtype='uint8')\n",
    "iris_test_f = iris[2*pop_num/3:,1:]\n",
    "iris_test_t = np.array(iris[2*pop_num/3:, 0], dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.printing as TP\n",
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor.signal.downsample as down\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, lamb = 0.1,rng=None, name=\"\"):\n",
    "        self.name = name\n",
    "        self.lamb = lamb\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "    def update(self, foo, alpha):\n",
    "        return []\n",
    "    def cost(self):\n",
    "        return 0;\n",
    "    def setInputDim(self, inputDim):\n",
    "        self.num_out = inputDim\n",
    "    def getOutputDim(self):\n",
    "        return self.num_out\n",
    "    def setMoments(self, moments):\n",
    "        self.moments = moments\n",
    "    def setLambda(self, lamb):\n",
    "        self.lamb = lamb\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_out, gamma  = 0.1, n = \"\", weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(name= n, **kwargs)\n",
    "        self.num_out = num_out\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(20. / (2* num_out))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        self.gamma= theano.shared(gamma)\n",
    "        self.b = theano.shared(bias_init.generate(self.rng, (num_out)), name=self.name +\" bias\")\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    @property\n",
    "    def parametersValues(self):\n",
    "        return [self.W.get_value(), self.b.get_value()]\n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def build(self, X):\n",
    "        #print self.name+ \" \",X.shape \n",
    "        return X.dot(self.W) + self.b\n",
    "    def cost(self):\n",
    "        return  (self.W ** 2).sum() * self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        gw, gb = T.grad(foo, self.parameters)\n",
    "        moments = self.moments\n",
    "        self.setMoments((gw, gb))\n",
    "        return  [(self.W, self.W - (alpha * gw + self.lamb * moments[0])), \n",
    "                 (self.b, self.b - (alpha * gb+ self.lamb * moments[1]))]\n",
    "    def setInputDim(self, inputDim):\n",
    "        shape = (inputDim, self.num_out)\n",
    "        print \"AffineLayer: \", shape\n",
    "        self.W = theano.shared(self.weight_init.generate(self.rng, shape),name=self.name +\" weight\")\n",
    "        self.setMoments(zeros(shape, dtype='float32'))\n",
    "\n",
    "class Affine2DLayer(Layer):\n",
    "    def __init__(self, num_out, gamma = None, n = \"\", weight_init=None, bias_init=None, **kwargs):\n",
    "        super(Affine2DLayer, self).__init__(name= n, **kwargs)\n",
    "        self.num_out = num_out\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(6. / 2* (num_out))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        if gamma is None:\n",
    "            self.gamma = theano.shared(0.1)\n",
    "        else:\n",
    "            self.gamma = theano.shared(gamma, name = self.name + \" gamma\")\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W]\n",
    "    @property\n",
    "    def parametersValues(self):\n",
    "        return [self.W.get_value()]\n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W']\n",
    "    \n",
    "    def build(self, X):\n",
    "        return X.dot(self.W)\n",
    "    def cost(self):\n",
    "        return  (self.W ** 2).sum() * self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        gw = T.grad(foo, self.parameters)\n",
    "        return  [(self.W, self.W -alpha * gw)] \n",
    "    def setInputDim(self, inputDim):\n",
    "        self.num_out = (self.num_out, inputDim[1], inputDim[2])\n",
    "        shape = inputDim +  self.num_out\n",
    "        print \"Affine2D\", shape\n",
    "        self.W = theano.shared(self.weight_init.generate(self.rng, shape),name=self.name +\" weight\")\n",
    "        self.setMoments(zeros(self.W.shape, dtype='float32'))\n",
    "    \n",
    "class LogRegLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(LogRegLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.nnet.sigmoid(X)\n",
    "\n",
    "\n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(TanhLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        print \"tanh layer\", X\n",
    "        return T.tanh(X)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(ReLULayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.maximum(0.0, X)\n",
    "\n",
    "class Conv(Layer):\n",
    "    def __init__(self, f_out, f_size, gamma = 0.1, n = \"\", weight_init = None, **kwargs):\n",
    "        super(Conv, self).__init__(name = n, **kwargs)\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(50. / (2*f_out+ f_size + f_size))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        self.gamma= theano.shared(gamma)\n",
    "        self.f_out = f_out\n",
    "        self.f_size = f_size\n",
    "    \n",
    "    \n",
    "    def setInputDim(self, inputDim):\n",
    "        F_size = (self.f_out, ) + (inputDim[0], self.f_size, self.f_size)                                   \n",
    "        self.num_out = (self.f_out, inputDim[1] - self.f_size + 1, inputDim[2] - self.f_size + 1)\n",
    "        print 'Conv filter', F_size\n",
    "        self.F = theano.shared(self.weight_init.generate(self.rng, F_size),name=self.name +\" filter\")\n",
    "        \n",
    "    def update(self, foo, alpha):\n",
    "        gf = T.grad(foo, self.F)\n",
    "        return  [(self.F, self.F - alpha * gf)]    \n",
    "    \n",
    "    #def cost(self):\n",
    "    #    return  (self.F ** 2).sum() * self.gamma\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.maximum(0.0, T.nnet.conv2d(X, self.F))\n",
    "        \n",
    "        \n",
    "        \n",
    "class Flatten(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(Flatten, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.flatten(X, 2)\n",
    "    def setInputDim(self, inputDim):\n",
    "        out_dim = 1\n",
    "        for i in inputDim:\n",
    "            out_dim = out_dim * i\n",
    "        self.num_out = out_dim\n",
    "    \n",
    "\n",
    "class BNLayer(Layer):\n",
    "    def __init__(self,num_out, gamma = 0.1, n = \"BNLayer\", alpha=1.0,**kwargs):\n",
    "        super(BNLayer, self).__init__(name = n, **kwargs)\n",
    "        self.num_out, self.alpha = num_out, alpha\n",
    "        self.gamma= theano.shared(gamma)\n",
    "    def build(self, X):\n",
    "        self.Gamma = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Gamma \" + self.name))\n",
    "        self.Beta  = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Beta \" + self.name))\n",
    "        self.Gamma.tag.initializer = Constant(1.0)\n",
    "        self.Beta.tag.initializer = Constant(0.0)\n",
    "    \n",
    "        #self.stored_means = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Means\" + self.name))\n",
    "        #self.stored_stds  = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Stds\" + self.name))\n",
    "        #self.stored_means.tag.initializer = Constant(0.0)\n",
    "        #self.stored_stds.tag.initializer = Constant(1.0)\n",
    "    \n",
    "        self.means = self.alpha * theano.tensor.mean(X, 0, keepdims=True)\n",
    "        self.stds = self.alpha * theano.tensor.std(X, 0, keepdims=True)\n",
    "        \n",
    "        #self.means = self.alpha *self.means + (1.0 - self.alpha) * self.stored_means.dimshuffle(0,'x')\n",
    "        #self.stds = self.alpha * self.stds + (1.0 - self.alpha) * self.stored_stds.dimshuffle(0,'x')\n",
    "        \n",
    "        normalized = theano.tensor.nnet.bn.batch_normalization(\n",
    "            X,\n",
    "            self.Gamma.dimshuffle('x',0),\n",
    "            self.Beta.dimshuffle('x',0),\n",
    "            self.means,\n",
    "            self.stds,\n",
    "            'high_mem'\n",
    "        )\n",
    "        return normalized\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.Gamma, self.Beta]\n",
    "    #def cost(self):\n",
    "    #    return  ((self.Gamma ** 2).sum() + (self.Gamma ** 2).sum())* self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        gg, gb = T.grad(foo, self.parameters)\n",
    "        return  [(self.Gamma, self.Gamma- alpha *gg),\n",
    "            (self.Beta, self.Beta - alpha * gb)] \n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.nnet.softmax(X)\n",
    "\n",
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, p_size, n = \"MP\", **kwargs):\n",
    "        super(MaxPoolLayer, self).__init__(name = n, **kwargs)\n",
    "        self.p_size = p_size\n",
    "    def build(self, input):\n",
    "        return down.max_pool_2d(input, (self.p_size,self.p_size), ignore_border=True)\n",
    "    def getOutputDim(self):\n",
    "        shape = (self.num_out[0], ) + (self.num_out[1]/self.p_size, self.num_out[2]/self.p_size) \n",
    "        print \"maxPool\", shape\n",
    "        return shape\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None, alpha=0.1, lamb = 0.1):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "        print type(alpha)\n",
    "        self.alpha = theano.shared(float32(alpha), name='alpha')\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def build(self, inputDim):\n",
    "        x = T.tensor4(\"x\")\n",
    "        y = T.vector(\"y\", dtype='int64')\n",
    "        cost = 0\n",
    "        paramUpdates = []\n",
    "        \n",
    "        X = x\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            #print inputDim\n",
    "            layer.setInputDim(inputDim)\n",
    "            layer.setLambda(lamb)\n",
    "            inputDim = layer.getOutputDim()\n",
    "            X = layer.build(X)\n",
    "            cost += layer.cost()\n",
    "        \n",
    "        pred = np.argmax(X, 1)\n",
    "        self.costFoo = T.nnet.categorical_crossentropy(X, y).mean() + cost\n",
    "        \n",
    "        #svgdotprint(self.costFoo)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            paramUpdates += layer.update(self.costFoo, self.alpha)\n",
    "        \n",
    "        paramUpdates += [(self.alpha, self.alpha * 0.99993)]\n",
    "        self.train = theano.function(inputs=[x,y], \n",
    "                                    outputs=[pred, self.costFoo, self.alpha],\n",
    "                                    updates=paramUpdates)\n",
    "        self.predict  = theano.function(inputs=[x], \n",
    "                                    outputs=pred)\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def trainFunction(self):\n",
    "        return self.train\n",
    "    \n",
    "    @property\n",
    "    def predictFunction(self):\n",
    "        return self.predict\n",
    "    @property\n",
    "    def costFunction(self):\n",
    "        return self.costFoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_er(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        predictions = net.predictFunction(X)\n",
    "        #print predictions != Y.ravel()\n",
    "        num_errs += (predictions != Y.ravel()).sum()\n",
    "        #print Y.shape[0], num_errs\n",
    "        num_examples += Y.shape[0]\n",
    "    return num_errs/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'float'>\n",
      "Conv filter (20, 3, 5, 5)\n",
      "maxPool (20, 14, 14)\n",
      "Conv filter (10, 20, 3, 3)\n",
      "AffineLayer:  (1440, 300)\n",
      "AffineLayer:  (300, 500)\n",
      "tanh layer Elemwise{add,no_inplace}.0\n",
      "AffineLayer:  (500, 800)\n",
      "tanh layer Elemwise{add,no_inplace}.0\n",
      "AffineLayer:  (800, 10)\n",
      "Start\n",
      "gamma:  0.001\n",
      "alpha:  0.01\n",
      "4.81284623146 0.89\n",
      "4.76773667145 0.81\n",
      "4.65217951012 0.78\n",
      "4.58657552338 0.73\n",
      "After epoch:  0 0.7431 time:  10.6701638699\n",
      "4.51370329666 0.74\n",
      "4.44244714832 0.7\n",
      "4.40765704346 0.69\n",
      "4.32471658897 0.64\n",
      "After epoch:  1 0.689 time:  10.4010398388\n",
      "4.34345390224 0.72\n",
      "4.28448804855 0.67\n",
      "4.21339969254 0.63\n",
      "4.05935362244 0.54\n",
      "After epoch:  2 0.6344 time:  10.7911539078\n",
      "4.09617424583 0.6\n",
      "4.27497760963 0.67\n",
      "4.22271362019 0.71\n",
      "4.10563140965 0.67\n",
      "After epoch:  3 0.6014 time:  10.525313139\n",
      "4.18260396576 0.68\n",
      "4.03715449524 0.58\n",
      "3.92911396503 0.59\n",
      "3.87746957874 0.55\n",
      "After epoch:  4 0.5819 time:  10.3746020794\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-224-69a92a9146fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcifar10_train_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_epoch_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mpr\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# + u.generate(numpy.random, X.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/temp/nn_assignments/libs/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "img_size = (32, 32)\n",
    "c1_i = 3\n",
    "c1_o = 20\n",
    "c1_f = 5\n",
    "p1   = 2\n",
    "c2_o = 10\n",
    "c2_f = 3\n",
    "hidden1 = 300\n",
    "hidden2 = 500\n",
    "hidden3 = 800\n",
    "hidden4 = 500\n",
    "hidden5 = 300\n",
    "outs = 10\n",
    "gamma = 0.001\n",
    "alpha = 0.01\n",
    "lamb = 0.1\n",
    "num_epochs  = 300\n",
    "\n",
    "net = FeedForwardNet([Conv(c1_o, c1_f, \"Conv1\"),\n",
    "                      MaxPoolLayer(p1, \"P\"),\n",
    "                      Conv(c2_o, c2_f, \"Conv1\"),\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden1, gamma, \"tA1\"), \n",
    "                      BNLayer(hidden1, 'BN1'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden2, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden3, gamma, \"tA\"),\n",
    "                      #BNLayer(hidden4, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a, = net.trainFunction(X , Y.ravel()) # + u.generate(numpy.random, X.shape)\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def xent(x , y):\n",
    "        return  -y * T.log(x) - (1-y) * T.log(1-x)\n",
    "    \n",
    "    def costF(x, w):\n",
    "        return x.mean() + 0.01 * (w ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Conv' object has no attribute 'F'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-7169afa264fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-f4364401003f>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Conv' object has no attribute 'F'"
     ]
    }
   ],
   "source": [
    "conv = Conv(3, 32, 3)\n",
    "flatten = Flatten()\n",
    "x = T.tensor4(\"x\")\n",
    "y = T.vector(\"y\", dtype='int64')\n",
    "\n",
    "C = conv.build(x)\n",
    "F = flatten.build(C)\n",
    "\n",
    "foo = theano.function(inputs=[x], \n",
    "                    outputs=[C, F])\n",
    "\n",
    "for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "    print X.shape\n",
    "    c,f = foo(X)\n",
    "    print c.shape\n",
    "    print f.shape\n",
    "    break\n",
    "\n",
    "conv = Conv(1, 1)\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\", dtype='int64')\n",
    "\n",
    "x = conv.build(x)\n",
    "for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "    X = X.T\n",
    "    Y = X.copy()\n",
    "    print X.shape\n",
    "    zeros(X.shape)\n",
    "    conv.resize(X)\n",
    "    print X.shape\n",
    "    print Y.shape\n",
    "    conv.reresize(X)\n",
    "    print X.shape\n",
    "    print X == Y\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fA  Shape.0\n",
      "sA  Shape.0\n",
      "tA  Shape.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x must be 1-d or 2-d tensor of floats. Got TensorType(float32, 4D)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-351-69855bf18bf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m                       \u001b[0mAffineLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tA\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                       SoftMaxLayer(\"fSoftMax\")], alpha)\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Start\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-348-89d17fe8b490>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[0mcost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-348-89d17fe8b490>\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mFeedForwardNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/temp/nn_assignments/libs/Theano/theano/tensor/nnet/nnet.pyc\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(c)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msoftmax_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/temp/nn_assignments/libs/Theano/theano/gof/op.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \"\"\"\n\u001b[0;32m    599\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'return_list'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_test_value\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'off'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/temp/nn_assignments/libs/Theano/theano/tensor/nnet/nnet.pyc\u001b[0m in \u001b[0;36mmake_node\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat_dtypes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             raise ValueError('x must be 1-d or 2-d tensor of floats. Got %s' %\n\u001b[1;32m--> 431\u001b[1;33m                              x.type)\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape_padleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_ones\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x must be 1-d or 2-d tensor of floats. Got TensorType(float32, 4D)"
     ]
    }
   ],
   "source": [
    "feats = 784\n",
    "hidden1 = 500\n",
    "hidden2 = 200\n",
    "outs = 10\n",
    "gamma = 0.001\n",
    "alpha = 0.1\n",
    "num_epochs  = 100\n",
    "\n",
    "net = FeedForwardNet([AffineLayer(feats, hidden1, gamma, \"fA\"), \n",
    "                      TanhLayer(\"fTanh\"),\n",
    "                      AffineLayer(hidden1, hidden2, gamma, \"sA\"), \n",
    "                      TanhLayer(\"fTanh\"),\n",
    "                      AffineLayer(hidden2, outs, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha)\n",
    "net.build()\n",
    "print \"Start\"\n",
    "i = 0\n",
    "e = 0\n",
    "while e < num_epochs:\n",
    "    for X, Y in mnist_train_stream.get_epoch_iterator():\n",
    "        pr ,c = net.trainFunction(X.T, Y.ravel())\n",
    "        i+=1\n",
    "        #if i % 100 == 0:\n",
    "            #print c, (pr  == Y).mean()\n",
    "    \n",
    "    print \"After epoch: \", e, compute_er(net, mnist_validation_stream)\n",
    "    e+=1\n",
    "\n",
    "    \n",
    "for X, Y in mnist_validation_stream.get_epoch_iterator():\n",
    "    predictions = net.predictFunction(X.T)\n",
    "    num_errs += (predictions != Y).sum()\n",
    "    num_examples += X.shape[1]\n",
    "    k+=1\n",
    "print num_errs, num_examples, k, num_errs/num_examples\n",
    "#print (iris_test_t  == net.predictFunction(iris_test_f)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.printing as TP\n",
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))\n",
    "\n",
    "\n",
    "\n",
    "feats = 4\n",
    "hidden = 500\n",
    "outs = 3\n",
    "gamma = 0.001\n",
    "alpha = 0.1\n",
    "\n",
    "net = FeedForwardNet([AffineLayer(feats, hidden, gamma, \"fA\"), \n",
    "          TanhLayer(\"fTanh\"),\n",
    "          AffineLayer(hidden, outs, gamma, \"sA\"), \n",
    "          SoftMaxLayer(\"fSoftMax\")], alpha)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#theano.printing.pydotprint(costFoo, outfile=\"symbolic_graph_unopt.png\", var_with_name_simple=True)  \n",
    "\n",
    "\n",
    "net.build()\n",
    "\n",
    "print iris_train_t\n",
    "for i in range(10000):\n",
    "    pr ,c = net.trainFunction(iris_train_f, iris_train_t)\n",
    "    if i % 100 == 0:\n",
    "        print c, (pr  == iris_train_t).mean()\n",
    "\n",
    "print (pr  == iris_train_t).mean()\n",
    "print pr\n",
    "\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=iris_test_t.ravel(), cmap='prism')\n",
    "subplot(2,1,2)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=net.predicFunction(iris_test_f), cmap='prism')\n",
    "print (iris_test_t  == net.predictFunction(iris_test_f)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print iris_test_t\n",
    "\n",
    "w = theano.shared(rng.randn(feats)*0.01, name=\"w\")\n",
    "#print w.get_value()\n",
    "b = theano.shared(0., name=\"b\")\n",
    "#print b.get_value()\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "\n",
    "p_1 = T.nnet.sigmoid(T.dot(x, w) + b)   # Probability that target = 1\n",
    "prediction = p_1 > 0.5                    # The prediction thresholded\n",
    "c = costF(xent(p_1, y), w)# The cost to minimize\n",
    "gw, gb = T.grad(c, [w, b])             # Compute the gradient of the cost\n",
    "                                          # (we shall return to this in a\n",
    "                                          # following section of this tutorial)\n",
    "\n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, c],\n",
    "          updates=((w, w - alpha * gw), (b, b - alpha * gb)))\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    p, c =train(iris_train_f, iris_train_t)\n",
    "    #print p, c, x.mean()\n",
    "    \n",
    "print (predict(iris_test_f) == iris_test_t).mean()\n",
    "\n",
    "\n",
    "#foo = theano.function(inputs=[iris_train_f], outputs=[f])\n",
    "\n",
    "#print iris_test_t\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=iris_test_t.ravel(), cmap='spring')\n",
    "subplot(2,1,2)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=predict(iris_test_f).ravel(), cmap='spring')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.printing as TP\n",
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))\n",
    "\n",
    "\n",
    "\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "feats = 4\n",
    "hidden = 500\n",
    "outs = 1\n",
    "gamma = 0.1\n",
    "alpha = 0.1\n",
    "\n",
    "fAL = AffineLayer(feats, hidden, gamma, \"fA\")\n",
    "tL = TanhLayer()\n",
    "sAL = AffineLayer(hidden, outs, gamma, \"sA\")\n",
    "lL = LogRegLayer()\n",
    "\n",
    "fa = fAL.build(x)\n",
    "t = tL.build(fa)\n",
    "sa = sAL.build(t)\n",
    "out = lL.build(sa)\n",
    "pred = out > 0.5\n",
    "c = xent(out.ravel(), y).mean() + fAL.cost() + sAL.cost()\n",
    "\n",
    "theano.printing.pydotprint(out, outfile=\"symbolic_graph_unopt.png\", var_with_name_simple=True)  \n",
    "fgw, fgb = T.grad(c, fAL.parameters)\n",
    "sgw, sgb = T.grad(c, sAL.parameters)\n",
    "\n",
    "train = theano.function(inputs=[x,y], \n",
    "                        outputs=[pred, c], \n",
    "                        updates=(fAL.update(c, alpha) + sAL.update(c, alpha)))\n",
    "predict  = theano.function(inputs=[x], \n",
    "                        outputs=[pred])\n",
    "\n",
    "for i in range(100):\n",
    "    pr, cost = train(iris_train_f, iris_train_t)\n",
    "\n",
    "print (pr.ravel() == iris_train_t).mean()\n",
    "\n",
    "\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=iris_test_t.ravel(), cmap='spring')\n",
    "subplot(2,1,2)\n",
    "scatter(iris_test_f[:,0], iris_test_f[:,1], c=predict(iris_test_f), cmap='spring')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = np.array([[12, 13], [1, 3]])\n",
    "y1 = [1, 2]\n",
    "print x.shape\n",
    "y = T.vector()\n",
    "x = T.matrix()\n",
    "f = theano.function(inputs=[x, y], outputs=x+y)\n",
    "f(x1, y1)\n",
    "\n",
    "\n",
    "x2 = [1,2]\n",
    "y2 = [2,3]\n",
    "x2 + y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = (32, 32)\n",
    "c1_i = 3\n",
    "c1_o = 50\n",
    "c1_f = 5\n",
    "p1   = 2\n",
    "c2_o = 10\n",
    "c2_f = 3\n",
    "hidden1 = 300\n",
    "hidden2 = 500\n",
    "hidden3 = 50\n",
    "hidden4 = 50\n",
    "hidden5 = 300\n",
    "outs = 10\n",
    "gamma = 0.001\n",
    "alpha = 0.01\n",
    "lamb = 0.1\n",
    "num_epochs  = 300\n",
    "\n",
    "net = FeedForwardNet([\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden3, gamma, \"tA1\"), \n",
    "                      BNLayer(hidden3, 'BN1'),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(hidden4, gamma, \"tA\"),\n",
    "                      BNLayer(hidden4, 'BN2'),\n",
    "                      TanhLayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")], alpha, lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "print \"alpha: \", alpha\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        pr ,c,a, gg1, gb1, gamma1, beta1, mean1, std1, gg2, gb2, gamma2, beta2, mean2, std2 = net.trainFunction(X , Y.ravel()) # + u.generate(numpy.random, X.shape)\n",
    "        i+=1\n",
    "        if i % 1 == 0:\n",
    "            print 'L1gg', gg1.shape, np.array(gg1)\n",
    "            print 'L1gb', gb1.shape, np.array(gb1)\n",
    "            print 'L1gamma', gamma1.shape, np.array(gamma1)\n",
    "            print 'L1beta', beta1.shape, np.array(beta1)\n",
    "            print 'L1mean', mean1.shape, np.array(mean1)\n",
    "            print 'L1std', std1.shape, np.array(std1)\n",
    "            print 'L2gg', gg2.shape, np.array(gg2)\n",
    "            print 'L2gb', gb2.shape, np.array(gb2)\n",
    "            print 'L2gamma', gamma2.shape, np.array(gamma2)\n",
    "            print 'L2beta', beta2.shape, np.array(beta2)\n",
    "            print 'L2mean', mean2.shape, np.array(mean2)\n",
    "            print 'L2std', std2.shape, np.array(std2)\n",
    "            print c, (pr  != Y.ravel()).mean()\n",
    "        if i % 3 == 0:\n",
    "            break\n",
    "    break\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano.tensor.signal.downsample as down\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, lamb = 0.1,rng=None, name=\"\"):\n",
    "        self.name = name\n",
    "        self.lamb = lamb\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "    def update(self, foo, alpha):\n",
    "        return []\n",
    "    def cost(self):\n",
    "        return 0;\n",
    "    def setInputDim(self, inputDim):\n",
    "        self.num_out = inputDim\n",
    "    def getOutputDim(self):\n",
    "        return self.num_out\n",
    "    def setMoments(self, moments):\n",
    "        self.moments = moments\n",
    "    def setLambda(self, lamb):\n",
    "        self.lamb = lamb\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_out, gamma  = 0.1, n = \"\", weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(name= n, **kwargs)\n",
    "        self.num_out = num_out\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(20. / (2* num_out))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        self.gamma= theano.shared(gamma)\n",
    "        self.b = theano.shared(bias_init.generate(self.rng, (num_out)), name=self.name +\" bias\")\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    @property\n",
    "    def parametersValues(self):\n",
    "        return [self.W.get_value(), self.b.get_value()]\n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def build(self, X):\n",
    "        #print self.name+ \" \",X.shape \n",
    "        return X.dot(self.W) + self.b\n",
    "    def cost(self):\n",
    "        return  (self.W ** 2).sum() * self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        gw, gb = T.grad(foo, self.parameters)\n",
    "        moments = self.moments\n",
    "        self.setMoments((gw, gb))\n",
    "        return  [(self.W, self.W - (alpha * gw + self.lamb * moments[0])), \n",
    "                 (self.b, self.b - (alpha * gb+ self.lamb * moments[1]))]\n",
    "    def setInputDim(self, inputDim):\n",
    "        shape = (inputDim, self.num_out)\n",
    "        print \"AffineLayer: \", shape\n",
    "        self.W = theano.shared(self.weight_init.generate(self.rng, shape),name=self.name +\" weight\")\n",
    "        self.setMoments(zeros(shape, dtype='float32'))\n",
    "\n",
    "class Affine2DLayer(Layer):\n",
    "    def __init__(self, num_out, gamma = None, n = \"\", weight_init=None, bias_init=None, **kwargs):\n",
    "        super(Affine2DLayer, self).__init__(name= n, **kwargs)\n",
    "        self.num_out = num_out\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(6. / 2* (num_out))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        if gamma is None:\n",
    "            self.gamma = theano.shared(0.1)\n",
    "        else:\n",
    "            self.gamma = theano.shared(gamma, name = self.name + \" gamma\")\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W]\n",
    "    @property\n",
    "    def parametersValues(self):\n",
    "        return [self.W.get_value()]\n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W']\n",
    "    \n",
    "    def build(self, X):\n",
    "        return X.dot(self.W)\n",
    "    def cost(self):\n",
    "        return  (self.W ** 2).sum() * self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        gw = T.grad(foo, self.parameters)\n",
    "        return  [(self.W, self.W -alpha * gw)] \n",
    "    def setInputDim(self, inputDim):\n",
    "        self.num_out = (self.num_out, inputDim[1], inputDim[2])\n",
    "        shape = inputDim +  self.num_out\n",
    "        print \"Affine2D\", shape\n",
    "        self.W = theano.shared(self.weight_init.generate(self.rng, shape),name=self.name +\" weight\")\n",
    "        self.setMoments(zeros(self.W.shape, dtype='float32'))\n",
    "    \n",
    "class LogRegLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(LogRegLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.nnet.sigmoid(X)\n",
    "\n",
    "\n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(TanhLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        print \"tanh layer\", X\n",
    "        return T.tanh(X)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(ReLULayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.maximum(0.0, X)\n",
    "\n",
    "class Conv(Layer):\n",
    "    def __init__(self, f_out, f_size, gamma = 0.1, n = \"\", weight_init = None, **kwargs):\n",
    "        super(Conv, self).__init__(name = n, **kwargs)\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(50. / (2*f_out+ f_size + f_size))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        self.gamma= theano.shared(gamma)\n",
    "        self.f_out = f_out\n",
    "        self.f_size = f_size\n",
    "    \n",
    "    \n",
    "    def setInputDim(self, inputDim):\n",
    "        F_size = (self.f_out, ) + (inputDim[0], self.f_size, self.f_size)                                   \n",
    "        self.num_out = (self.f_out, inputDim[1] - self.f_size + 1, inputDim[2] - self.f_size + 1)\n",
    "        print 'Conv filter', F_size\n",
    "        self.F = theano.shared(self.weight_init.generate(self.rng, F_size),name=self.name +\" filter\")\n",
    "        \n",
    "    def update(self, foo, alpha):\n",
    "        gf = T.grad(foo, self.F)\n",
    "        return  [(self.F, self.F - alpha * gf)]    \n",
    "    \n",
    "    def cost(self):\n",
    "        return  (self.F ** 2).sum() * self.gamma\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.maximum(0.0, T.nnet.conv2d(X, self.F))\n",
    "        \n",
    "        \n",
    "        \n",
    "class Flatten(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(Flatten, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.flatten(X, 2)\n",
    "    def setInputDim(self, inputDim):\n",
    "        out_dim = 1\n",
    "        for i in inputDim:\n",
    "            out_dim = out_dim * i\n",
    "        self.num_out = out_dim\n",
    "    \n",
    "\n",
    "class BNLayer(Layer):\n",
    "    def __init__(self,num_out, n = \"BNLayer\", gamma = 0.1, alpha=1.0,**kwargs):\n",
    "        super(BNLayer, self).__init__(name = n, **kwargs)\n",
    "        self.num_out, self.alpha = num_out, alpha\n",
    "        self.gamma= theano.shared(gamma)\n",
    "    def build(self, X):\n",
    "        self.Gamma = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Gamma \" + self.name))\n",
    "        print 'Gamma shape:', np.zeros((1, self.num_out)).shape\n",
    "        self.Beta  = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Beta \" + self.name))\n",
    "        print 'Beta shape:', np.zeros((1, self.num_out)).shape\n",
    "        self.Gamma.tag.initializer = Constant(1.0)\n",
    "        self.Beta.tag.initializer = Constant(0.0)\n",
    "    \n",
    "        #self.stored_means = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Means\" + self.name))\n",
    "        #self.stored_stds  = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Stds\" + self.name))\n",
    "        #self.stored_means.tag.initializer = Constant(0.0)\n",
    "        #self.stored_stds.tag.initializer = Constant(1.0)\n",
    "    \n",
    "        self.means = self.alpha * theano.tensor.mean(X, 0, keepdims=True)\n",
    "        self.stds = self.alpha * theano.tensor.std(X, 0, keepdims=True)\n",
    "        self.means.tag.initializer = Constant(0.0)\n",
    "        self.stds.tag.initializer = Constant(1.0)\n",
    "        #self.means = self.alpha *self.means + (1.0 - self.alpha) * self.stored_means.dimshuffle(0,'x')\n",
    "        #self.stds = self.alpha * self.stds + (1.0 - self.alpha) * self.stored_stds.dimshuffle(0,'x')\n",
    "        \n",
    "        normalized = theano.tensor.nnet.bn.batch_normalization(\n",
    "            X,\n",
    "            self.Gamma,\n",
    "            self.Beta,\n",
    "            self.means,\n",
    "            self.stds,\n",
    "            'high_mem'\n",
    "        )\n",
    "        return normalized\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.Gamma, self.Beta]\n",
    "    @property\n",
    "    def check(self):\n",
    "        return [self.gg, self.gb, self.Gamma, self.Beta,self.means, self.stds ]\n",
    "    #def cost(self):\n",
    "    #    return  ((self.Gamma ** 2).sum() + (self.Gamma ** 2).sum())* self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        self.gg, self.gb = T.grad(foo, self.parameters)\n",
    "        return  [(self.Gamma, self.Gamma- alpha *self.gg),\n",
    "            (self.Beta, self.Beta - alpha * self.gb)] \n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.nnet.softmax(X)\n",
    "\n",
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, p_size):\n",
    "        self.p_size = p_size\n",
    "    def build(self, input):\n",
    "        return down.max_pool_2d(input, (self.p_size,self.p_size), ignore_border=True)\n",
    "    def getOutputDim(self):\n",
    "        shape = (self.num_out[0], ) + (self.num_out[1]/self.p_size, self.num_out[2]/self.p_size) \n",
    "        print \"maxPool\", shape\n",
    "        return shape\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None, alpha=0.1, lamb = 0.1):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "        print type(alpha)\n",
    "        self.alpha = theano.shared(float32(alpha), name='alpha')\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def build(self, inputDim):\n",
    "        x = T.tensor4(\"x\")\n",
    "        y = T.vector(\"y\", dtype='int64')\n",
    "        cost = 0\n",
    "        paramUpdates = []\n",
    "        \n",
    "        X = x\n",
    "        o1 = []\n",
    "        o2 = []\n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            #print inputDim\n",
    "            layer.setInputDim(inputDim)\n",
    "            layer.setLambda(lamb)\n",
    "            inputDim = layer.getOutputDim()\n",
    "            X = layer.build(X)\n",
    "            print 'name', layer.name\n",
    "            if i == 2:\n",
    "                print 'BNname', layer.name\n",
    "                o1 = layer\n",
    "            if i == 5:\n",
    "                print 'BNname', layer.name\n",
    "                o2 = layer\n",
    "\n",
    "            cost += layer.cost()\n",
    "        \n",
    "        pred = np.argmax(X, 1)\n",
    "        self.costFoo = T.nnet.categorical_crossentropy(X, y).mean() + cost\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            print layer.name\n",
    "            paramUpdates += layer.update(self.costFoo, self.alpha)\n",
    "            \n",
    "        o1 = o1.check\n",
    "        o2 = o2.check\n",
    "        \n",
    "        paramUpdates += [(self.alpha, self.alpha * 0.99993)]\n",
    "        self.train = theano.function(inputs=[x,y], \n",
    "                                    outputs=[pred, self.costFoo, self.alpha]+o1+o2,\n",
    "                                    updates=paramUpdates)\n",
    "        self.predict  = theano.function(inputs=[x], \n",
    "                                    outputs=pred)\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def trainFunction(self):\n",
    "        return self.train\n",
    "    \n",
    "    @property\n",
    "    def predictFunction(self):\n",
    "        return self.predict\n",
    "    @property\n",
    "    def costFunction(self):\n",
    "        return self.costFoo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
