{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Up to now we used neural networks to learn functions that predicted a desired output $y$ based on an input vector $x$. We have assumed that $x$ has a constant size, such as an image of a given resolution.\n",
    "\n",
    "In contrast, recurrent networks can be applied to sequences of different length.\n",
    "\n",
    "Consider the problem of computing the parity checksum of a sequence of bits. Traditional neural networks that we know can not be applied to bit sequences of arbitrary length, yet a program which reads in the sequence bit-by-bit is very easy to write. \n",
    "\n",
    "Another motivating example is sequence generation, such as generating text or music. Again, normal neural networks that take a fixed input size are not directly applicable to whole sequences.\n",
    "\n",
    "## Autoregressive models\n",
    "\n",
    "An autoregressive model assumes, that the $i$-th element of a sequence depends only on a few preceding sequence elements. Lets first write the exact probability of observing a sequence $\\mathbf{Y} = [Y_1, Y_2, \\ldots, Y_m]$:\n",
    "\n",
    "$$\n",
    "    P(\\mathbf{Y}) = P(Y_1)P(Y_2|Y_1)P(Y_3|Y_1, Y_2)\\ldots P(Y_m|Y1,Y_2,\\ldots,Y_{n-1})=\\prod_i P(Y_i|Y_1,\\ldots, Y_{i-1})\n",
    "$$\n",
    "\n",
    "The autoregressive model simply assumes that we can use only a finite history, that is:\n",
    "$$\n",
    "    P(Y_i|Y_1,\\ldots, Y_{i-1}) \\approx P(Y_i|Y_{i-n},\\ldots, Y_{i-1})\n",
    "$$\n",
    "\n",
    "Autoregressive models are quite frequently used, because they are easy to implement - it is sufficient extract $n$ element long subsequences and learn a model that predicts the last element given the $n-1$ preceding ones. Thus autoregressive models reduce the sequence learning problem to the typicall supervised learning setup\n",
    "\n",
    "### Examples\n",
    "\n",
    "Autoregressive models are very popular, for instance:\n",
    "\n",
    "1. Language models (that is models which tell how probable is a given utterance) are often expressed as $n$-gram models in which $P(Y_i|Y_{i-n},\\ldots, Y_{i-1})$ is simply established by counting occurrences in a corpus of text.\n",
    "2. The ARMA model used in timeseries prediction is expressed as:\n",
    "    $$\n",
    "        Y_t = \\sum_{i=1}^{n}\\alpha_i Y_{t-i} + \\sum_{i=1}^k \\Theta_i \\epsilon_{t-i},\n",
    "    $$\n",
    "    where $\\epsilon_i$ are assumed to be normally distributed noise variables.\n",
    "    \n",
    "## Models with a hidden state\n",
    "\n",
    "Autoregressive models have a very short memory which limits their applicability. We can build a more powerfull model by introducing a sequence of hidden states $\\mathbf{H} = [H_0, H_1, \\ldots]$. We will assume that all history of a sequence can be captured by the state:\n",
    "\n",
    "$$\n",
    "    P(Y_i|Y_1,\\ldots, Y_{i-1}) \\approx P(Y_i|Y_{i-n}, H_{i-1})\n",
    "$$\n",
    "\n",
    "\n",
    "### Recurrent neural networks\n",
    "\n",
    "Recurrent neural networks are a generalization of the model with the hidden state. We will assume that there is an input sequence $\\mathbb{X}$. The network will process the elements of $\\mathbb{X}$  one at a time producing a sequence of hidden states and a sequence of outputs. We will train the model by specifying the desired outputs. We will be able to supervise the model at each step (which is common e.g. in sequence generators which are taught to predict the next sequence element) or only at the end (which can be used in the parity computing network).\n",
    "\n",
    "Define a recurrent computation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H_t &= f(X_t, H_{t-1}) \\\\\n",
    "    Y_t &= g(X_t, H_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In a recurrent neural network the function $f$ and $g$ are implemented as multilayer neural networks.\n",
    "\n",
    "### Backpropagation through time\n",
    "\n",
    "The last question is how to train such a recurrent network? The typicall approach is to *unroll the network in time*, then compute the loss and backpropagate it over the time steps. Gradient backpropagation algorithm works, thus in principle training doesn't require new skill. Hovewer, the unrolled network is very deep (its depth equals to the number of time steps!) and quite pathological, because the same weight matrices are reused at all times. For this reasons recurrent networks suffer from two problems:\n",
    "\n",
    "1. Gradient vanishing, when the gradient drops to zero and no training is possible.\n",
    "2. Gradient explosion, when the gradient grows really quickly and a single step can destroy the network.\n",
    "\n",
    "The two problems are related. Consider the following recurrency:\n",
    "\n",
    "$$\n",
    "    H_t = W_{hh}H_{t-1} = (W_{hh})^t H_0\n",
    "$$\n",
    "\n",
    "Suppose $H_0$ is an eigenvector - then depending on the magnitude of the associated eigenvalue the hidden state will either exponentially grow or decay! Of course, the network will typically have sone nonlinearities that will prevent the explosion of the hiden state. However, the backpropagation computation is linear (because all nonlinearities are linerized at the operating point). Thus the gradient is very prone to explode or implode!\n",
    "\n",
    "### Solutions to gradient pathologies\n",
    "\n",
    "#### Echo-state networks\n",
    "The first solution to training recurrent networks is... not to train the recurrent connections! This approach is taken in the *echo state networks* which perform the following computation:\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "        H_t &= \\tanh(W_{xh}X_t + W_{hh} H_{t-1} + b_h) \\\\\n",
    "        O_t &= W_{xo}X_t + W_{ho}H_T + b_o\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "The training procedure is as follows:\n",
    "\n",
    "1. Randomly sample $W_{hh}$ and $W_{xh}$.\n",
    "2. Rescale $W_{hh}$ to have the largest eigenvalue close to 1\n",
    "3. Fit the $W_{xo}$ and $W{ho}$ using the closed-form formula for least squares\n",
    "4. Since steps 1.-3. are super-fast, repeat them multiple times with different scaling of $W_{hh}$ and $W_{xh}$.\n",
    "\n",
    "Because $W_{hh}$ has the largest eigenvalue sligtly less than 1, the hidden states oscillate and decay slowly. This creates \"echoes\" of previous inputs that reverberate in the network.\n",
    "\n",
    "#### Gradient clipping\n",
    "\n",
    "Gradient explosion can be prevented by rescaling gradients that are larger than a specified threshold. You can either clip individual components of the gradient, or rescale the whole gradient.\n",
    "\n",
    "#### LSTM cells\n",
    "\n",
    "LSTM's are important! They contributed a lot to recent sucesses of recurrent neural networks!\n",
    "\n",
    "For introduction and intuitions please see http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "For more in-depth analysis please look at: http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf and http://arxiv.org/pdf/1503.04069v1.pdf\n",
    "\n",
    "The core idea of LSTM is to introduce multiplicative *gates* that enable long pathways of nearly constant values/gradients of memory cells that we will denote $c$. The core equations of LSTM are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    i_t &= \\sigma(W_{xi}X_t + W_{hi}H_{t-1} + W_{ci}c_{t-1} + b_i) \\\\\n",
    "    f_t &= \\sigma(W_{xf}X_t + W_{hf}H_{t-1} + W_{cf}c_{t-1} + b_f) \\\\\n",
    "    c_t &= f_t c_{t-1} + i_t\\tanh(W_{xc}X_t + W_{hc}H_{t-1} + b_i) \\\\\n",
    "    o_t &= \\sigma(W_{xo}X_t + W_{ho}H_{t-1} + W_{co}c_{t} + b_o) \\\\\n",
    "    H_t &= o_t \\tanh(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Multilayer and bi-directional LSTM networks\n",
    "\n",
    "It is possible to stack several LSTM layers (see the code below). Likewise, it is common to invert the input sequence to run the LSTM backward in time. Then the forward and backward hidden states give a summary of the sequence around a certain element.\n",
    "\n",
    "### Tricks of the trade\n",
    "\n",
    "1. Use train rules that allow per-parameter learnign rates (e.g. RMSProp)\n",
    "2. Monitor gradient magnitude!\n",
    "3. Initialization is important:\n",
    "    - it often helps to orthogonalize recurent weights and rescale to have the largest eigenvalue close to 1. This is similar to hidden-to-hidden weights in echo state networks\n",
    "    - forget-gate biases in LSTMs are ofetn initialized to 1 instead of 0. This enhances information retention at the beginning of training\n",
    "4. Learning interdependencies across many time steps is difficult. If possible train on short (or othervise simple) sequences first (this is often called \"curriculum learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug = True # global var to control debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "\n",
    "import theano.tensor as TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def default(var, val):\n",
    "    if var is None:\n",
    "        return val\n",
    "    else:\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN implementation in Theano\n",
    "\n",
    "Theano fully supports recurrent neural networks. One typically needs only to provide an implementation of a single step of the recurrency.\n",
    "\n",
    "Please read about the scan function which is used to implement the loops: http://deeplearning.net/software/theano/library/scan.html.\n",
    "\n",
    "**Attention**: through the code we will assume that the 0-th axis refers to time and that the 1-st axis refers to individual examples inside a minibatch. (This way in a C-major memory layout individual time steps occupy contiguous regions in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "        self._parameters = []\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self._parameters\n",
    "    \n",
    "    def add_param(self, name, shape, initializer, dtype='float32'):\n",
    "        param = theano.shared(numpy.zeros(\n",
    "            shape, dtype=dtype), name=name)\n",
    "        param.tag.initializer = initializer\n",
    "        self._parameters.append(param)\n",
    "        setattr(self, name, param)\n",
    "        \n",
    "    def initialize(self):\n",
    "        for p in self.parameters:\n",
    "            p.set_value(p.tag.initializer.generate(self.rng, \n",
    "                                                   p.get_value().shape))\n",
    "\n",
    "\n",
    "class RecurrentLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RecurrentLayer, self).__init__(**kwargs)\n",
    "        self.initial_states = []\n",
    "    \n",
    "    def apply(self, X, **kwargs):\n",
    "        batch_size = X.shape[1]\n",
    "        outputs_info = []\n",
    "        for h in self.initial_states:\n",
    "            h0 = TT.repeat(h, batch_size, axis=0)\n",
    "            outputs_info.append(dict(initial=h0))\n",
    "        \n",
    "        #\n",
    "        # Scan in theano takes a function which performs a single step of the\n",
    "        # recurrent computation. Subclasses just need to provide the\n",
    "        # self.transition function.\n",
    "        #\n",
    "        scan_result, scan_updates = theano.scan(\n",
    "            self.transition,\n",
    "            sequences=X,\n",
    "            outputs_info=outputs_info,\n",
    "            **kwargs\n",
    "            )\n",
    "        # Note: this in general will not be the case and we will need to\n",
    "        # make sure that the updates are given to theano.function\n",
    "        assert not scan_updates\n",
    "        return scan_result\n",
    "\n",
    "\n",
    "class MergeInputHiddens(Layer):\n",
    "    \"\"\"\n",
    "    Merge two sequences - inputs and hidden states and produce an output.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim,\n",
    "                 weight_init=None, bias_init=None, \n",
    "                 **kwargs):\n",
    "        super(MergeInputHiddens, self).__init__(**kwargs)\n",
    "        weight_init = default(\n",
    "            weight_init, IsotropicGaussian(1.0/sqrt(in_dim)))\n",
    "        bias_init = default(\n",
    "            weight_init, Constant(0.))\n",
    "        \n",
    "        # Input to output\n",
    "        self.add_param('Wxo', (in_dim, out_dim), \n",
    "                       weight_init)\n",
    "        \n",
    "        # Hidden to output\n",
    "        self.add_param('Who', (hidden_dim, out_dim), \n",
    "                       weight_init)\n",
    "        \n",
    "        # Output bias\n",
    "        self.add_param('Bo', (out_dim,), \n",
    "                       bias_init)\n",
    "        \n",
    "    def apply(self, X, H):\n",
    "        # Get the shape\n",
    "        nsteps, bs, nin = X.shape\n",
    "        nhid = H.shape[2]\n",
    "        \n",
    "        # Note - we flatten the steps and batch size\n",
    "        # as the computation of outputs can be performed in \n",
    "        # parallel for all time steps.\n",
    "        \n",
    "        O = (X.reshape((nsteps*bs, nin)).dot(self.Wxo) + \n",
    "             H.reshape((nsteps*bs, nhid)).dot(self.Who) +\n",
    "             self.Bo)\n",
    "        return O.reshape((nsteps, bs, O.shape[1]))\n",
    "    \n",
    "class Chain(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Chain, self).__init__(**kwargs)\n",
    "        self.children = []\n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        ret = list(self._parameters)\n",
    "        for c in self.children:\n",
    "            ret.extend(c.parameters)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(RecurrentLayer):\n",
    "    \"\"\"\n",
    "    The simplest recurrent transition - affine transformation and nonlinearity!\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 hidden_activation=TT.tanh,\n",
    "                 rec_weight_init=None,\n",
    "                 weight_init=None, bias_init=None, **kwargs):\n",
    "        super(SimpleRNN, self).__init__(**kwargs)\n",
    "        rec_weight_init = default(\n",
    "            rec_weight_init, IsotropicGaussian(1.0/sqrt(hidden_dim)))\n",
    "        weight_init = default(\n",
    "            weight_init, IsotropicGaussian(1.0/sqrt(in_dim)))\n",
    "        bias_init = default(\n",
    "            weight_init, Constant(0.))\n",
    "        self.hidden_activation = hidden_activation\n",
    "        \n",
    "        # Input to hidden\n",
    "        self.add_param('Wxh', (in_dim, hidden_dim), \n",
    "                       weight_init)\n",
    "        \n",
    "        # Hidden to hidden\n",
    "        self.add_param('Whh', (hidden_dim, hidden_dim), \n",
    "                       rec_weight_init)\n",
    "        \n",
    "        # Hidden bias\n",
    "        self.add_param('Bh', (hidden_dim,), \n",
    "                       bias_init)\n",
    "        \n",
    "        # Initial hidden state\n",
    "        #\n",
    "        # Note - here we introduce the first hidden state as a new\n",
    "        # parameter to be learned. This usually helps\n",
    "        #\n",
    "        # We initialize them in the same way as biases which is usually to zero\n",
    "        # If you use this code, make sure that this initialization make sense \n",
    "        # to you!\n",
    "        #\n",
    "        self.add_param('h0', (1, hidden_dim), \n",
    "                       bias_init)\n",
    "        self.initial_states.append(self.h0)\n",
    "    \n",
    "    def transition(self, x, h):\n",
    "        if debug:\n",
    "            print \"Inside recurrent transition\"\n",
    "            print 'x', repr(x)\n",
    "            print 'h', repr(h)\n",
    "        \n",
    "        h_new = self.hidden_activation(\n",
    "            x.dot(self.Wxh) + h.dot(self.Whh) + self.Bh)\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The parity task\n",
    "\n",
    "Here we solve the bit parity problem. Note that we will need at least two hidden neurons, because the network has to solve a XOR problem at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: Shape: (2, 6)\n",
      "[[ 0.  1.  1.  0.  0.  0.]\n",
      " [ 1.  1.  1.  0.  1.  1.]] \n",
      "Y: Shape: (2, 6)\n",
      "[[ 0.  1.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "def gen_parity_examples(time_steps, batch_size):\n",
    "    X = (numpy.random.rand(time_steps, batch_size, 1)>0.5).astype('float32')\n",
    "    Y = X.cumsum(0) % 2\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "Xp,Yp = gen_parity_examples(2, 6)\n",
    "print 'X:', Xp.reshape(Xp.shape[:-1]), '\\nY:', Yp.reshape(Yp.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside recurrent transition\n",
      "x X[t]\n",
      "Shape: (6, 1)\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "h <TensorType(float32, matrix)>\n",
      "Shape: (6, 3)\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "H Subtensor{int64::}.0\n",
      "Shape: (2, 6, 3)\n",
      "[[[ 0.          0.          0.        ]\n",
      "  [-0.64358824  0.76063931 -0.5170694 ]\n",
      "  [-0.64358824  0.76063931 -0.5170694 ]\n",
      "  [ 0.          0.          0.        ]\n",
      "  [ 0.          0.          0.        ]\n",
      "  [ 0.          0.          0.        ]]\n",
      "\n",
      " [[-0.64358824  0.76063931 -0.5170694 ]\n",
      "  [-0.71791375  0.47414044 -0.25570753]\n",
      "  [-0.71791375  0.47414044 -0.25570753]\n",
      "  [ 0.          0.          0.        ]\n",
      "  [-0.64358824  0.76063931 -0.5170694 ]\n",
      "  [-0.64358824  0.76063931 -0.5170694 ]]]\n"
     ]
    }
   ],
   "source": [
    "# The input variable - a 3D tensor with axes:\n",
    "# time x batch_size x num_features\n",
    "\n",
    "X = TT.tensor3('X')\n",
    "X.tag.test_value = Xp\n",
    "\n",
    "theano.config.compute_test_value='warn'\n",
    "theano.config.print_test_value=True\n",
    "\n",
    "test_net = SimpleRNN(in_dim=1,\n",
    "                     hidden_dim=3)\n",
    "test_net.initialize()\n",
    "\n",
    "H = test_net.apply(X)\n",
    "\n",
    "print 'H', repr(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ParityNet(Chain):\n",
    "    def __init__(self, hidden_dim, **kwargs):\n",
    "        super(ParityNet, self).__init__(**kwargs)\n",
    "        self.rec = SimpleRNN(\n",
    "            in_dim=1,\n",
    "            hidden_dim=hidden_dim,\n",
    "            hidden_activation=TT.nnet.sigmoid,\n",
    "            )\n",
    "        self.children.append(self.rec)\n",
    "        self.merge = MergeInputHiddens(\n",
    "            in_dim=1, hidden_dim=hidden_dim,\n",
    "            out_dim=1\n",
    "            )\n",
    "        self.children.append(self.merge)\n",
    "        \n",
    "        self.X = TT.tensor3('X')\n",
    "        self.Y = TT.tensor3('Y')\n",
    "        self.full_supervision = TT.lscalar('full_supervision')\n",
    "        \n",
    "        self.inputs = [self.X, self.Y, self.full_supervision]\n",
    "    \n",
    "    def apply(self, X):\n",
    "        H = self.rec.apply(X)\n",
    "        O = self.merge.apply(X, H)\n",
    "        return TT.nnet.sigmoid(O)\n",
    "\n",
    "    def get_loss(self):\n",
    "        parity_net_output = self.apply(self.X)\n",
    "\n",
    "        #\n",
    "        # our loss is the mean square error\n",
    "        #\n",
    "        # we can try to give two ways of supervising the network:\n",
    "        # - at each step\n",
    "        # - once at the end\n",
    "        #\n",
    "\n",
    "        parity_net_loss_full = ((parity_net_output - self.Y)**2).mean()\n",
    "        parity_net_loss_last = ((parity_net_output[-1] - self.Y[-1])**2).mean()\n",
    "\n",
    "        # \n",
    "        # We can choose how much supervision we want to give to the network\n",
    "        # note that in general ifelse is slow in Theano and should be avoided\n",
    "        #\n",
    "        parity_net_loss = theano.ifelse.ifelse(self.full_supervision, \n",
    "                                               parity_net_loss_full, \n",
    "                                               parity_net_loss_last)\n",
    "        return parity_net_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.357803791761"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.config.compute_test_value='off'\n",
    "theano.config.print_test_value=True\n",
    "debug = False\n",
    "\n",
    "parity_net = ParityNet(hidden_dim=10)\n",
    "parity_net.initialize()\n",
    "\n",
    "if parity_net.rec.Whh.get_value().shape[0]==2:\n",
    "    #\n",
    "    # Design the net to solve parity\n",
    "    #\n",
    "    parity_net.rec.h0.set_value([[0, 0]])\n",
    "\n",
    "    parity_net.rec.Wxh.set_value([[2000, 1000]])\n",
    "\n",
    "    parity_net.rec.Whh.set_value([[ 1000, 1000],\n",
    "                                  [-1000, -1000]])\n",
    "\n",
    "    parity_net.rec.Bh.set_value([-500, -1500])\n",
    "\n",
    "    parity_net.merge.Wxo.set_value([[0,]])\n",
    "\n",
    "    parity_net.merge.Who.set_value([[1000], [-1000]])\n",
    "    parity_net.merge.Bo.set_value([-500])\n",
    "\n",
    "\n",
    "parity_net_loss = parity_net.get_loss()\n",
    "\n",
    "parity_test_function = theano.function(parity_net.inputs, \n",
    "                                       parity_net_loss)\n",
    "Xp, Yp = gen_parity_examples(300, 100)\n",
    "parity_test_function(Xp, Yp, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    This class computes the updates to parameters using the RMSProp learning rule and adding weight decay.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, net_loss, parameters, inputs):\n",
    "        self.RMSProp_dec_rate = \\\n",
    "            theano.shared(np.array(0.9, dtype='float32'))\n",
    "        self.RMSProp_epsilon = \\\n",
    "            theano.shared(np.array(1e-5, dtype='float32'))\n",
    "        self.lrate = \\\n",
    "            theano.shared(np.array(1e-2, dtype='float32'))\n",
    "        self.max_grad_norm = \\\n",
    "            theano.shared(np.array(1., dtype='float32'))\n",
    "        self.wdec = \\\n",
    "            theano.shared(np.array(0., dtype='float32'))\n",
    "\n",
    "        theano.config.compute_test_value='off' # Turn off for gradient computation\n",
    "        \n",
    "        wdec_loss = 0\n",
    "        for p in parameters:\n",
    "            if p.name[0]=='W':\n",
    "                wdec_loss = wdec_loss + (p**2).sum()*self.wdec\n",
    "        \n",
    "        grads = theano.grad(net_loss + wdec_loss, parameters)\n",
    "        updates = []\n",
    "\n",
    "        grad_norm = 0.\n",
    "\n",
    "        for g in grads:\n",
    "            grad_norm = grad_norm + (g**2).sum()\n",
    "        \n",
    "\n",
    "        for g,p in zip(grads, parameters):\n",
    "            step = g\n",
    "            step = g / TT.maximum(1.0, grad_norm/self.max_grad_norm)\n",
    "            if 1:\n",
    "                g2 = theano.shared(p.get_value() * 0.,\n",
    "                                   name=p.name + '_g2')\n",
    "                g2_new = (self.RMSProp_dec_rate * g2 + \n",
    "                          (1.0 - self.RMSProp_dec_rate) * g**2)\n",
    "                updates.append((g2, g2_new))\n",
    "                step = step / TT.sqrt(g2_new + self.RMSProp_epsilon)\n",
    "\n",
    "            step = self.lrate * step\n",
    "            updates.append((p, p - step))\n",
    "\n",
    "        self.train_function = theano.function(inputs, \n",
    "                                              [net_loss, net_loss + wdec_loss, grad_norm], \n",
    "                                              updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use RMSProp to train the network\n",
    "\n",
    "parity_trainer = Trainer(parity_net_loss, parity_net.parameters, parity_net.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of various problems with RNN training\n",
    "\n",
    "In the following cell you can implement with various ways of providing supervision to the network (which one is easier to train - when the net recieves feedback after each step or whn it recieves feedback only at the end of training?), changing the gradient clipping and playing with a curriculum.\n",
    "\n",
    "Notice the ddynamics of training - at the beginning the network does very little. The, suddenly it notices the input-output relationship from which point training starts to progress very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.295237183571, 0.295237183571, 0.0345808081329]\n",
      "4359 Increasing seq length to:  11\n",
      "4368 Increasing seq length to:  12\n",
      "4609 Increasing seq length to:  13\n",
      "4614 Increasing seq length to:  14\n",
      "4640 Increasing seq length to:  15\n",
      "4692 Increasing seq length to:  16\n",
      "4773 Increasing seq length to:  17\n",
      "4778 Increasing seq length to:  18\n",
      "4798 Increasing seq length to:  19\n",
      "4808 Increasing seq length to:  20\n",
      "4829 Increasing seq length to:  21\n",
      "4857 Increasing seq length to:  22\n",
      "4863 Increasing seq length to:  23\n",
      "4891 Increasing seq length to:  24\n",
      "4908 Increasing seq length to:  25\n",
      "4916 Increasing seq length to:  26\n",
      "4917 Increasing seq length to:  27\n",
      "4924 Increasing seq length to:  28\n",
      "4937 Increasing seq length to:  29\n",
      "4947 Increasing seq length to:  30\n",
      "5000 [0.000112316571176, 0.000112316571176, 1.52266910192e-08]\n",
      "5003 Increasing seq length to:  31\n",
      "5010 Increasing seq length to:  32\n",
      "5016 Increasing seq length to:  33\n",
      "5019 Increasing seq length to:  34\n",
      "5026 Increasing seq length to:  35\n",
      "5027 Increasing seq length to:  36\n",
      "5041 Increasing seq length to:  37\n",
      "5043 Increasing seq length to:  38\n",
      "5053 Increasing seq length to:  39\n",
      "5056 Increasing seq length to:  40\n",
      "5057 Increasing seq length to:  41\n",
      "5062 Increasing seq length to:  42\n",
      "5072 Increasing seq length to:  43\n",
      "5083 Increasing seq length to:  44\n",
      "5102 Increasing seq length to:  45\n",
      "5107 Increasing seq length to:  46\n",
      "5125 Increasing seq length to:  47\n",
      "5126 Increasing seq length to:  48\n",
      "5131 Increasing seq length to:  49\n",
      "5136 Increasing seq length to:  50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f4e78d5cf50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEZCAYAAACaWyIJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e22d178d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parity_net.initialize()\n",
    "\n",
    "# you can tweek the learning rate. 1e-2 worked best for me\n",
    "parity_trainer.lrate.set_value(1e-2)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# when set to 1 the net recieves an error signal after each step\n",
    "# when set to 0 the net recieves an error signal only once at the end\n",
    "full_sup = 1\n",
    "\n",
    "# without full_supervision it doesn't train for sequences longer than 3\n",
    "seq_len = 10\n",
    "\n",
    "# this enables \"curriculum learning\" - we gradually train on \n",
    "# longer and longer sequences\n",
    "#\n",
    "max_seq_len = 50\n",
    "\n",
    "for i in xrange(100000):\n",
    "    Xp, Yp = gen_parity_examples(seq_len, 10)\n",
    "    ret = parity_trainer.train_function(Xp, Yp, full_sup)\n",
    "    losses.append((i,) + tuple(ret))\n",
    "    if ret[0]<1e-4:\n",
    "        seq_len += 1\n",
    "        if seq_len>max_seq_len:\n",
    "            break\n",
    "        print i, \"Increasing seq length to: \", seq_len\n",
    "    if i%5000 == 0:\n",
    "        print i, ret\n",
    "    \n",
    "losses_a = np.array(losses)\n",
    "\n",
    "semilogy(losses_a[:,0], losses_a[:,2], label='rms + wdec')\n",
    "semilogy(losses_a[:,0], losses_a[:,2], label='rms')\n",
    "plot(losses_a[:,0], losses_a[:,3], label='grad norm')\n",
    "\n",
    "legend(loc='lower left')\n",
    "title('Training loss')\n",
    "xlabel('iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010651069897"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xp, Yp = gen_parity_examples(500, 100)\n",
    "parity_test_function(Xp, Yp, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition task\n",
    "\n",
    "Here we explore the task of adding two inputs marked by a binary inicators. This is a difficult task, because the net must learn to ignore the spurious inputs and to discover the relationship between the desired output and two distant time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: Shape: (10, 2)\n",
      "[[ 1.          0.57852113]\n",
      " [ 0.          0.71686029]\n",
      " [ 0.          0.54578042]\n",
      " [ 1.          0.27510166]\n",
      " [ 0.          0.64356309]\n",
      " [ 0.          0.79832804]\n",
      " [ 0.          0.24421814]\n",
      " [ 0.          0.90099776]\n",
      " [ 0.          0.76795334]\n",
      " [ 0.          0.07127649]] \n",
      "Y: Shape: (1,)\n",
      "[ 0.4268114]\n"
     ]
    }
   ],
   "source": [
    "# adapted from \n",
    "# https://github.com/pascanur/trainingRNNs/blob/master/addition.py\n",
    "def gen_addition_example(T, batchsize):\n",
    "    rng = numpy.random\n",
    "    \n",
    "    l = rng.randint(T, int(T * 1.1+0.9))\n",
    "    p0 = rng.randint(0, int(l*.1), size=(batchsize,))\n",
    "    p1 = rng.randint(0, int(l*.4), size=(batchsize,)) + int(l*.1)\n",
    "    \n",
    "    X = rng.uniform(size=(l, batchsize, 2)).astype('float32')\n",
    "    X[:,:,0] = 0.\n",
    "    X[p0, numpy.arange(batchsize), numpy.zeros((batchsize,),\n",
    "                                                dtype='int32')] = 1.\n",
    "    X[p1, numpy.arange(batchsize), numpy.zeros((batchsize,),\n",
    "                                                dtype='int32')] = 1.\n",
    "\n",
    "    Y = (X[p0, numpy.arange(batchsize),\n",
    "           numpy.ones((batchsize,), dtype='int32')] + \\\n",
    "         X[p1, numpy.arange(batchsize),\n",
    "           numpy.ones((batchsize,), dtype='int32')])/2.\n",
    "    return X, Y.reshape((-1,1)).astype('float32')\n",
    "\n",
    "Xa,Ya = gen_addition_example(10, 3)\n",
    "print 'X:', Xa[:,0,:], '\\nY:', Ya[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_RNN(RecurrentLayer):\n",
    "    \"\"\"\n",
    "    Implementation follows Alex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\n",
    "    \"SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\"\n",
    "    http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 hidden_activation=TT.tanh,\n",
    "                 rec_weight_init=None,\n",
    "                 weight_init=None, bias_init=None, \n",
    "                 forget_bias_init=None, \n",
    "                 **kwargs):\n",
    "        super(LSTM_RNN, self).__init__(**kwargs)\n",
    "        rec_weight_init = default(\n",
    "            rec_weight_init, IsotropicGaussian(1.0/sqrt(hidden_dim)))\n",
    "        weight_init = default(\n",
    "            weight_init, IsotropicGaussian(1.0/sqrt(in_dim)))\n",
    "        bias_init = default(\n",
    "            weight_init, Constant(0.))\n",
    "        forget_bias_init = default(\n",
    "            weight_init, Constant(1.))\n",
    "        self.hidden_activation = hidden_activation\n",
    "        \n",
    "        #\n",
    "        # Gates\n",
    "        #\n",
    "        \n",
    "        for gate in 'ifo':\n",
    "            self.add_param('Wx' + gate, (in_dim, hidden_dim), \n",
    "                           weight_init)\n",
    "            self.add_param('Wh' + gate, (hidden_dim, hidden_dim), \n",
    "                           weight_init)\n",
    "            # Note: a cell is only connected to its own gates\n",
    "            # Wc... are diagonal - so we allocate only a vector\n",
    "            # for them\n",
    "            self.add_param('Wc' + gate, (hidden_dim,), \n",
    "                           weight_init)\n",
    "            self.add_param('B' + gate, (hidden_dim,), \n",
    "                           bias_init)\n",
    "        \n",
    "        #\n",
    "        # Note - forget gate bias has a different initializer, because\n",
    "        # we often want to initialize it to 1\n",
    "        #\n",
    "        self.Bf.tag.initializer = forget_bias_init\n",
    "        \n",
    "        # Cell\n",
    "        self.add_param('Wxc', (in_dim, hidden_dim), \n",
    "                       weight_init)\n",
    "        self.add_param('Whc', (hidden_dim, hidden_dim), \n",
    "                       weight_init)\n",
    "        self.add_param('Bc', (hidden_dim,), \n",
    "                       bias_init)\n",
    "        \n",
    "        # Initial states\n",
    "        self.add_param('h0', (1, hidden_dim), \n",
    "                       bias_init)\n",
    "        self.initial_states.append(self.h0)\n",
    "        self.add_param('c0', (1, hidden_dim), \n",
    "                       bias_init)\n",
    "        self.initial_states.append(self.c0)\n",
    "        \n",
    "        \n",
    "    def transition(self, x, h, c):\n",
    "        \"\"\"\n",
    "        One step of LSTM transition.\n",
    "        \n",
    "        x is the previous input\n",
    "        h is the previous hidden state\n",
    "        c is the previous memory cell content\n",
    "        \"\"\"\n",
    "        \n",
    "        #\n",
    "        # Please note:\n",
    "        # The implementation below is not speed-optimal\n",
    "        # usually, it pays off to group similar matrix multiplications\n",
    "        # by grouping gates.\n",
    "        #\n",
    "        # Also, input-related computations should be moved out of scan since\n",
    "        # they can be done for all steps in parallel.\n",
    "        #\n",
    "        \n",
    "        \n",
    "        s = TT.nnet.sigmoid\n",
    "\n",
    "        # Note: for cells we do element0wise multiplication which \n",
    "        # is equvalent to a matrix multiplication with a diagonal matrix!\n",
    "        i = s(x.dot(self.Wxi) + h.dot(self.Whi) + c*self.Wci + self.Bi)\n",
    "        f = s(x.dot(self.Wxf) + h.dot(self.Whf) + c*self.Wcf + self.Bf)\n",
    "        \n",
    "        c_new = f*c + i*self.hidden_activation(x.dot(self.Wxc) + h.dot(self.Whc) + self.Bc)\n",
    "        o = s(x.dot(self.Wxo) + h.dot(self.Who) + c_new*self.Wco + self.Bo)\n",
    "        h_new = o * self.hidden_activation(c_new)\n",
    "        \n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdditionNet(Chain):\n",
    "    def __init__(self, hidden_dim, num_layers=1,\n",
    "                 use_lstm=True,\n",
    "                 **kwargs):\n",
    "        super(AdditionNet, self).__init__(**kwargs)\n",
    "        self.use_lstm = use_lstm\n",
    "        self.recs = []\n",
    "        \n",
    "        #\n",
    "        # Here we introduce a multilayer LSTM network. We simply feed the hidden states of the\n",
    "        # lower layers as the inputs of the higher ones. Optionally, the inputs can also be connected\n",
    "        # to the upper layers (not implemented in the code).\n",
    "        #\n",
    "        \n",
    "        for hid in xrange(num_layers):\n",
    "            if hid == 0:\n",
    "                in_dim = 2\n",
    "            else:\n",
    "                in_dim = hidden_dim\n",
    "            if use_lstm:\n",
    "                rec = LSTM_RNN(in_dim=in_dim, hidden_dim=hidden_dim)\n",
    "            else:\n",
    "                rec = SimpleRNN(in_dim=in_dim, hidden_dim=hidden_dim)\n",
    "            self.recs.append(rec)\n",
    "            self.children.append(rec)\n",
    "        \n",
    "        self.merge = MergeInputHiddens(\n",
    "            in_dim=2, hidden_dim=hidden_dim,\n",
    "            out_dim=1\n",
    "            )\n",
    "        self.children.append(self.merge)\n",
    "        \n",
    "        self.X = TT.tensor3('X')\n",
    "        self.Y = TT.matrix('Y')\n",
    "        \n",
    "        self.inputs = [self.X, self.Y]\n",
    "    \n",
    "    def apply(self, X):\n",
    "        H = X\n",
    "        for rec in self.recs:\n",
    "            H = rec.apply(H)\n",
    "            if self.use_lstm: # we don't use cell contents\n",
    "                H = H[0]\n",
    "        O = self.merge.apply(X[-1:], H[-1:])\n",
    "        return O\n",
    "\n",
    "    def get_loss(self):\n",
    "        net_output = self.apply(self.X)\n",
    "        return ((net_output - self.Y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.44661998749"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theano.config.compute_test_value='off'\n",
    "theano.config.print_test_value=True\n",
    "debug = False\n",
    "\n",
    "addition_net = AdditionNet(hidden_dim=20, use_lstm=True)\n",
    "addition_net.initialize()\n",
    "\n",
    "addition_net_loss = addition_net.get_loss()\n",
    "\n",
    "addition_test_function = theano.function(addition_net.inputs, \n",
    "                                       addition_net_loss)\n",
    "Xa, Ya = gen_addition_example(10,20)\n",
    "addition_test_function(Xa, Ya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use RMSProp to train the network\n",
    "\n",
    "addition_trainer = Trainer(addition_net_loss, addition_net.parameters, addition_net.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleRNN net converges to an RMS of about 0.04 - this is roughly the RMS if we disregard one number - If we don't sume over one number, the error is simply the expected value (i.e. 0.5) divide by 2 (from the task definition) and squared, or 0.25\\**2 = 0.0625.\n",
    "\n",
    "The LSTM eventually discovers the relation and gets better, though it needs a curriculum to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1.31879770756, 1.41429138184, 247.862625122]\n",
      "500 [0.0331892333925, 0.112647615373, 1.15076601505]\n",
      "1000 [0.00378292915411, 0.0660145282745, 0.00784769933671]\n",
      "1500 [0.0172020792961, 0.0658032223582, 0.343105554581]\n",
      "2000 [0.00656956480816, 0.0445881262422, 0.107080623507]\n",
      "2500 [0.0110275791958, 0.0408838242292, 0.217169880867]\n",
      "3000 [0.00182098255027, 0.0253630969673, 0.0135718910024]\n",
      "3500 [0.00484537193552, 0.0235288348049, 0.077718809247]\n",
      "4000 [0.0038986846339, 0.0188046712428, 0.0724524855614]\n",
      "4500 [0.0068940478377, 0.0189107768238, 0.0768703967333]\n",
      "5000 [0.0033547070343, 0.0131110092625, 0.039958178997]\n",
      "5500 [0.00579873425886, 0.0138365961611, 0.127941668034]\n",
      "6000 [0.00213885912672, 0.00885619595647, 0.033503651619]\n",
      "6500 [0.00167602568399, 0.00736061763018, 0.0313998162746]\n",
      "7000 [0.000561986700632, 0.00544132757932, 0.00290216365829]\n",
      "7500 [0.000601124891546, 0.00482308398932, 0.00549443997443]\n",
      "8000 [0.001022765995, 0.00474198907614, 0.0184184871614]\n",
      "8500 [0.00298879435286, 0.00630593765527, 0.0552386529744]\n",
      "9000 [0.00136895512696, 0.00436315638945, 0.0380039587617]\n",
      "9500 [0.00032061219099, 0.00305466284044, 0.000163411910762]\n",
      "10000 [0.000900807383005, 0.00341781741008, 0.0139917265624]\n",
      "10500 [0.000563845154829, 0.00290396949276, 0.00445579970255]\n",
      "11000 [0.00100795889739, 0.00320641254075, 0.00668148091063]\n",
      "11500 [0.00180449488107, 0.00387883884832, 0.050039049238]\n",
      "12000 [0.000268388102995, 0.00223504076712, 0.000326284731273]\n",
      "12040 Increasing seq length to:  25\n",
      "12500 [0.000672682595905, 0.00258087203838, 0.0053851949051]\n",
      "13000 [0.000321094790706, 0.00217067287304, 0.000456658919575]\n",
      "13500 [0.00382323004305, 0.0056291911751, 0.103593699634]\n",
      "13616 Increasing seq length to:  30\n",
      "14000 [0.000292005686788, 0.00207258621231, 0.00212730234489]\n",
      "14500 [0.00165770633612, 0.00339799723588, 0.0865996256471]\n",
      "15000 [0.000677923904732, 0.00238668196835, 0.0155224315822]\n",
      "15500 [0.00267541059293, 0.00434351945296, 0.252495795488]\n",
      "16000 [0.00173375371378, 0.00336312665604, 0.0232940241694]\n",
      "16500 [0.000173007749254, 0.0017681340687, 0.000454641529359]\n",
      "17000 [0.000682497746311, 0.00223645544611, 0.010060288012]\n",
      "17500 [0.00016555802722, 0.00169009796809, 0.000311254611006]\n",
      "17856 Increasing seq length to:  35\n",
      "18000 [0.000252687110333, 0.0017530930927, 0.0032533579506]\n",
      "18500 [0.00071450980613, 0.00220557488501, 0.0088936239481]\n",
      "19000 [0.00315156998113, 0.00462963897735, 0.368469178677]\n",
      "19031 Increasing seq length to:  40\n",
      "19500 [0.00719454605132, 0.0086663197726, 0.685474216938]\n",
      "19842 Increasing seq length to:  45\n",
      "20000 [0.00050560652744, 0.00197849911638, 0.00299698486924]\n",
      "20318 Increasing seq length to:  50\n",
      "20500 [0.00142001500353, 0.00288787460886, 0.231842502952]\n",
      "21000 [0.000936619355343, 0.00241656182334, 0.0885110795498]\n",
      "21500 [0.000130593762151, 0.00161518738605, 3.76756033802e-05]\n",
      "21722 Increasing seq length to:  55\n",
      "22000 [0.00204895995557, 0.00353809283115, 0.226513743401]\n",
      "22500 [0.000221759997658, 0.0017158058472, 0.0031750628259]\n",
      "23000 [0.00116877863184, 0.00265997019596, 0.0376868620515]\n",
      "23500 [0.000216686545173, 0.00170769006945, 0.000404661434004]\n",
      "24000 [0.000548464711756, 0.00203663669527, 0.00405684486032]\n",
      "24049 Increasing seq length to:  60\n",
      "24500 [0.00029567925958, 0.00179314042907, 0.0051322793588]\n",
      "24954 Increasing seq length to:  65\n",
      "25000 [0.000175010150997, 0.00168101035524, 0.00275111314841]\n",
      "25494 Increasing seq length to:  70\n",
      "25500 [0.000155338260811, 0.00167568388861, 0.00155096093658]\n",
      "26000 [0.00017584222951, 0.001708435826, 0.0015333078336]\n",
      "26500 [0.000909984693862, 0.00245865248144, 0.0393319949508]\n",
      "26536 Increasing seq length to:  75\n",
      "27000 [0.00161027465947, 0.00317205977626, 0.104925379157]\n",
      "27500 [0.00139469187707, 0.00296932016499, 0.187646135688]\n",
      "28000 [0.000203584655537, 0.00178408704232, 0.00236413022503]\n",
      "28500 [0.000231373269344, 0.00180932856165, 0.00413531204686]\n",
      "28832 Increasing seq length to:  80\n",
      "29000 [0.000190801001736, 0.00177797442302, 0.00309214647859]\n",
      "29500 [0.00393018731847, 0.0055215395987, 0.239838600159]\n",
      "30000 [0.000309717550408, 0.00191229279153, 0.00823710393161]\n",
      "30500 [0.00254516978748, 0.00413860753179, 0.311479568481]\n",
      "30817 Increasing seq length to:  85\n",
      "31000 [0.000106996332761, 0.00170630298089, 5.16234576935e-05]\n",
      "31033 Increasing seq length to:  90\n",
      "31071 Increasing seq length to:  95\n",
      "31500 [0.00147876550909, 0.00310508068651, 0.0600920915604]\n",
      "31593 Increasing seq length to:  100\n",
      "32000 [0.00160836731084, 0.00324138859287, 0.273711562157]\n",
      "32102 Increasing seq length to:  105\n",
      "32500 [0.000195847707801, 0.00184960837942, 0.000444849138148]\n",
      "33000 [0.000129307212774, 0.00179522694089, 0.000247109303018]\n",
      "33500 [0.000429343868745, 0.00209756474942, 0.0132208373398]\n",
      "33964 Increasing seq length to:  110\n",
      "34000 [0.000862381479237, 0.00253507704474, 0.0641914084554]\n",
      "34135 Increasing seq length to:  115\n",
      "34500 [0.00751340854913, 0.00918333698064, 0.66328638792]\n",
      "35000 [6.57707350911e-05, 0.00176110549364, 0.00010870477854]\n",
      "35500 [0.000135098962346, 0.00183114898391, 0.000127105755382]\n",
      "35693 Increasing seq length to:  120\n",
      "36000 [0.000270914810244, 0.00197315774858, 0.00454310933128]\n",
      "36500 [0.000254702667007, 0.00196376023814, 0.00288359355181]\n",
      "37000 [0.000546161842067, 0.0022570095025, 0.0222943611443]\n",
      "37046 Increasing seq length to:  125\n",
      "37500 [0.00760783720762, 0.00930760521442, 0.907715439796]\n",
      "38000 [0.000118767115055, 0.00183416181244, 8.06942116469e-05]\n",
      "38500 [6.27786939731e-05, 0.00178083649371, 0.000141602286021]\n",
      "38678 Increasing seq length to:  130\n",
      "39000 [0.001344331773, 0.00307052815333, 0.0313188806176]\n",
      "39406 Increasing seq length to:  135\n",
      "39500 [0.00722070643678, 0.00895142368972, 0.851069569588]\n",
      "40000 [0.00040631563752, 0.00214457162656, 0.0140170864761]\n",
      "40013 Increasing seq length to:  140\n",
      "40114 Increasing seq length to:  145\n",
      "40500 [9.79617616395e-05, 0.00184819847345, 1.77161600732e-05]\n",
      "41000 [0.00173883140087, 0.00349593581632, 0.145782411098]\n",
      "41500 [0.000556245096959, 0.00232933857478, 0.0303546991199]\n",
      "41781 Increasing seq length to:  150\n",
      "42000 [0.000136927381391, 0.00191807106603, 0.00154030555859]\n",
      "42500 [0.00179882231168, 0.00358238536865, 0.0392335131764]\n",
      "43000 [0.000140978998388, 0.00193212297745, 2.71366661764e-05]\n",
      "43307 Increasing seq length to:  155\n",
      "43500 [0.000125458274852, 0.0019226546865, 0.000418539042585]\n",
      "44000 [0.000559790642001, 0.00236049760133, 0.0198949538171]\n",
      "44500 [0.000156501089805, 0.00195616018027, 0.000580437481403]\n",
      "45000 [0.00144315464422, 0.00324547663331, 0.142864450812]\n",
      "45219 Increasing seq length to:  160\n",
      "45431 Increasing seq length to:  165\n",
      "45484 Increasing seq length to:  170\n",
      "45500 [0.000133921406814, 0.00194081745576, 0.00116099009756]\n",
      "46000 [0.00267559220083, 0.00448787258938, 0.0835504829884]\n",
      "46234 Increasing seq length to:  175\n",
      "46241 Increasing seq length to:  180\n",
      "46500 [0.000236787949689, 0.00206724833697, 0.00168222677894]\n",
      "46981 Increasing seq length to:  185\n",
      "47000 [0.00232123187743, 0.00416584545746, 0.254094362259]\n",
      "47123 Increasing seq length to:  190\n",
      "47193 Increasing seq length to:  195\n",
      "47500 [0.00215199240483, 0.0040158550255, 0.0376651473343]\n",
      "48000 [0.00159324100241, 0.00346611603163, 0.0711992457509]\n",
      "48125 Increasing seq length to:  200\n",
      "48500 [0.00183614087291, 0.00372047862038, 0.175437569618]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f4dfe7b3510>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEZCAYAAAB4hzlwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noJRSyk6DglJKKTsNCkoppez+H1xFoWXz9DGGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e1b7f3e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "addition_net.initialize()\n",
    "\n",
    "# you can tweek the learning rate. 1e-2 worked best for me\n",
    "if addition_net.use_lstm:\n",
    "    addition_trainer.lrate.set_value(5e-3)\n",
    "else:\n",
    "    addition_trainer.lrate.set_value(1e-3)\n",
    "addition_trainer.max_grad_norm.set_value(10.)\n",
    "# weight decay seems to be important for generalization\n",
    "addition_trainer.wdec.set_value(1e-4)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# without full_supervision it doesn't train for sequences longer than 3\n",
    "seq_len = 20\n",
    "\n",
    "# this enables \"curriculum learning\" - we gradually train on \n",
    "# longer and longer sequences\n",
    "#\n",
    "max_seq_len = 200\n",
    "\n",
    "for i in xrange(100000):\n",
    "    # note: we need to train on sequences of all lengths in order to\n",
    "    # prevent forgetting the solution on short sequences\n",
    "    this_len = np.random.randint(10, seq_len+1)\n",
    "    Xa, Ya = gen_addition_example(this_len, 100)\n",
    "    ret = addition_trainer.train_function(Xa, Ya)\n",
    "    losses.append((i,) + tuple(ret))\n",
    "    if this_len>seq_len*0.9 and ret[0] < 0.0002:\n",
    "        seq_len += 5\n",
    "        if seq_len>max_seq_len:\n",
    "            break\n",
    "        print i, \"Increasing seq length to: \", seq_len\n",
    "    if i%500 == 0:\n",
    "        print i, ret\n",
    "    \n",
    "losses_a = np.array(losses)\n",
    "\n",
    "semilogy(losses_a[:,0], losses_a[:,1], label='rms')\n",
    "semilogy(losses_a[:,0], losses_a[:,1], alpha=0.5, label='rms + wdec')\n",
    "plot(losses_a[:,0], losses_a[:,2], alpha=0.5, label='grad norm')\n",
    "\n",
    "legend(loc='lower left')\n",
    "title('Training loss')\n",
    "xlabel('iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00158890557941"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa, Ya = gen_addition_example(400,2000)\n",
    "addition_test_function(Xa, Ya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.832095503807, 0.836304485798, 236.203704834]\n",
      "500 [0.526300609112, 0.530508577824, 110.485961914]\n",
      "1000 [0.0267470758408, 0.0309113115072, 2.32760238647]\n",
      "1500 [0.0103317759931, 0.0144850723445, 0.499453485012]\n",
      "2000 [0.00848443433642, 0.0126282218844, 4.84773159027]\n",
      "2500 [0.00318341841921, 0.00730416225269, 0.293113470078]\n",
      "3000 [0.00485196523368, 0.00894743297249, 4.56355237961]\n",
      "3500 [0.00708141550422, 0.0111521445215, 7.4047036171]\n",
      "4000 [0.00144326116424, 0.00548987463117, 0.0140902921557]\n",
      "4500 [0.00142763147596, 0.00545142870396, 0.00740704499185]\n",
      "5000 [0.00957231968641, 0.0135708926246, 14.1857290268]\n",
      "5500 [0.00770025793463, 0.011676883325, 11.3290777206]\n",
      "6000 [0.00762740010396, 0.0115815093741, 10.3973913193]\n",
      "6500 [0.00359190162271, 0.00752467941493, 4.54958343506]\n",
      "7000 [0.00155353161972, 0.00546516804025, 0.918671071529]\n",
      "7500 [0.00149022857659, 0.00538079114631, 1.11529827118]\n",
      "8000 [0.008719089441, 0.012589706108, 13.113658905]\n",
      "8500 [0.00198965822347, 0.00583996390924, 1.67684006691]\n",
      "9000 [0.000793983403128, 0.0046249316074, 0.338124096394]\n",
      "9500 [0.00497288163751, 0.00878470484167, 6.41601800919]\n",
      "10000 [0.00141499924939, 0.00520789856091, 1.17300963402]\n",
      "10500 [0.0142604494467, 0.0180348940194, 20.2591075897]\n",
      "11000 [0.000867632625159, 0.00462379259989, 0.391135215759]\n",
      "11500 [0.000652244489174, 0.00439005997032, 0.337078630924]\n",
      "12000 [0.00152173649985, 0.00524203199893, 1.44240450859]\n",
      "12500 [0.00315519841388, 0.00685681169853, 3.38457083702]\n",
      "13000 [0.00128566485364, 0.00496998615563, 1.07661855221]\n",
      "13500 [0.00427272124216, 0.00793967954814, 4.98508358002]\n",
      "14000 [0.0041583138518, 0.00780837330967, 4.85257148743]\n",
      "14500 [0.00088945357129, 0.00452182721347, 0.513228178024]\n",
      "15000 [0.000616724952124, 0.0042310282588, 0.252126008272]\n",
      "15500 [0.00134516344406, 0.00494325906038, 1.13200938702]\n",
      "16000 [0.00523223169148, 0.00881390925497, 5.30411863327]\n",
      "16500 [0.000749614846427, 0.00431409012526, 0.433353096247]\n",
      "17000 [0.00250000506639, 0.00604860717431, 2.21152234077]\n",
      "17500 [0.00046914347331, 0.00400128867477, 0.0607923343778]\n",
      "18000 [0.00188340677414, 0.00539888162166, 1.39244949818]\n",
      "18500 [0.000440971838543, 0.00394087936729, 0.13813701272]\n",
      "19000 [0.00159916479606, 0.00508181471378, 1.03337550163]\n",
      "19500 [0.00156660855282, 0.00503204576671, 0.956555724144]\n",
      "20000 [0.00638762954623, 0.00983604881912, 5.26412963867]\n",
      "20070 Increasing seq length to:  15\n",
      "20500 [0.427173763514, 0.430617719889, 271.395843506]\n",
      "21000 [0.78274255991, 0.786185383797, 186.648330688]\n",
      "21500 [0.306183427572, 0.309625536203, 172.20916748]\n",
      "22000 [0.0454660020769, 0.0489083863795, 34.2842826843]\n",
      "22500 [0.133723810315, 0.137167140841, 80.6216201782]\n",
      "23000 [0.313861489296, 0.317305743694, 114.595947266]\n",
      "23500 [0.37296128273, 0.376408040524, 83.2162628174]\n",
      "24000 [0.249225497246, 0.252674728632, 81.4941101074]\n",
      "24500 [0.0901596471667, 0.0936133489013, 44.3888015747]\n",
      "25000 [0.00306251412258, 0.00651848781854, 0.0970359072089]\n",
      "25500 [0.0195135101676, 0.0229676812887, 18.1791172028]\n",
      "26000 [0.116522617638, 0.119976341724, 87.2233047485]\n",
      "26500 [0.010348408483, 0.0138012096286, 12.9851436615]\n",
      "27000 [0.109327577055, 0.112779401243, 145.57875061]\n",
      "27500 [0.092364653945, 0.0958162099123, 68.171295166]\n",
      "28000 [0.00847627595067, 0.0119270896539, 7.93622493744]\n",
      "28500 [0.0347279198468, 0.0381785258651, 27.7208957672]\n",
      "29000 [0.00526613835245, 0.00871789548546, 6.02039480209]\n",
      "29500 [0.065379448235, 0.0688313767314, 117.054794312]\n",
      "30000 [0.0017266406212, 0.00517986947671, 0.284546881914]\n",
      "30500 [0.0419236570597, 0.0453793667257, 36.8188896179]\n",
      "31000 [0.0413359552622, 0.0447920486331, 83.6575088501]\n",
      "31500 [0.00170047581196, 0.00516136782244, 0.299937427044]\n",
      "32000 [0.00311218295246, 0.00657410966232, 3.42195701599]\n",
      "32500 [0.00296895531937, 0.00642756232992, 5.29527378082]\n",
      "33000 [0.00349449063651, 0.00694920774549, 6.03435325623]\n",
      "33500 [0.0011790250428, 0.00462471088395, 0.455251425505]\n",
      "34000 [0.00866645202041, 0.0121030313894, 19.9769611359]\n",
      "34500 [0.00150610087439, 0.00493450090289, 2.44338345528]\n",
      "35000 [0.00650206767023, 0.00992228835821, 19.1109962463]\n",
      "35500 [0.00616285670549, 0.0095729008317, 11.9748010635]\n",
      "36000 [0.000546262017451, 0.00394652783871, 0.0923475176096]\n",
      "36500 [0.00655914470553, 0.00994988158345, 10.3279561996]\n",
      "37000 [0.00390533893369, 0.00728734163567, 7.1595621109]\n",
      "37500 [0.00137725728564, 0.00475136609748, 1.89152467251]\n",
      "38000 [0.00253868172877, 0.00590552389622, 3.49290394783]\n",
      "38500 [0.000820234767161, 0.00417694309726, 0.757753431797]\n",
      "39000 [0.00367963081226, 0.00702744862065, 4.73436403275]\n",
      "39500 [0.0038676194381, 0.00720672495663, 5.97263288498]\n",
      "40000 [0.00163379206788, 0.00496420636773, 3.06289696693]\n",
      "40500 [0.000677280884702, 0.00400145165622, 0.395734906197]\n",
      "41000 [0.000448832201073, 0.00376252783462, 0.15624243021]\n",
      "41500 [0.00690005999058, 0.0102067952976, 16.1936740875]\n",
      "42000 [0.00370078184642, 0.0069984914735, 9.69159030914]\n",
      "42500 [0.00407734327018, 0.00736821303144, 6.54174661636]\n",
      "43000 [0.00479552894831, 0.00807969737798, 12.371295929]\n",
      "43500 [0.000581514032092, 0.00385662680492, 0.532653868198]\n",
      "44000 [0.00560899125412, 0.00887671392411, 9.30551147461]\n",
      "44500 [0.00348362186924, 0.00674268603325, 3.74430608749]\n",
      "45000 [0.00495449174196, 0.0082039590925, 9.31558895111]\n",
      "45500 [0.00132190121803, 0.0045651155524, 2.41872382164]\n",
      "46000 [0.00301434984431, 0.00624786922708, 3.57412290573]\n",
      "46500 [0.00134124234319, 0.0045684450306, 1.53094744682]\n",
      "47000 [0.00100874819327, 0.00423067575321, 1.20773684978]\n",
      "47500 [0.00124692451209, 0.00446041766554, 1.08869040012]\n",
      "48000 [0.00155470636673, 0.00476170098409, 1.26584196091]\n",
      "48500 [0.0113277854398, 0.0145276989788, 24.7574291229]\n",
      "49000 [0.000652727263514, 0.00384384230711, 0.041655946523]\n",
      "49500 [0.00957763940096, 0.0127615807578, 22.2003364563]\n",
      "50000 [0.00530106853694, 0.00847732461989, 10.9138584137]\n",
      "50500 [0.0019893986173, 0.00515834009275, 1.74467802048]\n",
      "51000 [0.000535135506652, 0.00369819765911, 0.129192233086]\n",
      "51500 [0.00595957087353, 0.00911564845592, 8.7332277298]\n",
      "52000 [0.000372495269403, 0.00352132250555, 0.0699425712228]\n",
      "52500 [0.00227326131426, 0.00541541632265, 4.19950294495]\n",
      "53000 [0.000334846932674, 0.00347097869962, 0.0573843084276]\n",
      "53500 [0.00450684409589, 0.00763465184718, 4.3416762352]\n",
      "54000 [0.0037095984444, 0.00683072023094, 4.95247650146]\n",
      "54500 [0.00149105128367, 0.00460577197373, 2.97899508476]\n",
      "55000 [0.000342347892001, 0.00344990403391, 0.186900019646]\n",
      "55500 [0.00227613840252, 0.00537748262286, 3.9119310379]\n",
      "56000 [0.000430024258094, 0.00352430273779, 0.153357103467]\n",
      "56500 [0.0014515502844, 0.00453868322074, 2.40189647675]\n",
      "57000 [0.00781280640513, 0.0108933635056, 14.0536794662]\n",
      "57500 [0.000355226948159, 0.00342943286523, 0.0711954087019]\n",
      "58000 [0.00116989191156, 0.00423786463216, 1.80895042419]\n",
      "58500 [0.00402422295883, 0.00708589656278, 10.200419426]\n",
      "59000 [0.000643020670395, 0.00369851547293, 0.496220827103]\n",
      "59500 [0.000471611128887, 0.00351957557723, 0.324095070362]\n",
      "60000 [0.000308113609208, 0.00334963528439, 0.176136612892]\n",
      "60500 [0.00666303979233, 0.00969901774079, 12.3139820099]\n",
      "61000 [0.00431897304952, 0.00734644243494, 8.82420253754]\n",
      "61500 [0.000748910009861, 0.00376916164532, 0.766346096992]\n",
      "62000 [0.000715997011866, 0.00373039860278, 0.563263177872]\n",
      "62500 [0.0002492107742, 0.00325482245535, 0.0502699315548]\n",
      "63000 [0.00171886954922, 0.0047185565345, 2.19722318649]\n",
      "63500 [0.00334488181397, 0.00634002825245, 4.80973577499]\n",
      "64000 [0.000267741881544, 0.00325449509546, 0.0617661103606]\n",
      "64500 [0.00147008430213, 0.00445132283494, 1.27139079571]\n",
      "65000 [0.000745433266275, 0.00372077152133, 0.367312133312]\n",
      "65500 [0.00030709913699, 0.00327266380191, 0.196192324162]\n",
      "66000 [0.000298079277854, 0.00325590954162, 0.0196617040783]\n",
      "66500 [0.00143788696732, 0.00439001992345, 1.01096045971]\n",
      "67000 [0.00479414686561, 0.00773821491748, 6.35704565048]\n",
      "67500 [0.00022957076726, 0.00316763995215, 0.0302008073777]\n",
      "68000 [0.00273707858287, 0.00566758913919, 4.81881189346]\n",
      "68500 [0.000366787688108, 0.00329011376016, 0.141009315848]\n",
      "69000 [0.00177462771535, 0.00469103455544, 2.8679766655]\n",
      "69500 [0.00169532897417, 0.00460817525163, 1.1904810667]\n",
      "70000 [0.00158986076713, 0.00449468521401, 1.92844951153]\n",
      "70500 [0.00267352373339, 0.00557124149054, 5.32735443115]\n",
      "70645 Increasing seq length to:  20\n",
      "71000 [0.0040829940699, 0.00697475951165, 8.47030830383]\n",
      "71500 [0.00922806654125, 0.012112788856, 29.2207717896]\n",
      "72000 [0.00315026543103, 0.0060269664973, 5.03771448135]\n",
      "72500 [0.000507203920279, 0.00337857566774, 0.226461395621]\n",
      "73000 [0.00517908437178, 0.00804335344583, 12.4796943665]\n",
      "73500 [0.00193634093739, 0.00479530729353, 3.23302006721]\n",
      "74000 [0.0004649333423, 0.00331738591194, 0.27622115612]\n",
      "74500 [0.00598540622741, 0.00883284118026, 13.9604597092]\n",
      "75000 [0.00279301754199, 0.00563462357968, 6.16935586929]\n",
      "75500 [0.00258477777243, 0.00542131764814, 4.44919157028]\n",
      "76000 [0.00267607904971, 0.00550579885021, 1.99777400494]\n",
      "76500 [0.000548677926417, 0.00337241962552, 0.132664710283]\n",
      "77000 [0.017975827679, 0.0207960736006, 51.9792137146]\n",
      "77500 [0.00744807953015, 0.0102599607781, 22.6201286316]\n",
      "78000 [0.000273377023404, 0.00308096548542, 0.0822184979916]\n",
      "78500 [0.00391354830936, 0.00671527348459, 7.82112169266]\n",
      "79000 [0.000878228398506, 0.0036747942213, 0.887905716896]\n",
      "79500 [0.000228071337915, 0.00301815709099, 0.0313186757267]\n",
      "80000 [0.000315437879181, 0.00309971556999, 0.088849581778]\n",
      "80500 [0.00120530743152, 0.00398536399007, 1.7057672739]\n",
      "81000 [0.00131834554486, 0.00409184815362, 3.03426408768]\n",
      "81500 [0.000945243285969, 0.0037132028956, 0.885757029057]\n",
      "82000 [0.00033010012703, 0.00309381401166, 0.0834841951728]\n",
      "82500 [0.000284813984763, 0.00304210977629, 0.0353576913476]\n",
      "83000 [0.00298881530762, 0.00574125768617, 8.4314622879]\n",
      "83500 [0.00036014788202, 0.0031075605657, 0.28493103385]\n",
      "84000 [0.000653220107779, 0.00339472782798, 0.915406227112]\n",
      "84500 [0.00590698840097, 0.00864332169294, 20.8350353241]\n",
      "85000 [0.00179575441871, 0.00452751666307, 1.90622806549]\n",
      "85500 [0.000205733478651, 0.00293105165474, 0.00313834287226]\n",
      "86000 [0.00166405702475, 0.00438408134505, 4.66386318207]\n",
      "86500 [0.000258157291682, 0.00297347293235, 0.028079804033]\n",
      "87000 [0.000440379837528, 0.0031508714892, 0.194367215037]\n",
      "87500 [0.000590400595684, 0.00329674384557, 0.00218811398372]\n",
      "88000 [0.000513626320753, 0.00321527780034, 0.747381091118]\n",
      "88500 [0.000727999664377, 0.00342328078113, 0.895842373371]\n",
      "89000 [0.00159964442719, 0.00428981939331, 2.28385162354]\n",
      "89500 [0.00118223915342, 0.0038673216477, 2.24602603912]\n",
      "90000 [0.00218682549894, 0.00486767943949, 3.04545521736]\n",
      "90500 [0.00069295510184, 0.00336748128757, 0.505320668221]\n",
      "91000 [0.000814216269646, 0.00348381814547, 1.14995181561]\n",
      "91500 [0.000275617814623, 0.00293994834647, 0.204490453005]\n",
      "92000 [0.000582781387493, 0.00324102002196, 0.328169316053]\n",
      "92500 [0.00167540775146, 0.00432892004028, 4.63644313812]\n",
      "93000 [0.00718075223267, 0.00983030069619, 23.8699169159]\n",
      "93500 [0.00294205825776, 0.0055851759389, 7.89881706238]\n",
      "94000 [0.00270869396627, 0.00534774782136, 6.95280408859]\n",
      "94500 [0.0018278962234, 0.00446122558787, 2.55654597282]\n",
      "95000 [0.00112793117296, 0.00375612080097, 0.939571499825]\n",
      "95500 [0.00748751033098, 0.0101115303114, 18.7687854767]\n",
      "96000 [0.000302779517369, 0.00292117870413, 0.0831815451384]\n",
      "96500 [0.000449520914117, 0.00306188012473, 0.54752177]\n",
      "97000 [0.000545760791283, 0.00315346289426, 1.10102796555]\n",
      "97500 [0.000595083518419, 0.00319857965223, 0.83585703373]\n",
      "98000 [0.000504596100654, 0.00310416147113, 0.0703170597553]\n",
      "98500 [0.000361267389962, 0.00295439688489, 0.571637511253]\n",
      "99000 [0.00018915066903, 0.00277758203447, 0.0714717283845]\n",
      "99500 [0.00142823369242, 0.00401247106493, 1.47827291489]\n",
      "100000 [0.00856781471521, 0.0111469449475, 32.4142456055]\n",
      "100500 [0.00259444187395, 0.00516750663519, 2.31499361992]\n",
      "101000 [0.00193332135677, 0.00450189644471, 1.42174077034]\n",
      "101500 [0.000506753858645, 0.00307003268972, 0.718593299389]\n",
      "102000 [0.000885862275027, 0.00344443041831, 1.68685805798]\n",
      "102500 [0.000285997026367, 0.00284082675353, 0.044217325747]\n",
      "103000 [0.00342794624157, 0.00597820850089, 11.0917310715]\n",
      "103500 [0.000455324450741, 0.00300114788115, 0.309934049845]\n",
      "104000 [0.00351860094815, 0.00605819188058, 5.20443582535]\n",
      "104500 [0.00033259819611, 0.0028685152065, 0.0168235115707]\n",
      "105000 [0.0144157465547, 0.0169483609498, 37.6719551086]\n",
      "105500 [0.00174975197297, 0.0042775394395, 1.82634294033]\n",
      "106000 [0.000892475771252, 0.00341650797054, 1.82591950893]\n",
      "106500 [0.000580267223995, 0.00310038984753, 0.914328336716]\n",
      "107000 [0.000469544407679, 0.00298347324133, 0.528395414352]\n",
      "107500 [0.00291156792082, 0.00542142009363, 5.25602960587]\n",
      "108000 [0.00208081537858, 0.00458643212914, 3.75357699394]\n",
      "108500 [0.0172552801669, 0.0197574272752, 55.7804794312]\n",
      "109000 [0.000628729292657, 0.00312436092645, 0.83985298872]\n",
      "109500 [0.00155136629473, 0.00404313439503, 1.36471509933]\n",
      "110000 [0.00640335073695, 0.00889160204679, 16.0784759521]\n",
      "110500 [0.000708033505362, 0.00319222314283, 1.17566859722]\n",
      "111000 [0.000268182600848, 0.00274779670872, 0.0957874208689]\n",
      "111500 [0.00149752851576, 0.00397391198203, 1.76896631718]\n",
      "112000 [0.000325946952216, 0.00279709836468, 0.00461888825521]\n",
      "112500 [0.00239682430401, 0.00486406125128, 2.9633898735]\n",
      "113000 [0.00698472838849, 0.00944867450744, 14.8457050323]\n",
      "113500 [0.00691922474653, 0.00937889982015, 26.9040985107]\n",
      "114000 [0.000240706242039, 0.0026950542815, 0.100130803883]\n",
      "114500 [0.00220338837244, 0.00465348409489, 5.0905995369]\n",
      "115000 [0.000613118463662, 0.00305955251679, 1.05605053902]\n",
      "115500 [0.000310181756504, 0.002752302913, 0.227257817984]\n",
      "115676 Increasing seq length to:  25\n",
      "116000 [0.00387071515433, 0.00630874885246, 13.8086929321]\n",
      "116500 [0.000203167903237, 0.00263746734709, 0.00693828379735]\n",
      "117000 [0.000612833595369, 0.00304325902835, 0.290076255798]\n",
      "117500 [0.000503605173435, 0.00293079018593, 0.28229123354]\n",
      "118000 [0.00900920946151, 0.0114330239594, 59.7537536621]\n",
      "118500 [0.00321127707139, 0.00563143240288, 3.57423305511]\n",
      "119000 [0.00821080897003, 0.0106273302808, 46.0993843079]\n",
      "119500 [0.000296176061966, 0.00270953797735, 0.174361258745]\n",
      "120000 [0.00312197208405, 0.00553181720898, 15.2504749298]\n",
      "120500 [0.00734685501084, 0.0097525594756, 24.1378707886]\n",
      "121000 [0.0019564263057, 0.00435925973579, 6.74622488022]\n",
      "121500 [0.000301831372781, 0.00270216586068, 0.289981096983]\n",
      "122000 [0.000330917566316, 0.00272709736601, 0.0799850150943]\n",
      "122500 [0.00280664302409, 0.00519900210202, 6.92594718933]\n",
      "123000 [0.000401583936764, 0.00279030902311, 0.19689694047]\n",
      "123500 [0.000894555530977, 0.00328061101027, 1.48032200336]\n",
      "124000 [0.00309426966123, 0.00547605846077, 5.55622005463]\n",
      "124500 [0.0092986850068, 0.011677974835, 31.4347934723]\n",
      "125000 [0.000236581036006, 0.00261254492216, 0.134734004736]\n",
      "125500 [0.00018336638459, 0.00255577079952, 0.0416963249445]\n",
      "126000 [0.000516431347933, 0.00288543733768, 0.402014374733]\n",
      "126500 [0.000837697007228, 0.00320400320925, 2.46196675301]\n",
      "127000 [0.000134181464091, 0.0024964530021, 0.0159961208701]\n",
      "127500 [0.00913780368865, 0.0114968623966, 63.4060134888]\n",
      "128000 [0.00356039684266, 0.00591707555577, 14.7532577515]\n",
      "128500 [0.00808967463672, 0.0104428753257, 59.9926757812]\n",
      "129000 [0.0112048247829, 0.0135543327779, 59.4600715637]\n",
      "129500 [0.000949508743361, 0.00329526723363, 2.50050902367]\n",
      "130000 [0.00140468624886, 0.0037479030434, 2.49599766731]\n",
      "130500 [0.00508248154074, 0.00742240669206, 24.4585380554]\n",
      "131000 [0.000188971986063, 0.00252476846799, 0.0620119273663]\n",
      "131500 [0.00584149500355, 0.00817531254143, 38.7452354431]\n",
      "132000 [0.000209453923162, 0.0025395588018, 0.0258530080318]\n",
      "132500 [0.00540677830577, 0.00773327704519, 16.7574768066]\n",
      "133000 [0.00233643967658, 0.00466029532254, 13.5424280167]\n",
      "133500 [0.000806087919045, 0.00312695698813, 0.850688517094]\n",
      "134000 [0.00274502485991, 0.00506245763972, 15.9812908173]\n",
      "134500 [0.00197017844766, 0.00428499467671, 10.0566596985]\n",
      "135000 [0.00016576271446, 0.00247680349275, 0.0584556795657]\n",
      "135500 [0.000143468932947, 0.00245187873952, 0.0048799966462]\n",
      "136000 [0.000903854728676, 0.00320973573253, 1.13362908363]\n",
      "136500 [0.00020329853578, 0.00250581186265, 0.0741459727287]\n",
      "137000 [0.00153429794591, 0.00383443548344, 6.78170776367]\n",
      "137500 [0.00339963636361, 0.00569606106728, 21.199092865]\n",
      "138000 [0.000832456804346, 0.00312619330361, 1.31117379665]\n",
      "138500 [0.00228522415273, 0.00457562040538, 10.7559595108]\n",
      "139000 [0.00183123082388, 0.004118564073, 6.57671642303]\n",
      "139500 [0.000559137319215, 0.00284438603558, 1.48934435844]\n",
      "140000 [0.000873270852026, 0.00315539073199, 2.34723949432]\n",
      "140500 [0.000448277045507, 0.00272719049826, 0.918852686882]\n",
      "141000 [0.00545146130025, 0.00772694684565, 44.3728904724]\n",
      "141500 [0.000383408769267, 0.00265614781529, 0.189776405692]\n",
      "142000 [0.00128540408332, 0.00355538260192, 1.47317576408]\n",
      "142500 [0.00143508729525, 0.0037035048008, 3.41313028336]\n",
      "143000 [0.00219141342677, 0.00445581041276, 4.15990447998]\n",
      "143500 [0.00600314326584, 0.00826539751142, 23.9895706177]\n",
      "144000 [0.000199425747269, 0.00245834258385, 0.276757210493]\n",
      "144500 [0.000975696370006, 0.00323207490146, 3.83863306046]\n",
      "145000 [0.000375876872567, 0.00262904772535, 0.484186381102]\n",
      "145500 [0.0063509028405, 0.00860162638128, 23.3163280487]\n",
      "146000 [0.00402491306886, 0.00627166358754, 8.17408466339]\n",
      "146500 [0.00347477663308, 0.00571842677891, 25.4514045715]\n",
      "147000 [0.00614144839346, 0.00838272087276, 17.2779788971]\n",
      "147500 [0.000124435478938, 0.00236342917196, 0.00195912574418]\n",
      "148000 [0.000192119681742, 0.00242854910903, 0.0794840753078]\n",
      "148500 [0.000367608939996, 0.00260133901611, 0.166580066085]\n",
      "149000 [0.00268636504188, 0.00491680251434, 13.9923582077]\n",
      "149500 [0.00791503023356, 0.0101436302066, 17.1283760071]\n",
      "150000 [0.000444581179181, 0.00267065642402, 0.455359309912]\n",
      "150500 [0.000935411080718, 0.00315839494579, 2.61115050316]\n",
      "151000 [0.00799607485533, 0.0102163404226, 17.5509338379]\n",
      "151500 [0.000400133925723, 0.00261763529852, 0.451121479273]\n",
      "152000 [0.00362500012852, 0.00583994528279, 14.404168129]\n",
      "152500 [0.000166688929312, 0.00237834174186, 0.0544946081936]\n",
      "153000 [0.000141035328852, 0.00234982953407, 0.053499288857]\n",
      "153500 [0.000441893585958, 0.00264839269221, 0.32590430975]\n",
      "154000 [0.00260907714255, 0.00481372606009, 7.35127639771]\n",
      "154500 [0.00037207006244, 0.00257340120152, 0.628674566746]\n",
      "155000 [0.000324220309267, 0.0025234206114, 0.013564877212]\n",
      "155500 [0.000547071860638, 0.0027443615254, 0.568987607956]\n",
      "156000 [0.00258878059685, 0.00478285737336, 6.33833885193]\n",
      "156500 [0.0014892105246, 0.00368036492728, 3.40734314919]\n",
      "157000 [0.000656289805193, 0.00284558581188, 0.636019766331]\n",
      "157500 [0.00317242648453, 0.00535864429548, 9.02270698547]\n",
      "158000 [0.000570968375541, 0.00275438977405, 2.44321036339]\n",
      "158500 [0.0102406395599, 0.0124223167077, 34.3235282898]\n",
      "159000 [0.0105477394536, 0.0127267027274, 58.8258094788]\n",
      "159500 [0.000520991859958, 0.00269647454843, 1.21613240242]\n",
      "160000 [0.000784225470852, 0.00295814010315, 3.66005325317]\n",
      "160500 [0.00020109230536, 0.0023729596287, 0.0263228528202]\n",
      "161000 [0.00301546626724, 0.00518453260884, 9.99722957611]\n",
      "161366 Increasing seq length to:  30\n",
      "161500 [0.00101720762905, 0.00318425847217, 6.50014305115]\n",
      "162000 [0.00568975973874, 0.00785437971354, 44.478931427]\n",
      "162500 [0.0150019535795, 0.0171638727188, 146.659439087]\n",
      "163000 [0.000205242686206, 0.00236552464776, 0.210871309042]\n",
      "163500 [0.000609070062637, 0.00276711490005, 1.29188144207]\n",
      "164000 [0.00064946123166, 0.00280534359626, 1.7592997551]\n",
      "164500 [0.0054851109162, 0.00763840274885, 21.0622463226]\n",
      "165000 [0.000991586828604, 0.00314309378155, 4.50296354294]\n",
      "165500 [0.000441467098426, 0.00259106000885, 1.01021552086]\n",
      "166000 [0.00513672176749, 0.00728394975886, 26.9599285126]\n",
      "166500 [0.000989504624158, 0.00313472701237, 3.08329868317]\n",
      "167000 [0.00212323735468, 0.00426646415144, 4.03598356247]\n",
      "167500 [0.00123443547636, 0.00337568763644, 3.01994681358]\n",
      "168000 [0.000111578301585, 0.00225071352907, 0.00919216684997]\n",
      "168500 [0.000476419227198, 0.00261321896687, 1.93802320957]\n",
      "169000 [0.00206277705729, 0.00419767666608, 3.75215411186]\n",
      "169500 [0.00260738935322, 0.00473929010332, 7.74209070206]\n",
      "170000 [0.000776268308982, 0.00290656089783, 1.02296233177]\n",
      "170500 [0.010383669287, 0.0125118885189, 95.4513092041]\n",
      "171000 [0.000395249866415, 0.00252132117748, 1.52290809155]\n",
      "171500 [0.000198400361114, 0.00232237088494, 0.146733641624]\n",
      "172000 [0.000576448685024, 0.00269858073443, 3.55922460556]\n",
      "172500 [0.000306875561364, 0.0024272731971, 0.158270552754]\n",
      "173000 [0.00161299761385, 0.00373104587197, 4.12249660492]\n",
      "173500 [0.00148271024227, 0.00359853031114, 7.98255872726]\n",
      "174000 [0.000106590159703, 0.00222055474296, 0.0308285877109]\n",
      "174500 [0.0017914052587, 0.00390311935917, 6.672519207]\n",
      "175000 [0.00475222337991, 0.00686170067638, 38.6262207031]\n",
      "175500 [0.000208983721677, 0.00231654429808, 0.311095863581]\n",
      "176000 [0.000683930586092, 0.00278965919279, 3.00692558289]\n",
      "176500 [0.000277601822745, 0.00238130986691, 0.086893171072]\n",
      "177000 [0.0134289935231, 0.0155306104571, 135.362258911]\n",
      "177500 [0.00136802601628, 0.00346763269044, 7.56738328934]\n",
      "178000 [0.00505351135507, 0.00715087074786, 46.9186286926]\n",
      "178500 [0.000104947612272, 0.00220004562289, 0.000502529670484]\n",
      "178842 Increasing seq length to:  35\n",
      "179000 [0.0132086873055, 0.0153020145372, 149.232254028]\n",
      "179500 [0.00211060070433, 0.00420201895759, 14.7267494202]\n",
      "180000 [0.0068518454209, 0.00894145481288, 42.3802757263]\n",
      "180500 [0.000173471722519, 0.00226143398322, 0.15003670752]\n",
      "181000 [0.000279475032585, 0.00236542918719, 0.0700638517737]\n",
      "181500 [0.00498506380245, 0.00706926407292, 28.732585907]\n",
      "182000 [0.00217062514275, 0.00425288453698, 33.6334991455]\n",
      "182500 [0.00165601912886, 0.00373659632169, 9.7210521698]\n",
      "183000 [0.00134710420389, 0.00342610478401, 18.9908847809]\n",
      "183500 [0.000234075079788, 0.00231170770712, 0.230652332306]\n",
      "184000 [0.00586586771533, 0.00794165581465, 59.1203041077]\n",
      "184500 [0.00191408814862, 0.00398800009862, 24.8696422577]\n",
      "185000 [0.00134428648744, 0.003416540334, 12.6153697968]\n",
      "185500 [0.000362765451428, 0.00243331911042, 0.543817520142]\n",
      "186000 [0.00213360670023, 0.00420241151005, 14.3153676987]\n",
      "186500 [0.000444660225185, 0.00251183076762, 1.03853011131]\n",
      "187000 [0.000439691997599, 0.00250543910079, 0.26773673296]\n",
      "187500 [0.00132370705251, 0.00338741973974, 11.4213953018]\n",
      "188000 [0.00165878410917, 0.0037207917776, 27.0308666229]\n",
      "188500 [0.000224857460125, 0.00228524231352, 0.651446938515]\n",
      "189000 [0.00211151083931, 0.00417028227821, 34.9151687622]\n",
      "189500 [0.00134930061176, 0.00340669648722, 12.5257139206]\n",
      "190000 [0.00326955411583, 0.0053254486993, 25.9819278717]\n",
      "190500 [0.000375019619241, 0.00242907949723, 0.478219896555]\n",
      "191000 [0.000153683577082, 0.00220595649444, 0.156373515725]\n",
      "191500 [0.000116638264444, 0.00216736388393, 0.0913473591208]\n",
      "192000 [0.0198171902448, 0.021866120398, 386.817199707]\n",
      "192500 [0.000230283738347, 0.00227779010311, 0.530370593071]\n",
      "193000 [0.00114768894855, 0.0031933861319, 10.2354650497]\n",
      "193500 [0.000879967643414, 0.00292394193821, 9.11360454559]\n",
      "194000 [0.00398413371295, 0.00602651434019, 61.2361602783]\n",
      "194500 [0.000117877025332, 0.00215879268944, 0.00273099727929]\n",
      "195000 [0.00039364641998, 0.00243311631493, 2.66003346443]\n",
      "195500 [0.00177663634531, 0.00381422881037, 8.14529705048]\n",
      "196000 [0.00027620853507, 0.00231217499822, 0.00348832691088]\n",
      "196500 [0.00169402908068, 0.00372864515521, 14.6227302551]\n",
      "197000 [0.00201883073896, 0.00405189348385, 16.2559490204]\n",
      "197500 [0.00685692857951, 0.00888797454536, 61.593296051]\n",
      "198000 [0.000567205192056, 0.00259647588246, 2.09253644943]\n",
      "198500 [0.0055762170814, 0.00760415336117, 78.2461242676]\n",
      "198526 Increasing seq length to:  40\n",
      "199000 [0.00072076509241, 0.00274681509472, 5.1786942482]\n",
      "199500 [0.000330870330799, 0.00235554482788, 1.94300782681]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f4dfcf3bc90>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npyyOvCbxCCcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4dfcb9c9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theano.config.compute_test_value='off'\n",
    "theano.config.print_test_value=True\n",
    "debug = False\n",
    "\n",
    "addition_rnn_net = AdditionNet(hidden_dim=20, use_lstm=False)\n",
    "addition_rnn_net.initialize()\n",
    "\n",
    "addition_rnn_net_loss = addition_rnn_net.get_loss()\n",
    "\n",
    "addition_rnn_test_function = theano.function(addition_rnn_net.inputs, \n",
    "                                             addition_rnn_net_loss)\n",
    "Xa, Ya = gen_addition_example(10,20)\n",
    "addition_rnn_test_function(Xa, Ya)\n",
    "\n",
    "addition_rnn_trainer = Trainer(addition_rnn_net_loss, addition_rnn_net.parameters, addition_rnn_net.inputs)\n",
    "\n",
    "addition_rnn_net.initialize()\n",
    "\n",
    "# you can tweek the learning rate. 1e-2 worked best for me\n",
    "addition_rnn_trainer.lrate.set_value(1e-3)\n",
    "addition_rnn_trainer.max_grad_norm.set_value(.5)\n",
    "addition_rnn_trainer.wdec.set_value(1e-4)\n",
    "\n",
    "rnn_losses = []\n",
    "\n",
    "# without full_supervision it doesn't train for sequences longer than 3\n",
    "seq_len = 10\n",
    "\n",
    "# this enables \"curriculum learning\" - we gradually train on \n",
    "# longer and longer sequences\n",
    "#\n",
    "max_seq_len = 100\n",
    "\n",
    "for i in xrange(200000):\n",
    "    this_len = np.random.randint(10, seq_len+1)\n",
    "    Xa, Ya = gen_addition_example(this_len, 100)\n",
    "    rnn_ret = addition_rnn_trainer.train_function(Xa, Ya)\n",
    "    rnn_losses.append((i,) + tuple(rnn_ret))\n",
    "    if this_len>seq_len*0.9 and rnn_ret[0] < 0.0002:\n",
    "        seq_len += 5\n",
    "        if seq_len>max_seq_len:\n",
    "            break\n",
    "        print i, \"Increasing seq length to: \", seq_len\n",
    "    if i%500 == 0:\n",
    "        print i, rnn_ret\n",
    "    \n",
    "rnn_losses_a = np.array(rnn_losses)\n",
    "\n",
    "semilogy(rnn_losses_a[:,0], rnn_losses_a[:,1], label='rms')\n",
    "plot(rnn_losses_a[:,0], rnn_losses_a[:,2], alpha=0.5, label='grad norm')\n",
    "\n",
    "legend(loc='lower left')\n",
    "title('Training loss')\n",
    "xlabel('iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00430398294702"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa, Ya = gen_addition_example(20,2000)\n",
    "addition_rnn_test_function(Xa, Ya)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
