{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular network implementation\n",
    "\n",
    "In the following cells, I implement in a modular way a feedforward neural network. Please study the code -- many network implementations follow a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # TODO propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            #\n",
    "            # https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2001-6.pdf\n",
    "            weight_init = Uniform(width = 0.1)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        #\n",
    "        # TODO: fill in gradient computation\n",
    "        #\n",
    "        dYdX = self.W\n",
    "        #print \"Affine shapes: \", dLdY.shape, \" \", dYdX.shape\n",
    "        dLdX = dLdY.T.dot(dYdX).T\n",
    "        return dLdX\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #\n",
    "        # Fill in proper gradient computation\n",
    "        #\n",
    "        dYdX = (1.0 - Y**2)\n",
    "        #print \"Tanh shapes: \", Y.shape, \" \",dLdY.shape, \" \", dYdX.shape\n",
    "        return (dLdY * dYdX)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #print \"RELU shapes: \", Y.shape, \" \", dLdY\n",
    "        return dLdY * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        #print \"SoftMax error \", dLdX.shape \n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "dict = unpickle(\"../cifar10/data_batch_1\")\n",
    "for pic in \n",
    "print dict['data'][0:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        if (old_L - L)<tolerance:\n",
    "            print \"Tolerance level reached exiting\"\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, train error rate %f%%\" % (i, L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss 1.101256, train error rate 93.333333%\n",
      "At iteration 1000, loss 0.055549, train error rate 2.000000%\n",
      "At iteration 2000, loss 0.044579, train error rate 2.000000%\n",
      "At iteration 3000, loss 0.041862, train error rate 1.333333%\n",
      "At iteration 4000, loss 0.040746, train error rate 1.333333%\n",
      "At iteration 5000, loss 0.040121, train error rate 1.333333%\n",
      "At iteration 6000, loss 0.039667, train error rate 1.333333%\n",
      "At iteration 7000, loss 0.039265, train error rate 1.333333%\n",
      "At iteration 8000, loss 0.038867, train error rate 1.333333%\n",
      "At iteration 9000, loss 0.038459, train error rate 1.333333%\n",
      "At iteration 10000, loss 0.038047, train error rate 1.333333%\n",
      "At iteration 11000, loss 0.037643, train error rate 1.333333%\n",
      "At iteration 12000, loss 0.037263, train error rate 1.333333%\n",
      "At iteration 13000, loss 0.036914, train error rate 1.333333%\n",
      "At iteration 14000, loss 0.036599, train error rate 1.333333%\n",
      "At iteration 15000, loss 0.036313, train error rate 1.333333%\n",
      "At iteration 16000, loss 0.036051, train error rate 1.333333%\n",
      "At iteration 17000, loss 0.035802, train error rate 1.333333%\n",
      "At iteration 18000, loss 0.035558, train error rate 1.333333%\n",
      "At iteration 19000, loss 0.035310, train error rate 1.333333%\n",
      "At iteration 20000, loss 0.035047, train error rate 1.333333%\n",
      "At iteration 21000, loss 0.034756, train error rate 1.333333%\n",
      "At iteration 22000, loss 0.034430, train error rate 1.333333%\n",
      "At iteration 23000, loss 0.034072, train error rate 1.333333%\n",
      "At iteration 24000, loss 0.033687, train error rate 1.333333%\n",
      "At iteration 25000, loss 0.033272, train error rate 1.333333%\n",
      "At iteration 26000, loss 0.032816, train error rate 1.333333%\n",
      "At iteration 27000, loss 0.032302, train error rate 1.333333%\n",
      "At iteration 28000, loss 0.031695, train error rate 1.333333%\n",
      "At iteration 29000, loss 0.030919, train error rate 1.333333%\n",
      "At iteration 30000, loss 0.029799, train error rate 1.333333%\n",
      "At iteration 31000, loss 0.028060, train error rate 1.333333%\n",
      "At iteration 32000, loss 0.025629, train error rate 0.666667%\n",
      "At iteration 33000, loss 0.022898, train error rate 0.666667%\n",
      "At iteration 34000, loss 0.020212, train error rate 0.666667%\n",
      "At iteration 35000, loss 0.017728, train error rate 0.666667%\n",
      "At iteration 36000, loss 0.015541, train error rate 0.666667%\n",
      "At iteration 37000, loss 0.013674, train error rate 0.666667%\n",
      "At iteration 38000, loss 0.012093, train error rate 0.666667%\n",
      "At iteration 39000, loss 0.010750, train error rate 0.000000%\n",
      "At iteration 40000, loss 0.009606, train error rate 0.000000%\n",
      "At iteration 41000, loss 0.008627, train error rate 0.000000%\n",
      "At iteration 42000, loss 0.007787, train error rate 0.000000%\n",
      "At iteration 43000, loss 0.007064, train error rate 0.000000%\n",
      "At iteration 44000, loss 0.006439, train error rate 0.000000%\n",
      "At iteration 45000, loss 0.005897, train error rate 0.000000%\n",
      "At iteration 46000, loss 0.005425, train error rate 0.000000%\n",
      "At iteration 47000, loss 0.005012, train error rate 0.000000%\n",
      "At iteration 48000, loss 0.004649, train error rate 0.000000%\n",
      "At iteration 49000, loss 0.004328, train error rate 0.000000%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.fprop(IrisX).argmax(0) != IrisY).mean() *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "Your job is to implement SGD training on MNIST with the following elements:\n",
    "1. SGD + momentum\n",
    "2. weight decay\n",
    "3. early stopping\n",
    "\n",
    "In overall, you should get below 2% trainig errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Fuel\n",
    "\n",
    "The following cell prepares the data pipeline in fuel. please see SGD template for usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (2, 5)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip([1,2],[3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "\n",
    "                         \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_train1 = unpickle(\"../cifar10/data_batch_1\")\n",
    "cifar10_train2 = unpickle(\"../cifar10/data_batch_2\")\n",
    "cifar10_train3 = unpickle(\"../cifar10/data_batch_3\")\n",
    "cifar10_train4 = unpickle(\"../cifar10/data_batch_4\")\n",
    "cifar10_train5 = unpickle(\"../cifar10/data_batch_5\")\n",
    "cifar10_test = unpickle(\"../cifar10/test_batch\")\n",
    "meta = unpickle(\"../cifar10/batches.meta\")\n",
    "np.array(cifar10_train1['labels']).reshape(10000,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('../fuel/fuel/datasets/cifar10.hdf5', mode='w')\n",
    "batch1_features = f.create_dataset('batch1_features', (10000, 3072), dtype='uint8')\n",
    "batch1_features[...] = cifar10_train1['data']\n",
    "batch1_targets = f.create_dataset('batch1_targets', (10000, 1), dtype='uint8')\n",
    "batch1_targets[...] = np.array(cifar10_train1['labels']).reshape(10000,1)\n",
    "batch2_features = f.create_dataset('batch2_features', (10000, 3072), dtype='uint8')\n",
    "batch2_features[...] = cifar10_train2['data']\n",
    "batch2_targets = f.create_dataset('batch2_targets', (10000, 1), dtype='uint8')\n",
    "batch2_targets[...] = np.array(cifar10_train2['labels']).reshape(10000,1)\n",
    "batch3_features = f.create_dataset('batch3_features', (10000, 3072), dtype='uint8')\n",
    "batch3_features[...] = cifar10_train3['data']\n",
    "batch3_targets = f.create_dataset('batch3_targets', (10000, 1), dtype='uint8')\n",
    "batch3_targets[...] = np.array(cifar10_train3['labels']).reshape(10000,1)\n",
    "batch4_features = f.create_dataset('batch4_features', (10000, 3072), dtype='uint8')\n",
    "batch4_features[...] = cifar10_train4['data']\n",
    "batch4_targets = f.create_dataset('batch4_targets', (10000, 1), dtype='uint8')\n",
    "batch4_targets[...] = np.array(cifar10_train4['labels']).reshape(10000,1)\n",
    "batch5_features = f.create_dataset('batch5_features', (10000, 3072), dtype='uint8')\n",
    "batch5_features[...] = cifar10_train5['data']\n",
    "batch5_targets = f.create_dataset('batch5_targets', (10000, 1), dtype='uint8')\n",
    "batch5_targets[...] = np.array(cifar10_train5['labels']).reshape(10000,1)\n",
    "test_features = f.create_dataset('test_features', (10000, 3072), dtype='uint8')\n",
    "test_features[...] = cifar10_test['data']\n",
    "test_targets = f.create_dataset('test_targets', (10000, 1), dtype='uint8')\n",
    "test_targets[...] = np.array(cifar10_test['labels']).reshape(10000,1)\n",
    "\n",
    "\n",
    "split_array = numpy.empty(\n",
    "    12,\n",
    "    dtype=numpy.dtype([\n",
    "    ('split', 'a', 6),\n",
    "    ('source', 'a', 15),\n",
    "    ('start', numpy.int64, 1),\n",
    "    ('stop', numpy.int64, 1),\n",
    "    ('indices', h5py.special_dtype(ref=h5py.Reference)),\n",
    "    ('available', numpy.bool, 1),\n",
    "    ('comment', 'a', 1)]))\n",
    "split_array[0:2]['split'] = 'train1'.encode('utf8')\n",
    "split_array[2:4]['split'] = 'train2'.encode('utf8')\n",
    "split_array[4:6]['split'] = 'train3'.encode('utf8')\n",
    "split_array[6:8]['split'] = 'train4'.encode('utf8')\n",
    "split_array[8:10]['split'] = 'train5'.encode('utf8')\n",
    "split_array[10:12]['split'] = 'test'.encode('utf8')\n",
    "split_array[0]['source'] = 'batch1_features'.encode('utf8')\n",
    "split_array[1]['source'] = 'batch1_targets'.encode('utf8')\n",
    "split_array[2]['source'] = 'batch2_features'.encode('utf8')\n",
    "split_array[3]['source'] = 'batch2_targets'.encode('utf8')\n",
    "split_array[4]['source'] = 'batch3_features'.encode('utf8')\n",
    "split_array[5]['source'] = 'batch3_targets'.encode('utf8')\n",
    "split_array[6]['source'] = 'batch4_features'.encode('utf8')\n",
    "split_array[7]['source'] = 'batch4_targets'.encode('utf8')\n",
    "split_array[8]['source'] = 'batch5_features'.encode('utf8')\n",
    "split_array[9]['source'] = 'batch5_targets'.encode('utf8')\n",
    "split_array[10]['source'] = 'test_features'.encode('utf8')\n",
    "split_array[11]['source'] = 'test_targets'.encode('utf8')\n",
    "split_array[0:12]['start'] = 0\n",
    "split_array[0:12]['stop'] = 10000\n",
    "split_array[:]['indices'] = h5py.Reference()\n",
    "split_array[:]['available'] = True\n",
    "split_array[:]['comment'] = '.'.encode('utf8')\n",
    "f.attrs['split'] = split_array\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "cifar10_train = CIFAR10((\"train1\",), subset=slice(None,10000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(\n",
    "    cifar10_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar10_train.num_examples, 100))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train2\",), subset=slice(None,10000))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(\n",
    "    cifar10_validation, iteration_scheme=SequentialScheme(cifar10_validation.num_examples, 250))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(\n",
    "    cifar10_test, iteration_scheme=SequentialScheme(cifar10_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'batch1_features', u'batch1_targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (3072, 100) containing uint8\n",
      " - an array of size (1, 100) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (3072, 250) containing uint8\n",
      " - an array of size (1, 250) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09528022,  0.98029363,  1.09625913],\n",
       "       [ 1.02509709,  0.84414134,  1.09693962],\n",
       "       [ 0.92893431,  1.04985735,  0.95987327]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.normal(1, 0.1, (3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please note, the code blow is able to train a SoftMax regression model on mnist to poor results (ca 8%test error), \n",
    "# you must improve it\n",
    "#\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "    return num_errs/num_examples\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream, lamb = 0, epsilon = 0):\n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    #initialize momentum variables\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Hint: you need one valocity matrix for each parameter\n",
    "    velocities = [None for P in net.parameters]\n",
    "    \n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = 300\n",
    "    patience_expansion = 1.5\n",
    "    \n",
    "    try:\n",
    "        while e<number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            #First train on all data from this batch\n",
    "            for X,Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                train_loss.append((i,L))\n",
    "                train_erros.append((i,err_rate))\n",
    "                if i % 100 == 0:\n",
    "                    print \"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100)\n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a learning rate\n",
    "                    #\n",
    "                    # Hint, use the iteration counter i\n",
    "                    # alpha = TODO\n",
    "                    alpha = (i+1.0)/(20.0*i)\n",
    "                    #\n",
    "                    # TODO: set the momentum constant \n",
    "                    # \n",
    "                    \n",
    "                    # epsilon = TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: implement velocity update in momentum\n",
    "                    #\n",
    "                    \n",
    "                    # V[...] = TODO\n",
    "                    if V == None:\n",
    "                        V = 0.\n",
    "                    V = epsilon*(V+G)\n",
    "                    \n",
    "                    if N=='W':\n",
    "                        #\n",
    "                        # TODO: implement the weight decay addition to gradient\n",
    "                        #\n",
    "                        #G += TODO\n",
    "                        G += lamb * P\n",
    "                    #\n",
    "                    # TODO: set a more sensible learning rule here,\n",
    "                    # using your learning rate schedule and momentum\n",
    "                    #\n",
    "                    #!!!!! Need to modify the actual parameter here! \n",
    "                    P += -5e-2  *(G+V)\n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            print \"After epoch %d: valid_err_rate: %f%% currently going ot do %d epochs\" %(\n",
    "                e, val_error_rate, number_of_epochs)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_loss = np.array(train_loss)\n",
    "        semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "        legend()\n",
    "        \n",
    "        subplot(2,1,2)\n",
    "        train_erros = np.array(train_erros)\n",
    "        plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "        validation_errors = np.array(validation_errors)\n",
    "        plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.295357, batch error rate 90.000000%\n",
      "After epoch 1: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 200, batch loss 2.305973, batch error rate 92.000000%\n",
      "After epoch 2: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 300, batch loss 2.307586, batch error rate 84.000000%\n",
      "After epoch 3: valid_err_rate: 0.901300% currently going ot do 300 epochs\n",
      "At minibatch 400, batch loss 2.301503, batch error rate 91.000000%\n",
      "After epoch 4: valid_err_rate: 0.901300% currently going ot do 300 epochs\n",
      "At minibatch 500, batch loss 2.311094, batch error rate 89.000000%\n",
      "After epoch 5: valid_err_rate: 0.898700% currently going ot do 300 epochs\n",
      "At minibatch 600, batch loss 2.310238, batch error rate 89.000000%\n",
      "After epoch 6: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 700, batch loss 2.304056, batch error rate 94.000000%\n",
      "After epoch 7: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 800, batch loss 2.305140, batch error rate 91.000000%\n",
      "After epoch 8: valid_err_rate: 0.901600% currently going ot do 300 epochs\n",
      "At minibatch 900, batch loss 2.303967, batch error rate 87.000000%\n",
      "After epoch 9: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 1000, batch loss 2.300100, batch error rate 85.000000%\n",
      "After epoch 10: valid_err_rate: 0.901300% currently going ot do 300 epochs\n",
      "At minibatch 1100, batch loss 2.311407, batch error rate 92.000000%\n",
      "After epoch 11: valid_err_rate: 0.898800% currently going ot do 300 epochs\n",
      "At minibatch 1200, batch loss 2.308370, batch error rate 91.000000%\n",
      "After epoch 12: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 1300, batch loss 2.305516, batch error rate 90.000000%\n",
      "After epoch 13: valid_err_rate: 0.900500% currently going ot do 300 epochs\n",
      "At minibatch 1400, batch loss 2.311525, batch error rate 93.000000%\n",
      "After epoch 14: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 1500, batch loss 2.299005, batch error rate 86.000000%\n",
      "After epoch 15: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 1600, batch loss 2.317021, batch error rate 95.000000%\n",
      "After epoch 16: valid_err_rate: 0.901700% currently going ot do 300 epochs\n",
      "At minibatch 1700, batch loss 2.293209, batch error rate 89.000000%\n",
      "After epoch 17: valid_err_rate: 0.901300% currently going ot do 300 epochs\n",
      "At minibatch 1800, batch loss 2.302111, batch error rate 87.000000%\n",
      "After epoch 18: valid_err_rate: 0.901100% currently going ot do 300 epochs\n",
      "At minibatch 1900, batch loss 2.311940, batch error rate 93.000000%\n",
      "After epoch 19: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 2000, batch loss 2.303662, batch error rate 91.000000%\n",
      "After epoch 20: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 2100, batch loss 2.298570, batch error rate 92.000000%\n",
      "After epoch 21: valid_err_rate: 0.901300% currently going ot do 300 epochs\n",
      "At minibatch 2200, batch loss 2.303500, batch error rate 86.000000%\n",
      "After epoch 22: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 2300, batch loss 2.312234, batch error rate 93.000000%\n",
      "After epoch 23: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 2400, batch loss 2.282521, batch error rate 87.000000%\n",
      "After epoch 24: valid_err_rate: 0.901000% currently going ot do 300 epochs\n",
      "At minibatch 2500, batch loss 2.295712, batch error rate 85.000000%\n",
      "After epoch 25: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 2600, batch loss 2.295173, batch error rate 90.000000%\n",
      "After epoch 26: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 2700, batch loss 2.301649, batch error rate 88.000000%\n",
      "After epoch 27: valid_err_rate: 0.899100% currently going ot do 300 epochs\n",
      "At minibatch 2800, batch loss 2.309784, batch error rate 88.000000%\n",
      "After epoch 28: valid_err_rate: 0.901600% currently going ot do 300 epochs\n",
      "At minibatch 2900, batch loss 2.300266, batch error rate 87.000000%\n",
      "After epoch 29: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 3000, batch loss 2.303119, batch error rate 96.000000%\n",
      "After epoch 30: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 3100, batch loss 2.308806, batch error rate 87.000000%\n",
      "After epoch 31: valid_err_rate: 0.901600% currently going ot do 300 epochs\n",
      "At minibatch 3200, batch loss 2.301910, batch error rate 93.000000%\n",
      "After epoch 32: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 3300, batch loss 2.324204, batch error rate 92.000000%\n",
      "After epoch 33: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 3400, batch loss 2.309686, batch error rate 90.000000%\n",
      "After epoch 34: valid_err_rate: 0.901000% currently going ot do 300 epochs\n",
      "At minibatch 3500, batch loss 2.305596, batch error rate 88.000000%\n",
      "After epoch 35: valid_err_rate: 0.899500% currently going ot do 300 epochs\n",
      "At minibatch 3600, batch loss 2.310258, batch error rate 94.000000%\n",
      "After epoch 36: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 3700, batch loss 2.307954, batch error rate 90.000000%\n",
      "After epoch 37: valid_err_rate: 0.899400% currently going ot do 300 epochs\n",
      "At minibatch 3800, batch loss 2.304765, batch error rate 91.000000%\n",
      "After epoch 38: valid_err_rate: 0.900800% currently going ot do 300 epochs\n",
      "At minibatch 3900, batch loss 2.303672, batch error rate 89.000000%\n",
      "After epoch 39: valid_err_rate: 0.901600% currently going ot do 300 epochs\n",
      "At minibatch 4000, batch loss 2.304708, batch error rate 90.000000%\n",
      "After epoch 40: valid_err_rate: 0.899600% currently going ot do 300 epochs\n",
      "At minibatch 4100, batch loss 2.306519, batch error rate 94.000000%\n",
      "After epoch 41: valid_err_rate: 0.897500% currently going ot do 300 epochs\n",
      "At minibatch 4200, batch loss 2.302229, batch error rate 86.000000%\n",
      "After epoch 42: valid_err_rate: 0.900700% currently going ot do 300 epochs\n",
      "At minibatch 4300, batch loss 2.314327, batch error rate 90.000000%\n",
      "After epoch 43: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 4400, batch loss 2.318448, batch error rate 92.000000%\n",
      "After epoch 44: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 4500, batch loss 2.301082, batch error rate 92.000000%\n",
      "After epoch 45: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 4600, batch loss 2.307182, batch error rate 92.000000%\n",
      "After epoch 46: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 4700, batch loss 2.316751, batch error rate 93.000000%\n",
      "After epoch 47: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 4800, batch loss 2.308749, batch error rate 92.000000%\n",
      "After epoch 48: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 4900, batch loss 2.286366, batch error rate 88.000000%\n",
      "After epoch 49: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 5000, batch loss 2.300339, batch error rate 88.000000%\n",
      "After epoch 50: valid_err_rate: 0.901900% currently going ot do 300 epochs\n",
      "At minibatch 5100, batch loss 2.309874, batch error rate 89.000000%\n",
      "After epoch 51: valid_err_rate: 0.901600% currently going ot do 300 epochs\n",
      "At minibatch 5200, batch loss 2.300639, batch error rate 90.000000%\n",
      "After epoch 52: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 5300, batch loss 2.302383, batch error rate 90.000000%\n",
      "After epoch 53: valid_err_rate: 0.901800% currently going ot do 300 epochs\n",
      "At minibatch 5400, batch loss 2.304177, batch error rate 88.000000%\n",
      "After epoch 54: valid_err_rate: 0.899400% currently going ot do 300 epochs\n",
      "At minibatch 5500, batch loss 2.297299, batch error rate 89.000000%\n",
      "After epoch 55: valid_err_rate: 0.901300% currently going ot do 300 epochs\n",
      "At minibatch 5600, batch loss 2.308852, batch error rate 89.000000%\n",
      "After epoch 56: valid_err_rate: 0.901300% currently going ot do 300 epochs\n",
      "At minibatch 5700, batch loss 2.298379, batch error rate 86.000000%\n",
      "After epoch 57: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 5800, batch loss 2.297961, batch error rate 88.000000%\n",
      "After epoch 58: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 5900, batch loss 2.315293, batch error rate 88.000000%\n",
      "After epoch 59: valid_err_rate: 0.901400% currently going ot do 300 epochs\n",
      "At minibatch 6000, batch loss 2.303235, batch error rate 94.000000%\n",
      "After epoch 60: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 6100, batch loss 2.303916, batch error rate 89.000000%\n",
      "After epoch 61: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 6200, batch loss 2.312165, batch error rate 92.000000%\n",
      "After epoch 62: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 6300, batch loss 2.304261, batch error rate 93.000000%\n",
      "After epoch 63: valid_err_rate: 0.901100% currently going ot do 300 epochs\n",
      "At minibatch 6400, batch loss 2.305887, batch error rate 86.000000%\n",
      "After epoch 64: valid_err_rate: 0.900700% currently going ot do 300 epochs\n",
      "At minibatch 6500, batch loss 2.310066, batch error rate 92.000000%\n",
      "After epoch 65: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 6600, batch loss 2.307192, batch error rate 90.000000%\n",
      "After epoch 66: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 6700, batch loss 2.296843, batch error rate 87.000000%\n",
      "After epoch 67: valid_err_rate: 0.900700% currently going ot do 300 epochs\n",
      "At minibatch 6800, batch loss 2.303451, batch error rate 92.000000%\n",
      "After epoch 68: valid_err_rate: 0.900700% currently going ot do 300 epochs\n",
      "At minibatch 6900, batch loss 2.300077, batch error rate 90.000000%\n",
      "After epoch 69: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 7000, batch loss 2.320550, batch error rate 94.000000%\n",
      "After epoch 70: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 7100, batch loss 2.305927, batch error rate 93.000000%\n",
      "After epoch 71: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 7200, batch loss 2.299588, batch error rate 88.000000%\n",
      "After epoch 72: valid_err_rate: 0.898700% currently going ot do 300 epochs\n",
      "At minibatch 7300, batch loss 2.297386, batch error rate 91.000000%\n",
      "After epoch 73: valid_err_rate: 0.901800% currently going ot do 300 epochs\n",
      "At minibatch 7400, batch loss 2.297643, batch error rate 89.000000%\n",
      "After epoch 74: valid_err_rate: 0.897500% currently going ot do 300 epochs\n",
      "At minibatch 7500, batch loss 2.302683, batch error rate 88.000000%\n",
      "After epoch 75: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 7600, batch loss 2.306990, batch error rate 97.000000%\n",
      "After epoch 76: valid_err_rate: 0.901900% currently going ot do 300 epochs\n",
      "At minibatch 7700, batch loss 2.301967, batch error rate 93.000000%\n",
      "After epoch 77: valid_err_rate: 0.898700% currently going ot do 300 epochs\n",
      "At minibatch 7800, batch loss 2.305529, batch error rate 87.000000%\n",
      "After epoch 78: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 7900, batch loss 2.297984, batch error rate 87.000000%\n",
      "After epoch 79: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 8000, batch loss 2.303355, batch error rate 90.000000%\n",
      "After epoch 80: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 8100, batch loss 2.312952, batch error rate 93.000000%\n",
      "After epoch 81: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 8200, batch loss 2.307085, batch error rate 92.000000%\n",
      "After epoch 82: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 8300, batch loss 2.305505, batch error rate 91.000000%\n",
      "After epoch 83: valid_err_rate: 0.900700% currently going ot do 300 epochs\n",
      "At minibatch 8400, batch loss 2.311468, batch error rate 91.000000%\n",
      "After epoch 84: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 8500, batch loss 2.307669, batch error rate 92.000000%\n",
      "After epoch 85: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 8600, batch loss 2.303130, batch error rate 89.000000%\n",
      "After epoch 86: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 8700, batch loss 2.301329, batch error rate 91.000000%\n",
      "After epoch 87: valid_err_rate: 0.901300% currently going ot do 300 epochs\n",
      "At minibatch 8800, batch loss 2.307832, batch error rate 90.000000%\n",
      "After epoch 88: valid_err_rate: 0.901400% currently going ot do 300 epochs\n",
      "At minibatch 8900, batch loss 2.303651, batch error rate 87.000000%\n",
      "After epoch 89: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 9000, batch loss 2.308103, batch error rate 95.000000%\n",
      "After epoch 90: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 9100, batch loss 2.303241, batch error rate 91.000000%\n",
      "After epoch 91: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 9200, batch loss 2.304795, batch error rate 92.000000%\n",
      "After epoch 92: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 9300, batch loss 2.307192, batch error rate 94.000000%\n",
      "After epoch 93: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 9400, batch loss 2.295699, batch error rate 91.000000%\n",
      "After epoch 94: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 9500, batch loss 2.302049, batch error rate 94.000000%\n",
      "After epoch 95: valid_err_rate: 0.901400% currently going ot do 300 epochs\n",
      "At minibatch 9600, batch loss 2.312498, batch error rate 95.000000%\n",
      "After epoch 96: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 9700, batch loss 2.309370, batch error rate 92.000000%\n",
      "After epoch 97: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 9800, batch loss 2.314207, batch error rate 89.000000%\n",
      "After epoch 98: valid_err_rate: 0.900700% currently going ot do 300 epochs\n",
      "At minibatch 9900, batch loss 2.306313, batch error rate 88.000000%\n",
      "After epoch 99: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 10000, batch loss 2.317323, batch error rate 92.000000%\n",
      "After epoch 100: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 10100, batch loss 2.299535, batch error rate 91.000000%\n",
      "After epoch 101: valid_err_rate: 0.901400% currently going ot do 300 epochs\n",
      "At minibatch 10200, batch loss 2.297102, batch error rate 86.000000%\n",
      "After epoch 102: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 10300, batch loss 2.304095, batch error rate 92.000000%\n",
      "After epoch 103: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 10400, batch loss 2.311038, batch error rate 89.000000%\n",
      "After epoch 104: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 10500, batch loss 2.299969, batch error rate 88.000000%\n",
      "After epoch 105: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 10600, batch loss 2.307493, batch error rate 91.000000%\n",
      "After epoch 106: valid_err_rate: 0.899000% currently going ot do 300 epochs\n",
      "At minibatch 10700, batch loss 2.304518, batch error rate 90.000000%\n",
      "After epoch 107: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 10800, batch loss 2.287057, batch error rate 87.000000%\n",
      "After epoch 108: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 10900, batch loss 2.307433, batch error rate 94.000000%\n",
      "After epoch 109: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 11000, batch loss 2.299432, batch error rate 89.000000%\n",
      "After epoch 110: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 11100, batch loss 2.304022, batch error rate 93.000000%\n",
      "After epoch 111: valid_err_rate: 0.898700% currently going ot do 300 epochs\n",
      "At minibatch 11200, batch loss 2.326023, batch error rate 90.000000%\n",
      "After epoch 112: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 11300, batch loss 2.304086, batch error rate 90.000000%\n",
      "After epoch 113: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 11400, batch loss 2.307091, batch error rate 89.000000%\n",
      "After epoch 114: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 11500, batch loss 2.306310, batch error rate 87.000000%\n",
      "After epoch 115: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 11600, batch loss 2.304341, batch error rate 92.000000%\n",
      "After epoch 116: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 11700, batch loss 2.301974, batch error rate 93.000000%\n",
      "After epoch 117: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 11800, batch loss 2.306381, batch error rate 91.000000%\n",
      "After epoch 118: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 11900, batch loss 2.306705, batch error rate 93.000000%\n",
      "After epoch 119: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 12000, batch loss 2.304947, batch error rate 88.000000%\n",
      "After epoch 120: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 12100, batch loss 2.299891, batch error rate 88.000000%\n",
      "After epoch 121: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 12200, batch loss 2.303845, batch error rate 90.000000%\n",
      "After epoch 122: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 12300, batch loss 2.294729, batch error rate 90.000000%\n",
      "After epoch 123: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 12400, batch loss 2.306225, batch error rate 90.000000%\n",
      "After epoch 124: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 12500, batch loss 2.300039, batch error rate 89.000000%\n",
      "After epoch 125: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 12600, batch loss 2.317321, batch error rate 88.000000%\n",
      "After epoch 126: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 12700, batch loss 2.299566, batch error rate 88.000000%\n",
      "After epoch 127: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 12800, batch loss 2.299762, batch error rate 88.000000%\n",
      "After epoch 128: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 12900, batch loss 2.300796, batch error rate 90.000000%\n",
      "After epoch 129: valid_err_rate: 0.901400% currently going ot do 300 epochs\n",
      "At minibatch 13000, batch loss 2.311068, batch error rate 85.000000%\n",
      "After epoch 130: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 13100, batch loss 2.315569, batch error rate 93.000000%\n",
      "After epoch 131: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 13200, batch loss 2.306108, batch error rate 90.000000%\n",
      "After epoch 132: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 13300, batch loss 2.295601, batch error rate 90.000000%\n",
      "After epoch 133: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 13400, batch loss 2.309103, batch error rate 87.000000%\n",
      "After epoch 134: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 13500, batch loss 2.302371, batch error rate 93.000000%\n",
      "After epoch 135: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 13600, batch loss 2.318672, batch error rate 91.000000%\n",
      "After epoch 136: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 13700, batch loss 2.308720, batch error rate 89.000000%\n",
      "After epoch 137: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 13800, batch loss 2.317483, batch error rate 93.000000%\n",
      "After epoch 138: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 13900, batch loss 2.301636, batch error rate 91.000000%\n",
      "After epoch 139: valid_err_rate: 0.901400% currently going ot do 300 epochs\n",
      "At minibatch 14000, batch loss 2.308083, batch error rate 91.000000%\n",
      "After epoch 140: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 14100, batch loss 2.308237, batch error rate 89.000000%\n",
      "After epoch 141: valid_err_rate: 0.901400% currently going ot do 300 epochs\n",
      "At minibatch 14200, batch loss 2.301201, batch error rate 88.000000%\n",
      "After epoch 142: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 14300, batch loss 2.301000, batch error rate 89.000000%\n",
      "After epoch 143: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 14400, batch loss 2.301903, batch error rate 91.000000%\n",
      "After epoch 144: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 14500, batch loss 2.304019, batch error rate 91.000000%\n",
      "After epoch 145: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 14600, batch loss 2.288206, batch error rate 86.000000%\n",
      "After epoch 146: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 14700, batch loss 2.301402, batch error rate 89.000000%\n",
      "After epoch 147: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 14800, batch loss 2.310905, batch error rate 90.000000%\n",
      "After epoch 148: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 14900, batch loss 2.315574, batch error rate 91.000000%\n",
      "After epoch 149: valid_err_rate: 0.898600% currently going ot do 300 epochs\n",
      "At minibatch 15000, batch loss 2.299495, batch error rate 87.000000%\n",
      "After epoch 150: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 15100, batch loss 2.301991, batch error rate 93.000000%\n",
      "After epoch 151: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 15200, batch loss 2.303513, batch error rate 94.000000%\n",
      "After epoch 152: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 15300, batch loss 2.304263, batch error rate 89.000000%\n",
      "After epoch 153: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 15400, batch loss 2.297757, batch error rate 84.000000%\n",
      "After epoch 154: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 15500, batch loss 2.304635, batch error rate 86.000000%\n",
      "After epoch 155: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 15600, batch loss 2.306203, batch error rate 91.000000%\n",
      "After epoch 156: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 15700, batch loss 2.295562, batch error rate 86.000000%\n",
      "After epoch 157: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 15800, batch loss 2.296845, batch error rate 84.000000%\n",
      "After epoch 158: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 15900, batch loss 2.317661, batch error rate 95.000000%\n",
      "After epoch 159: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 16000, batch loss 2.307855, batch error rate 92.000000%\n",
      "After epoch 160: valid_err_rate: 0.901400% currently going ot do 300 epochs\n",
      "At minibatch 16100, batch loss 2.306444, batch error rate 89.000000%\n",
      "After epoch 161: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 16200, batch loss 2.311463, batch error rate 94.000000%\n",
      "After epoch 162: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 16300, batch loss 2.299644, batch error rate 85.000000%\n",
      "After epoch 163: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 16400, batch loss 2.298701, batch error rate 89.000000%\n",
      "After epoch 164: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 16500, batch loss 2.308347, batch error rate 89.000000%\n",
      "After epoch 165: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 16600, batch loss 2.313261, batch error rate 90.000000%\n",
      "After epoch 166: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 16700, batch loss 2.308065, batch error rate 92.000000%\n",
      "After epoch 167: valid_err_rate: 0.901800% currently going ot do 300 epochs\n",
      "At minibatch 16800, batch loss 2.305522, batch error rate 89.000000%\n",
      "After epoch 168: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 16900, batch loss 2.310712, batch error rate 92.000000%\n",
      "After epoch 169: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 17000, batch loss 2.309387, batch error rate 89.000000%\n",
      "After epoch 170: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 17100, batch loss 2.300146, batch error rate 85.000000%\n",
      "After epoch 171: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 17200, batch loss 2.306986, batch error rate 90.000000%\n",
      "After epoch 172: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 17300, batch loss 2.303539, batch error rate 88.000000%\n",
      "After epoch 173: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 17400, batch loss 2.309924, batch error rate 95.000000%\n",
      "After epoch 174: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 17500, batch loss 2.312213, batch error rate 93.000000%\n",
      "After epoch 175: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 17600, batch loss 2.297231, batch error rate 86.000000%\n",
      "After epoch 176: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 17700, batch loss 2.304554, batch error rate 91.000000%\n",
      "After epoch 177: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 17800, batch loss 2.305396, batch error rate 88.000000%\n",
      "After epoch 178: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 17900, batch loss 2.289943, batch error rate 89.000000%\n",
      "After epoch 179: valid_err_rate: 0.901800% currently going ot do 300 epochs\n",
      "At minibatch 18000, batch loss 2.299334, batch error rate 91.000000%\n",
      "After epoch 180: valid_err_rate: 0.901800% currently going ot do 300 epochs\n",
      "At minibatch 18100, batch loss 2.301806, batch error rate 94.000000%\n",
      "After epoch 181: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 18200, batch loss 2.306442, batch error rate 93.000000%\n",
      "After epoch 182: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 18300, batch loss 2.322368, batch error rate 90.000000%\n",
      "After epoch 183: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 18400, batch loss 2.295494, batch error rate 91.000000%\n",
      "After epoch 184: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 18500, batch loss 2.308280, batch error rate 93.000000%\n",
      "After epoch 185: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 18600, batch loss 2.299236, batch error rate 89.000000%\n",
      "After epoch 186: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 18700, batch loss 2.301176, batch error rate 87.000000%\n",
      "After epoch 187: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 18800, batch loss 2.308949, batch error rate 89.000000%\n",
      "After epoch 188: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 18900, batch loss 2.297284, batch error rate 87.000000%\n",
      "After epoch 189: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 19000, batch loss 2.313625, batch error rate 90.000000%\n",
      "After epoch 190: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 19100, batch loss 2.308194, batch error rate 89.000000%\n",
      "After epoch 191: valid_err_rate: 0.901700% currently going ot do 300 epochs\n",
      "At minibatch 19200, batch loss 2.317786, batch error rate 95.000000%\n",
      "After epoch 192: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 19300, batch loss 2.311756, batch error rate 90.000000%\n",
      "After epoch 193: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 19400, batch loss 2.300119, batch error rate 90.000000%\n",
      "After epoch 194: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 19500, batch loss 2.301328, batch error rate 89.000000%\n",
      "After epoch 195: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 19600, batch loss 2.300157, batch error rate 90.000000%\n",
      "After epoch 196: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 19700, batch loss 2.309433, batch error rate 87.000000%\n",
      "After epoch 197: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 19800, batch loss 2.305398, batch error rate 93.000000%\n",
      "After epoch 198: valid_err_rate: 0.901800% currently going ot do 300 epochs\n",
      "At minibatch 19900, batch loss 2.285736, batch error rate 87.000000%\n",
      "After epoch 199: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 20000, batch loss 2.305572, batch error rate 82.000000%\n",
      "After epoch 200: valid_err_rate: 0.900500% currently going ot do 300 epochs\n",
      "At minibatch 20100, batch loss 2.306437, batch error rate 86.000000%\n",
      "After epoch 201: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 20200, batch loss 2.303553, batch error rate 90.000000%\n",
      "After epoch 202: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 20300, batch loss 2.299245, batch error rate 90.000000%\n",
      "After epoch 203: valid_err_rate: 0.901100% currently going ot do 300 epochs\n",
      "At minibatch 20400, batch loss 2.310023, batch error rate 89.000000%\n",
      "After epoch 204: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 20500, batch loss 2.307351, batch error rate 89.000000%\n",
      "After epoch 205: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 20600, batch loss 2.301861, batch error rate 85.000000%\n",
      "After epoch 206: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 20700, batch loss 2.308702, batch error rate 93.000000%\n",
      "After epoch 207: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 20800, batch loss 2.317646, batch error rate 93.000000%\n",
      "After epoch 208: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 20900, batch loss 2.302379, batch error rate 90.000000%\n",
      "After epoch 209: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 21000, batch loss 2.312029, batch error rate 90.000000%\n",
      "After epoch 210: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 21100, batch loss 2.306861, batch error rate 92.000000%\n",
      "After epoch 211: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 21200, batch loss 2.307385, batch error rate 93.000000%\n",
      "After epoch 212: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 21300, batch loss 2.306540, batch error rate 91.000000%\n",
      "After epoch 213: valid_err_rate: 0.901400% currently going ot do 300 epochs\n",
      "At minibatch 21400, batch loss 2.301174, batch error rate 89.000000%\n",
      "After epoch 214: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 21500, batch loss 2.302281, batch error rate 91.000000%\n",
      "After epoch 215: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 21600, batch loss 2.301162, batch error rate 89.000000%\n",
      "After epoch 216: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 21700, batch loss 2.290277, batch error rate 82.000000%\n",
      "After epoch 217: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 21800, batch loss 2.301787, batch error rate 89.000000%\n",
      "After epoch 218: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 21900, batch loss 2.306572, batch error rate 90.000000%\n",
      "After epoch 219: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 22000, batch loss 2.299968, batch error rate 90.000000%\n",
      "After epoch 220: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 22100, batch loss 2.323379, batch error rate 93.000000%\n",
      "After epoch 221: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 22200, batch loss 2.305333, batch error rate 91.000000%\n",
      "After epoch 222: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 22300, batch loss 2.307648, batch error rate 91.000000%\n",
      "After epoch 223: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 22400, batch loss 2.299087, batch error rate 88.000000%\n",
      "After epoch 224: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 22500, batch loss 2.317645, batch error rate 94.000000%\n",
      "After epoch 225: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 22600, batch loss 2.304915, batch error rate 93.000000%\n",
      "After epoch 226: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 22700, batch loss 2.303550, batch error rate 84.000000%\n",
      "After epoch 227: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 22800, batch loss 2.299947, batch error rate 90.000000%\n",
      "After epoch 228: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 22900, batch loss 2.312861, batch error rate 94.000000%\n",
      "After epoch 229: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 23000, batch loss 2.302351, batch error rate 89.000000%\n",
      "After epoch 230: valid_err_rate: 0.897400% currently going ot do 300 epochs\n",
      "At minibatch 23100, batch loss 2.304089, batch error rate 86.000000%\n",
      "After epoch 231: valid_err_rate: 0.901500% currently going ot do 300 epochs\n",
      "At minibatch 23200, batch loss 2.298298, batch error rate 86.000000%\n",
      "After epoch 232: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 23300, batch loss 2.284323, batch error rate 86.000000%\n",
      "After epoch 233: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 23400, batch loss 2.300594, batch error rate 81.000000%\n",
      "After epoch 234: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 23500, batch loss 2.299489, batch error rate 87.000000%\n",
      "After epoch 235: valid_err_rate: 0.900500% currently going ot do 300 epochs\n",
      "At minibatch 23600, batch loss 2.287915, batch error rate 85.000000%\n",
      "After epoch 236: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 23700, batch loss 2.303532, batch error rate 91.000000%\n",
      "After epoch 237: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 23800, batch loss 2.309128, batch error rate 92.000000%\n",
      "After epoch 238: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 23900, batch loss 2.307052, batch error rate 93.000000%\n",
      "After epoch 239: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 24000, batch loss 2.309147, batch error rate 93.000000%\n",
      "After epoch 240: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 24100, batch loss 2.309332, batch error rate 91.000000%\n",
      "After epoch 241: valid_err_rate: 0.899300% currently going ot do 300 epochs\n",
      "At minibatch 24200, batch loss 2.307525, batch error rate 94.000000%\n",
      "After epoch 242: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 24300, batch loss 2.300384, batch error rate 86.000000%\n",
      "After epoch 243: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 24400, batch loss 2.302303, batch error rate 91.000000%\n",
      "After epoch 244: valid_err_rate: 0.900600% currently going ot do 300 epochs\n",
      "At minibatch 24500, batch loss 2.309882, batch error rate 94.000000%\n",
      "After epoch 245: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 24600, batch loss 2.321451, batch error rate 97.000000%\n",
      "After epoch 246: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 24700, batch loss 2.305330, batch error rate 94.000000%\n",
      "After epoch 247: valid_err_rate: 0.901200% currently going ot do 300 epochs\n",
      "At minibatch 24800, batch loss 2.318895, batch error rate 89.000000%\n",
      "After epoch 248: valid_err_rate: 0.898900% currently going ot do 300 epochs\n",
      "At minibatch 24900, batch loss 2.299565, batch error rate 88.000000%\n",
      "After epoch 249: valid_err_rate: 0.899200% currently going ot do 300 epochs\n",
      "At minibatch 25000, batch loss 2.303164, batch error rate 92.000000%\n",
      "After epoch 250: valid_err_rate: 0.901800% currently going ot do 300 epochs\n",
      "Setting network parameters from after epoch 12\n",
      "Test error rate: 0.899900\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQUREQkoKIiISUlIQEZGQkoKIiIT+P++LDO0kWsqtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9f32b0e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# TODO: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(3072,500),\n",
    "        ReLULayer(),\n",
    "        AffineLayer(500,20),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(20,10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net, cifar10_train_stream, cifar10_validation_stream, cifar10_test_stream, 0.0001)\n",
    "\n",
    "print \"Test error rate: %f\" % (compute_error_rate(net, cifar10_test_stream), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "compute_error_rate(net, cifar10_test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
