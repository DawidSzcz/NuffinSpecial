{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular network implementation\n",
    "\n",
    "In the following cells, I implement in a modular way a feedforward neural network. Please study the code -- many network implementations follow a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # TODO propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            #\n",
    "            # https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2001-6.pdf\n",
    "            weight_init = Uniform(width = 0.1)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        #\n",
    "        # TODO: fill in gradient computation\n",
    "        #\n",
    "        dYdX = self.W\n",
    "        #print \"Affine shapes: \", dLdY.shape, \" \", dYdX.shape\n",
    "        dLdX = dLdY.T.dot(dYdX).T\n",
    "        return dLdX\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #\n",
    "        # Fill in proper gradient computation\n",
    "        #\n",
    "        dYdX = (1.0 - Y**2)\n",
    "        #print \"Tanh shapes: \", Y.shape, \" \",dLdY.shape, \" \", dYdX.shape\n",
    "        return (dLdY * dYdX)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #print \"RELU shapes: \", Y.shape, \" \", dLdY\n",
    "        return dLdY * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        #print \"SoftMax error \", dLdX.shape \n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4]\n",
    "x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        if (old_L - L)<tolerance:\n",
    "            print \"Tolerance level reached exiting\"\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, train error rate %f%%\" % (i, L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss 1.100302, train error rate 82.000000%\n",
      "At iteration 1000, loss 0.055704, train error rate 2.000000%\n",
      "At iteration 2000, loss 0.044678, train error rate 2.000000%\n",
      "At iteration 3000, loss 0.041907, train error rate 1.333333%\n",
      "At iteration 4000, loss 0.040770, train error rate 1.333333%\n",
      "At iteration 5000, loss 0.040141, train error rate 1.333333%\n",
      "At iteration 6000, loss 0.039699, train error rate 1.333333%\n",
      "At iteration 7000, loss 0.039325, train error rate 1.333333%\n",
      "At iteration 8000, loss 0.038974, train error rate 1.333333%\n",
      "At iteration 9000, loss 0.038630, train error rate 1.333333%\n",
      "At iteration 10000, loss 0.038293, train error rate 1.333333%\n",
      "At iteration 11000, loss 0.037968, train error rate 1.333333%\n",
      "At iteration 12000, loss 0.037658, train error rate 1.333333%\n",
      "At iteration 13000, loss 0.037368, train error rate 1.333333%\n",
      "At iteration 14000, loss 0.037095, train error rate 1.333333%\n",
      "At iteration 15000, loss 0.036836, train error rate 1.333333%\n",
      "At iteration 16000, loss 0.036582, train error rate 1.333333%\n",
      "At iteration 17000, loss 0.036325, train error rate 1.333333%\n",
      "At iteration 18000, loss 0.036055, train error rate 1.333333%\n",
      "At iteration 19000, loss 0.035765, train error rate 1.333333%\n",
      "At iteration 20000, loss 0.035446, train error rate 1.333333%\n",
      "At iteration 21000, loss 0.035092, train error rate 1.333333%\n",
      "At iteration 22000, loss 0.034702, train error rate 1.333333%\n",
      "At iteration 23000, loss 0.034280, train error rate 1.333333%\n",
      "At iteration 24000, loss 0.033836, train error rate 1.333333%\n",
      "At iteration 25000, loss 0.033373, train error rate 1.333333%\n",
      "At iteration 26000, loss 0.032886, train error rate 1.333333%\n",
      "At iteration 27000, loss 0.032367, train error rate 1.333333%\n",
      "At iteration 28000, loss 0.031803, train error rate 1.333333%\n",
      "At iteration 29000, loss 0.031177, train error rate 1.333333%\n",
      "At iteration 30000, loss 0.030467, train error rate 1.333333%\n",
      "At iteration 31000, loss 0.029645, train error rate 1.333333%\n",
      "At iteration 32000, loss 0.028684, train error rate 1.333333%\n",
      "At iteration 33000, loss 0.027566, train error rate 1.333333%\n",
      "At iteration 34000, loss 0.026309, train error rate 1.333333%\n",
      "At iteration 35000, loss 0.024996, train error rate 1.333333%\n",
      "At iteration 36000, loss 0.023733, train error rate 1.333333%\n",
      "At iteration 37000, loss 0.022550, train error rate 1.333333%\n",
      "At iteration 38000, loss 0.021426, train error rate 1.333333%\n",
      "At iteration 39000, loss 0.020341, train error rate 1.333333%\n",
      "At iteration 40000, loss 0.019284, train error rate 1.333333%\n",
      "At iteration 41000, loss 0.018246, train error rate 1.333333%\n",
      "At iteration 42000, loss 0.017220, train error rate 1.333333%\n",
      "At iteration 43000, loss 0.016200, train error rate 0.666667%\n",
      "At iteration 44000, loss 0.015182, train error rate 0.000000%\n",
      "At iteration 45000, loss 0.014174, train error rate 0.000000%\n",
      "At iteration 46000, loss 0.013187, train error rate 0.000000%\n",
      "At iteration 47000, loss 0.012238, train error rate 0.000000%\n",
      "At iteration 48000, loss 0.011338, train error rate 0.000000%\n",
      "At iteration 49000, loss 0.010495, train error rate 0.000000%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.fprop(IrisX).argmax(0) != IrisY).mean() *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "Your job is to implement SGD training on MNIST with the following elements:\n",
    "1. SGD + momentum\n",
    "2. weight decay\n",
    "3. early stopping\n",
    "\n",
    "In overall, you should get below 2% trainig errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Fuel\n",
    "\n",
    "The following cell prepares the data pipeline in fuel. please see SGD template for usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (2, 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip([1,2],[3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (784, 100) containing float32\n",
      " - an array of size (1, 100) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (784, 250) containing float32\n",
      " - an array of size (1, 250) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.94037623,  0.93875049,  0.95517461],\n",
       "       [ 1.01004436,  0.83902154,  0.96134391],\n",
       "       [ 0.84632557,  1.0085622 ,  0.97158993]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.normal(1, 0.1, (3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please note, the code blow is able to train a SoftMax regression model on mnist to poor results (ca 8%test error), \n",
    "# you must improve it\n",
    "#\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "    return num_errs/num_examples\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream, lamb = 0, epsilon = 0):\n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    #initialize momentum variables\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Hint: you need one valocity matrix for each parameter\n",
    "    velocities = [None for P in net.parameters]\n",
    "    \n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = 3\n",
    "    patience_expansion = 1.5\n",
    "    \n",
    "    try:\n",
    "        while e<number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            #First train on all data from this batch\n",
    "            for X,Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                train_loss.append((i,L))\n",
    "                train_erros.append((i,err_rate))\n",
    "                if i % 100 == 0:\n",
    "                    print \"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100)\n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a learning rate\n",
    "                    #\n",
    "                    # Hint, use the iteration counter i\n",
    "                    # alpha = TODO\n",
    "                    alpha = (i+1.0)/(20.0*i)\n",
    "                    #\n",
    "                    # TODO: set the momentum constant \n",
    "                    # \n",
    "                    \n",
    "                    # epsilon = TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: implement velocity update in momentum\n",
    "                    #\n",
    "                    \n",
    "                    # V[...] = TODO\n",
    "                    if V == None:\n",
    "                        V = 0.\n",
    "                    V = epsilon*(V+G)\n",
    "                    \n",
    "                    if N=='W':\n",
    "                        #\n",
    "                        # TODO: implement the weight decay addition to gradient\n",
    "                        #\n",
    "                        #G += TODO\n",
    "                        G += lamb * P\n",
    "                    #\n",
    "                    # TODO: set a more sensible learning rule here,\n",
    "                    # using your learning rate schedule and momentum\n",
    "                    #\n",
    "                    #!!!!! Need to modify the actual parameter here! \n",
    "                    P += -5e-2  *(G+V)\n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            print \"After epoch %d: valid_err_rate: %f%% currently going ot do %d epochs\" %(\n",
    "                e, val_error_rate, number_of_epochs)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_loss = np.array(train_loss)\n",
    "        semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "        legend()\n",
    "        \n",
    "        subplot(2,1,2)\n",
    "        train_erros = np.array(train_erros)\n",
    "        plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "        validation_errors = np.array(validation_errors)\n",
    "        plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 1.564948, batch error rate 39.000000%\n",
      "At minibatch 200, batch loss 0.804298, batch error rate 20.000000%\n",
      "At minibatch 300, batch loss 0.703934, batch error rate 19.000000%\n",
      "At minibatch 400, batch loss 0.546324, batch error rate 17.000000%\n",
      "At minibatch 500, batch loss 0.408736, batch error rate 7.000000%\n",
      "After epoch 1: valid_err_rate: 0.096600% currently going ot do 3 epochs\n",
      "At minibatch 600, batch loss 0.275042, batch error rate 7.000000%\n",
      "At minibatch 700, batch loss 0.308676, batch error rate 7.000000%\n",
      "At minibatch 800, batch loss 0.281865, batch error rate 7.000000%\n",
      "At minibatch 900, batch loss 0.404896, batch error rate 11.000000%\n",
      "At minibatch 1000, batch loss 0.330414, batch error rate 9.000000%\n",
      "After epoch 2: valid_err_rate: 0.063700% currently going ot do 4 epochs\n",
      "At minibatch 1100, batch loss 0.361918, batch error rate 10.000000%\n",
      "At minibatch 1200, batch loss 0.242163, batch error rate 8.000000%\n",
      "At minibatch 1300, batch loss 0.194134, batch error rate 4.000000%\n",
      "At minibatch 1400, batch loss 0.273121, batch error rate 9.000000%\n",
      "At minibatch 1500, batch loss 0.161305, batch error rate 5.000000%\n",
      "After epoch 3: valid_err_rate: 0.050000% currently going ot do 5 epochs\n",
      "At minibatch 1600, batch loss 0.133298, batch error rate 3.000000%\n",
      "At minibatch 1700, batch loss 0.188893, batch error rate 4.000000%\n",
      "At minibatch 1800, batch loss 0.231065, batch error rate 8.000000%\n",
      "At minibatch 1900, batch loss 0.191072, batch error rate 6.000000%\n",
      "At minibatch 2000, batch loss 0.195106, batch error rate 5.000000%\n",
      "After epoch 4: valid_err_rate: 0.038700% currently going ot do 7 epochs\n",
      "At minibatch 2100, batch loss 0.096490, batch error rate 3.000000%\n",
      "At minibatch 2200, batch loss 0.115431, batch error rate 4.000000%\n",
      "At minibatch 2300, batch loss 0.181203, batch error rate 5.000000%\n",
      "At minibatch 2400, batch loss 0.167769, batch error rate 4.000000%\n",
      "At minibatch 2500, batch loss 0.110243, batch error rate 3.000000%\n",
      "After epoch 5: valid_err_rate: 0.035300% currently going ot do 8 epochs\n",
      "At minibatch 2600, batch loss 0.084282, batch error rate 2.000000%\n",
      "At minibatch 2700, batch loss 0.167362, batch error rate 4.000000%\n",
      "At minibatch 2800, batch loss 0.109801, batch error rate 3.000000%\n",
      "At minibatch 2900, batch loss 0.108280, batch error rate 2.000000%\n",
      "At minibatch 3000, batch loss 0.036015, batch error rate 0.000000%\n",
      "After epoch 6: valid_err_rate: 0.029900% currently going ot do 10 epochs\n",
      "At minibatch 3100, batch loss 0.053574, batch error rate 2.000000%\n",
      "At minibatch 3200, batch loss 0.158005, batch error rate 4.000000%\n",
      "At minibatch 3300, batch loss 0.076814, batch error rate 2.000000%\n",
      "At minibatch 3400, batch loss 0.057362, batch error rate 2.000000%\n",
      "At minibatch 3500, batch loss 0.173836, batch error rate 5.000000%\n",
      "After epoch 7: valid_err_rate: 0.029400% currently going ot do 11 epochs\n",
      "At minibatch 3600, batch loss 0.101372, batch error rate 3.000000%\n",
      "At minibatch 3700, batch loss 0.097947, batch error rate 2.000000%\n",
      "At minibatch 3800, batch loss 0.128340, batch error rate 4.000000%\n",
      "At minibatch 3900, batch loss 0.085103, batch error rate 1.000000%\n",
      "At minibatch 4000, batch loss 0.036467, batch error rate 1.000000%\n",
      "After epoch 8: valid_err_rate: 0.026900% currently going ot do 13 epochs\n",
      "At minibatch 4100, batch loss 0.081144, batch error rate 1.000000%\n",
      "At minibatch 4200, batch loss 0.049284, batch error rate 1.000000%\n",
      "At minibatch 4300, batch loss 0.213971, batch error rate 5.000000%\n",
      "At minibatch 4400, batch loss 0.083051, batch error rate 1.000000%\n",
      "At minibatch 4500, batch loss 0.107840, batch error rate 3.000000%\n",
      "After epoch 9: valid_err_rate: 0.027100% currently going ot do 13 epochs\n",
      "At minibatch 4600, batch loss 0.043722, batch error rate 0.000000%\n",
      "At minibatch 4700, batch loss 0.076703, batch error rate 2.000000%\n",
      "At minibatch 4800, batch loss 0.089876, batch error rate 4.000000%\n",
      "At minibatch 4900, batch loss 0.141918, batch error rate 3.000000%\n",
      "At minibatch 5000, batch loss 0.029648, batch error rate 0.000000%\n",
      "After epoch 10: valid_err_rate: 0.025700% currently going ot do 16 epochs\n",
      "At minibatch 5100, batch loss 0.031626, batch error rate 1.000000%\n",
      "At minibatch 5200, batch loss 0.063736, batch error rate 2.000000%\n",
      "At minibatch 5300, batch loss 0.051545, batch error rate 1.000000%\n",
      "At minibatch 5400, batch loss 0.083083, batch error rate 2.000000%\n",
      "At minibatch 5500, batch loss 0.126681, batch error rate 2.000000%\n",
      "After epoch 11: valid_err_rate: 0.032700% currently going ot do 16 epochs\n",
      "At minibatch 5600, batch loss 0.094556, batch error rate 3.000000%\n",
      "At minibatch 5700, batch loss 0.070225, batch error rate 3.000000%\n",
      "At minibatch 5800, batch loss 0.080755, batch error rate 2.000000%\n",
      "At minibatch 5900, batch loss 0.027745, batch error rate 1.000000%\n",
      "At minibatch 6000, batch loss 0.068425, batch error rate 3.000000%\n",
      "After epoch 12: valid_err_rate: 0.024900% currently going ot do 19 epochs\n",
      "At minibatch 6100, batch loss 0.058104, batch error rate 0.000000%\n",
      "At minibatch 6200, batch loss 0.098153, batch error rate 2.000000%\n",
      "At minibatch 6300, batch loss 0.026405, batch error rate 1.000000%\n",
      "At minibatch 6400, batch loss 0.040430, batch error rate 0.000000%\n",
      "At minibatch 6500, batch loss 0.050086, batch error rate 2.000000%\n",
      "After epoch 13: valid_err_rate: 0.023300% currently going ot do 20 epochs\n",
      "At minibatch 6600, batch loss 0.019796, batch error rate 0.000000%\n",
      "At minibatch 6700, batch loss 0.077000, batch error rate 3.000000%\n",
      "At minibatch 6800, batch loss 0.050331, batch error rate 1.000000%\n",
      "At minibatch 6900, batch loss 0.028339, batch error rate 1.000000%\n",
      "At minibatch 7000, batch loss 0.035854, batch error rate 1.000000%\n",
      "After epoch 14: valid_err_rate: 0.022600% currently going ot do 22 epochs\n",
      "At minibatch 7100, batch loss 0.011428, batch error rate 0.000000%\n",
      "At minibatch 7200, batch loss 0.067099, batch error rate 2.000000%\n",
      "At minibatch 7300, batch loss 0.048011, batch error rate 1.000000%\n",
      "At minibatch 7400, batch loss 0.026520, batch error rate 0.000000%\n",
      "At minibatch 7500, batch loss 0.055312, batch error rate 2.000000%\n",
      "After epoch 15: valid_err_rate: 0.022200% currently going ot do 23 epochs\n",
      "At minibatch 7600, batch loss 0.034161, batch error rate 1.000000%\n",
      "At minibatch 7700, batch loss 0.056538, batch error rate 1.000000%\n",
      "At minibatch 7800, batch loss 0.023621, batch error rate 0.000000%\n",
      "At minibatch 7900, batch loss 0.039004, batch error rate 2.000000%\n",
      "At minibatch 8000, batch loss 0.028674, batch error rate 0.000000%\n",
      "After epoch 16: valid_err_rate: 0.021500% currently going ot do 25 epochs\n",
      "At minibatch 8100, batch loss 0.018206, batch error rate 0.000000%\n",
      "At minibatch 8200, batch loss 0.014580, batch error rate 0.000000%\n",
      "At minibatch 8300, batch loss 0.018331, batch error rate 0.000000%\n",
      "At minibatch 8400, batch loss 0.048464, batch error rate 1.000000%\n",
      "At minibatch 8500, batch loss 0.015777, batch error rate 0.000000%\n",
      "After epoch 17: valid_err_rate: 0.021000% currently going ot do 26 epochs\n",
      "At minibatch 8600, batch loss 0.010075, batch error rate 0.000000%\n",
      "At minibatch 8700, batch loss 0.024234, batch error rate 0.000000%\n",
      "At minibatch 8800, batch loss 0.051482, batch error rate 1.000000%\n",
      "At minibatch 8900, batch loss 0.015400, batch error rate 0.000000%\n",
      "At minibatch 9000, batch loss 0.012890, batch error rate 0.000000%\n",
      "After epoch 18: valid_err_rate: 0.021200% currently going ot do 26 epochs\n",
      "At minibatch 9100, batch loss 0.028020, batch error rate 1.000000%\n",
      "At minibatch 9200, batch loss 0.037745, batch error rate 1.000000%\n",
      "At minibatch 9300, batch loss 0.021805, batch error rate 0.000000%\n",
      "At minibatch 9400, batch loss 0.035267, batch error rate 1.000000%\n",
      "At minibatch 9500, batch loss 0.076077, batch error rate 3.000000%\n",
      "After epoch 19: valid_err_rate: 0.028900% currently going ot do 26 epochs\n",
      "At minibatch 9600, batch loss 0.069419, batch error rate 2.000000%\n",
      "At minibatch 9700, batch loss 0.027421, batch error rate 0.000000%\n",
      "At minibatch 9800, batch loss 0.022118, batch error rate 0.000000%\n",
      "At minibatch 9900, batch loss 0.048429, batch error rate 1.000000%\n",
      "At minibatch 10000, batch loss 0.028182, batch error rate 1.000000%\n",
      "After epoch 20: valid_err_rate: 0.022400% currently going ot do 26 epochs\n",
      "At minibatch 10100, batch loss 0.035078, batch error rate 1.000000%\n",
      "At minibatch 10200, batch loss 0.023525, batch error rate 0.000000%\n",
      "At minibatch 10300, batch loss 0.041544, batch error rate 1.000000%\n",
      "At minibatch 10400, batch loss 0.023625, batch error rate 0.000000%\n",
      "At minibatch 10500, batch loss 0.027121, batch error rate 1.000000%\n",
      "After epoch 21: valid_err_rate: 0.021600% currently going ot do 26 epochs\n",
      "At minibatch 10600, batch loss 0.031232, batch error rate 1.000000%\n",
      "At minibatch 10700, batch loss 0.044223, batch error rate 2.000000%\n",
      "At minibatch 10800, batch loss 0.011458, batch error rate 0.000000%\n",
      "At minibatch 10900, batch loss 0.030212, batch error rate 1.000000%\n",
      "At minibatch 11000, batch loss 0.011675, batch error rate 0.000000%\n",
      "After epoch 22: valid_err_rate: 0.018400% currently going ot do 34 epochs\n",
      "At minibatch 11100, batch loss 0.013923, batch error rate 0.000000%\n",
      "At minibatch 11200, batch loss 0.018785, batch error rate 0.000000%\n",
      "At minibatch 11300, batch loss 0.034307, batch error rate 1.000000%\n",
      "At minibatch 11400, batch loss 0.020914, batch error rate 0.000000%\n",
      "At minibatch 11500, batch loss 0.091572, batch error rate 3.000000%\n",
      "After epoch 23: valid_err_rate: 0.030000% currently going ot do 34 epochs\n",
      "At minibatch 11600, batch loss 0.010506, batch error rate 0.000000%\n",
      "At minibatch 11700, batch loss 0.013533, batch error rate 0.000000%\n",
      "At minibatch 11800, batch loss 0.015156, batch error rate 1.000000%\n",
      "At minibatch 11900, batch loss 0.006239, batch error rate 0.000000%\n",
      "At minibatch 12000, batch loss 0.026245, batch error rate 0.000000%\n",
      "After epoch 24: valid_err_rate: 0.020200% currently going ot do 34 epochs\n",
      "At minibatch 12100, batch loss 0.033821, batch error rate 1.000000%\n",
      "At minibatch 12200, batch loss 0.006367, batch error rate 0.000000%\n",
      "At minibatch 12300, batch loss 0.011961, batch error rate 0.000000%\n",
      "At minibatch 12400, batch loss 0.010601, batch error rate 0.000000%\n",
      "At minibatch 12500, batch loss 0.019487, batch error rate 0.000000%\n",
      "After epoch 25: valid_err_rate: 0.019200% currently going ot do 34 epochs\n",
      "At minibatch 12600, batch loss 0.012917, batch error rate 0.000000%\n",
      "At minibatch 12700, batch loss 0.008897, batch error rate 0.000000%\n",
      "At minibatch 12800, batch loss 0.019700, batch error rate 0.000000%\n",
      "At minibatch 12900, batch loss 0.009962, batch error rate 0.000000%\n",
      "At minibatch 13000, batch loss 0.016524, batch error rate 1.000000%\n",
      "After epoch 26: valid_err_rate: 0.020400% currently going ot do 34 epochs\n",
      "At minibatch 13100, batch loss 0.008796, batch error rate 0.000000%\n",
      "At minibatch 13200, batch loss 0.006986, batch error rate 0.000000%\n",
      "At minibatch 13300, batch loss 0.010842, batch error rate 0.000000%\n",
      "At minibatch 13400, batch loss 0.007815, batch error rate 0.000000%\n",
      "At minibatch 13500, batch loss 0.026294, batch error rate 1.000000%\n",
      "After epoch 27: valid_err_rate: 0.018900% currently going ot do 34 epochs\n",
      "At minibatch 13600, batch loss 0.006479, batch error rate 0.000000%\n",
      "At minibatch 13700, batch loss 0.006195, batch error rate 0.000000%\n",
      "At minibatch 13800, batch loss 0.019620, batch error rate 0.000000%\n",
      "At minibatch 13900, batch loss 0.009376, batch error rate 0.000000%\n",
      "At minibatch 14000, batch loss 0.005323, batch error rate 0.000000%\n",
      "After epoch 28: valid_err_rate: 0.018900% currently going ot do 34 epochs\n",
      "At minibatch 14100, batch loss 0.011180, batch error rate 0.000000%\n",
      "At minibatch 14200, batch loss 0.019756, batch error rate 0.000000%\n",
      "At minibatch 14300, batch loss 0.002845, batch error rate 0.000000%\n",
      "At minibatch 14400, batch loss 0.004933, batch error rate 0.000000%\n",
      "At minibatch 14500, batch loss 0.005376, batch error rate 0.000000%\n",
      "After epoch 29: valid_err_rate: 0.018000% currently going ot do 44 epochs\n",
      "At minibatch 14600, batch loss 0.008496, batch error rate 0.000000%\n",
      "At minibatch 14700, batch loss 0.021803, batch error rate 0.000000%\n",
      "At minibatch 14800, batch loss 0.009369, batch error rate 0.000000%\n",
      "At minibatch 14900, batch loss 0.017778, batch error rate 0.000000%\n",
      "At minibatch 15000, batch loss 0.024460, batch error rate 1.000000%\n",
      "After epoch 30: valid_err_rate: 0.020100% currently going ot do 44 epochs\n",
      "At minibatch 15100, batch loss 0.008087, batch error rate 0.000000%\n",
      "At minibatch 15200, batch loss 0.006488, batch error rate 0.000000%\n",
      "At minibatch 15300, batch loss 0.011297, batch error rate 0.000000%\n",
      "At minibatch 15400, batch loss 0.006558, batch error rate 0.000000%\n",
      "At minibatch 15500, batch loss 0.006953, batch error rate 0.000000%\n",
      "After epoch 31: valid_err_rate: 0.018500% currently going ot do 44 epochs\n",
      "At minibatch 15600, batch loss 0.017120, batch error rate 0.000000%\n",
      "At minibatch 15700, batch loss 0.008449, batch error rate 0.000000%\n",
      "At minibatch 15800, batch loss 0.014008, batch error rate 1.000000%\n",
      "At minibatch 15900, batch loss 0.003249, batch error rate 0.000000%\n",
      "At minibatch 16000, batch loss 0.005329, batch error rate 0.000000%\n",
      "After epoch 32: valid_err_rate: 0.018900% currently going ot do 44 epochs\n",
      "At minibatch 16100, batch loss 0.010343, batch error rate 0.000000%\n",
      "At minibatch 16200, batch loss 0.006192, batch error rate 0.000000%\n",
      "At minibatch 16300, batch loss 0.004468, batch error rate 0.000000%\n",
      "At minibatch 16400, batch loss 0.008783, batch error rate 0.000000%\n",
      "At minibatch 16500, batch loss 0.004655, batch error rate 0.000000%\n",
      "After epoch 33: valid_err_rate: 0.018800% currently going ot do 44 epochs\n",
      "At minibatch 16600, batch loss 0.003264, batch error rate 0.000000%"
     ]
    }
   ],
   "source": [
    "#\n",
    "# TODO: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(784,500),\n",
    "        ReLULayer(),\n",
    "        AffineLayer(500,20),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(20,10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream, 0.0001)\n",
    "\n",
    "print \"Test error rate: %f\" % (compute_error_rate(net, mnist_test_stream), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "compute_error_rate(net, mnist_test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
