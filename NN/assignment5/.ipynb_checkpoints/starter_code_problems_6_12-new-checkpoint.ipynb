{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n",
      "Vendor:  Continuum Analytics, Inc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular network implementation\n",
    "\n",
    "In the following cells, I implement in a modular way a feedforward neural network. Please study the code -- many network implementations follow a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            #\n",
    "            # TODO propose a default initialization scheme.\n",
    "            # Type a sentence explaining why, and if you use a reference, \n",
    "            # cite it here\n",
    "            #\n",
    "            # https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2001-6.pdf\n",
    "            weight_init = Uniform(width = 0.1)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        #Save X for later reusal\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) +  self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        #\n",
    "        # TODO: fill in gradient computation\n",
    "        #\n",
    "        dYdX = self.W\n",
    "        #print \"Affine shapes: \", dLdY.shape, \" \", dYdX.shape\n",
    "        dLdX = dLdY.T.dot(dYdX).T\n",
    "        return dLdX\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "    \n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #\n",
    "        # Fill in proper gradient computation\n",
    "        #\n",
    "        dYdX = (1.0 - Y**2)\n",
    "        #print \"Tanh shapes: \", Y.shape, \" \",dLdY.shape, \" \", dYdX.shape\n",
    "        return (dLdY * dYdX)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        #print \"RELU shapes: \", Y.shape, \" \", dLdY\n",
    "        return dLdY * (Y>0)\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        #print \"SoftMax error \", dLdX.shape \n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "        return L, O, dLdP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4]\n",
    "x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training algorithms. They change the network!\n",
    "def GD(net, X, Y, alpha=1e-4, max_iters=1000000, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Simple batch gradient descent\n",
    "    \"\"\"\n",
    "    old_L = np.inf\n",
    "    for i in xrange(max_iters):\n",
    "        L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "        if old_L < L:\n",
    "            print \"Iter: %d, loss increased!!\" % (i,)\n",
    "        if (old_L - L)<tolerance:\n",
    "            print \"Tolerance level reached exiting\"\n",
    "            break\n",
    "        if i % 1000 == 0:\n",
    "            err_rate = (O.argmax(0) != Y).mean()\n",
    "            print \"At iteration %d, loss %f, train error rate %f%%\" % (i, L, err_rate*100)\n",
    "        for P,G in zip(net.parameters, gradients):\n",
    "            P -= alpha * G\n",
    "        old_L = L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "IrisX = iris.data.T\n",
    "IrisX = (IrisX - IrisX.mean(axis=1, keepdims=True)) / IrisX.std(axis=1, keepdims=True)\n",
    "IrisY = iris.target.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, loss 1.097980, train error rate 62.666667%\n",
      "At iteration 1000, loss 0.054496, train error rate 2.000000%\n",
      "At iteration 2000, loss 0.044378, train error rate 2.000000%\n",
      "At iteration 3000, loss 0.041779, train error rate 1.333333%\n",
      "At iteration 4000, loss 0.040703, train error rate 1.333333%\n",
      "At iteration 5000, loss 0.040111, train error rate 1.333333%\n",
      "At iteration 6000, loss 0.039696, train error rate 1.333333%\n",
      "At iteration 7000, loss 0.039342, train error rate 1.333333%\n",
      "At iteration 8000, loss 0.039001, train error rate 1.333333%\n",
      "At iteration 9000, loss 0.038661, train error rate 1.333333%\n",
      "At iteration 10000, loss 0.038327, train error rate 1.333333%\n",
      "At iteration 11000, loss 0.038007, train error rate 1.333333%\n",
      "At iteration 12000, loss 0.037709, train error rate 1.333333%\n",
      "At iteration 13000, loss 0.037434, train error rate 1.333333%\n",
      "At iteration 14000, loss 0.037179, train error rate 1.333333%\n",
      "At iteration 15000, loss 0.036940, train error rate 1.333333%\n",
      "At iteration 16000, loss 0.036710, train error rate 1.333333%\n",
      "At iteration 17000, loss 0.036481, train error rate 1.333333%\n",
      "At iteration 18000, loss 0.036246, train error rate 1.333333%\n",
      "At iteration 19000, loss 0.035999, train error rate 1.333333%\n",
      "At iteration 20000, loss 0.035731, train error rate 1.333333%\n",
      "At iteration 21000, loss 0.035435, train error rate 1.333333%\n",
      "At iteration 22000, loss 0.035103, train error rate 1.333333%\n",
      "At iteration 23000, loss 0.034730, train error rate 1.333333%\n",
      "At iteration 24000, loss 0.034322, train error rate 1.333333%\n",
      "At iteration 25000, loss 0.033886, train error rate 1.333333%\n",
      "At iteration 26000, loss 0.033427, train error rate 1.333333%\n",
      "At iteration 27000, loss 0.032942, train error rate 1.333333%\n",
      "At iteration 28000, loss 0.032418, train error rate 1.333333%\n",
      "At iteration 29000, loss 0.031841, train error rate 1.333333%\n",
      "At iteration 30000, loss 0.031191, train error rate 1.333333%\n",
      "At iteration 31000, loss 0.030444, train error rate 1.333333%\n",
      "At iteration 32000, loss 0.029578, train error rate 1.333333%\n",
      "At iteration 33000, loss 0.028574, train error rate 1.333333%\n",
      "At iteration 34000, loss 0.027451, train error rate 1.333333%\n",
      "At iteration 35000, loss 0.026281, train error rate 1.333333%\n",
      "At iteration 36000, loss 0.025147, train error rate 1.333333%\n",
      "At iteration 37000, loss 0.024080, train error rate 1.333333%\n",
      "At iteration 38000, loss 0.023069, train error rate 1.333333%\n",
      "At iteration 39000, loss 0.022093, train error rate 1.333333%\n",
      "At iteration 40000, loss 0.021140, train error rate 1.333333%\n",
      "At iteration 41000, loss 0.020201, train error rate 1.333333%\n",
      "At iteration 42000, loss 0.019273, train error rate 1.333333%\n",
      "At iteration 43000, loss 0.018355, train error rate 1.333333%\n",
      "At iteration 44000, loss 0.017446, train error rate 1.333333%\n",
      "At iteration 45000, loss 0.016544, train error rate 1.333333%\n",
      "At iteration 46000, loss 0.015649, train error rate 0.000000%\n",
      "At iteration 47000, loss 0.014764, train error rate 0.000000%\n",
      "At iteration 48000, loss 0.013891, train error rate 0.000000%\n",
      "At iteration 49000, loss 0.013034, train error rate 0.000000%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Here we verify that the network can be trained on Irises.\n",
    "# Most runs should result in 100% accurracy\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(4,10),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(10,3),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "GD(net, IrisX,IrisY, 1e-1, tolerance=1e-7, max_iters=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.fprop(IrisX).argmax(0) != IrisY).mean() *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD\n",
    "\n",
    "Your job is to implement SGD training on MNIST with the following elements:\n",
    "1. SGD + momentum\n",
    "2. weight decay\n",
    "3. early stopping\n",
    "\n",
    "In overall, you should get below 2% trainig errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from Fuel\n",
    "\n",
    "The following cell prepares the data pipeline in fuel. please see SGD template for usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (2, 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip([1,2],[3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named fuel.datasets.mnist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-89cc6f6128c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfuel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfuel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mScaleAndShift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfuel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreams\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataStream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfuel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschemes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequentialScheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mShuffledScheme\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named fuel.datasets.mnist"
     ]
    }
   ],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist_train_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7fa20adee693>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"The streams return batches containing %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmnist_train_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Each trainin batch consits of a tuple containing:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_train_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_epoch_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\" - an array of size %s containing %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist_train_stream' is not defined"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95773979,  0.97634202,  0.98737913],\n",
       "       [ 0.99989116,  1.03122523,  0.90607973],\n",
       "       [ 1.16382861,  1.09229816,  0.94595652]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.normal(1, 0.1, (3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please note, the code blow is able to train a SoftMax regression model on mnist to poor results (ca 8%test error), \n",
    "# you must improve it\n",
    "#\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "    return num_errs/num_examples\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream, lamb = 0, epsilon = 0):\n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    #initialize momentum variables\n",
    "    #\n",
    "    # TODO\n",
    "    #\n",
    "    # Hint: you need one valocity matrix for each parameter\n",
    "    velocities = [None for P in net.parameters]\n",
    "    \n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    train_erros = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = 3\n",
    "    patience_expansion = 1.5\n",
    "    \n",
    "    try:\n",
    "        while e<number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            #First train on all data from this batch\n",
    "            for X,Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                train_loss.append((i,L))\n",
    "                train_erros.append((i,err_rate))\n",
    "                if i % 100 == 0:\n",
    "                    print \"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100)\n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: set a learning rate\n",
    "                    #\n",
    "                    # Hint, use the iteration counter i\n",
    "                    # alpha = TODO\n",
    "                    alpha = (i+1.0)/(2.0*i)\n",
    "                    #\n",
    "                    # TODO: set the momentum constant \n",
    "                    # \n",
    "                    \n",
    "                    # epsilon = TODO\n",
    "                    \n",
    "                    #\n",
    "                    # TODO: implement velocity update in momentum\n",
    "                    #\n",
    "                    \n",
    "                    # V[...] = TODO\n",
    "                    if V == None:\n",
    "                        V = 0.\n",
    "                    V = epsilon*(V+G)\n",
    "                    \n",
    "                    if N=='W':\n",
    "                        #\n",
    "                        # TODO: implement the weight decay addition to gradient\n",
    "                        #\n",
    "                        #G += TODO\n",
    "                        G += lamb * P**2\n",
    "                    #\n",
    "                    # TODO: set a more sensible learning rule here,\n",
    "                    # using your learning rate schedule and momentum\n",
    "                    #\n",
    "                    #!!!!! Need to modify the actual parameter here! \n",
    "                    P += -5e-2 * alpha *(G+V)\n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            print \"After epoch %d: valid_err_rate: %f%% currently going ot do %d epochs\" %(\n",
    "                e, val_error_rate, number_of_epochs)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "        \n",
    "        subplot(2,1,1)\n",
    "        train_loss = np.array(train_loss)\n",
    "        semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "        legend()\n",
    "        \n",
    "        subplot(2,1,2)\n",
    "        train_erros = np.array(train_erros)\n",
    "        plot(train_erros[:,0], train_erros[:,1], label='batch train error rate')\n",
    "        validation_errors = np.array(validation_errors)\n",
    "        plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "        ylim(0,0.2)\n",
    "        legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.270604, batch error rate 85.000000%\n",
      "At minibatch 200, batch loss 1.998226, batch error rate 62.000000%\n",
      "At minibatch 300, batch loss 1.781548, batch error rate 59.000000%\n",
      "At minibatch 400, batch loss 1.368889, batch error rate 39.000000%\n",
      "At minibatch 500, batch loss 1.013006, batch error rate 29.000000%\n",
      "After epoch 1: valid_err_rate: 0.292300% currently going ot do 3 epochs\n",
      "At minibatch 600, batch loss 0.808481, batch error rate 18.000000%\n",
      "At minibatch 700, batch loss 0.765417, batch error rate 18.000000%\n",
      "At minibatch 800, batch loss 0.745663, batch error rate 21.000000%\n",
      "At minibatch 900, batch loss 0.790019, batch error rate 23.000000%\n",
      "At minibatch 1000, batch loss 0.527878, batch error rate 13.000000%\n",
      "After epoch 2: valid_err_rate: 0.122800% currently going ot do 4 epochs\n",
      "At minibatch 1100, batch loss 0.449980, batch error rate 10.000000%\n",
      "At minibatch 1200, batch loss 0.551816, batch error rate 13.000000%\n",
      "At minibatch 1300, batch loss 0.461873, batch error rate 9.000000%\n",
      "At minibatch 1400, batch loss 0.535986, batch error rate 15.000000%\n",
      "At minibatch 1500, batch loss 0.342761, batch error rate 5.000000%\n",
      "After epoch 3: valid_err_rate: 0.091600% currently going ot do 5 epochs\n",
      "At minibatch 1600, batch loss 0.322970, batch error rate 7.000000%\n",
      "At minibatch 1700, batch loss 0.381418, batch error rate 9.000000%\n",
      "At minibatch 1800, batch loss 0.450587, batch error rate 17.000000%\n",
      "At minibatch 1900, batch loss 0.493736, batch error rate 16.000000%\n",
      "At minibatch 2000, batch loss 0.442664, batch error rate 13.000000%\n",
      "After epoch 4: valid_err_rate: 0.079800% currently going ot do 7 epochs\n",
      "At minibatch 2100, batch loss 0.407068, batch error rate 11.000000%\n",
      "At minibatch 2200, batch loss 0.547692, batch error rate 15.000000%\n",
      "At minibatch 2300, batch loss 0.332801, batch error rate 8.000000%\n",
      "At minibatch 2400, batch loss 0.438761, batch error rate 13.000000%\n",
      "At minibatch 2500, batch loss 0.164120, batch error rate 3.000000%\n",
      "After epoch 5: valid_err_rate: 0.068900% currently going ot do 8 epochs\n",
      "At minibatch 2600, batch loss 0.266010, batch error rate 9.000000%\n",
      "At minibatch 2700, batch loss 0.227339, batch error rate 8.000000%\n",
      "At minibatch 2800, batch loss 0.221107, batch error rate 5.000000%\n",
      "At minibatch 2900, batch loss 0.253259, batch error rate 6.000000%\n",
      "At minibatch 3000, batch loss 0.261784, batch error rate 8.000000%\n",
      "After epoch 6: valid_err_rate: 0.062100% currently going ot do 10 epochs\n",
      "At minibatch 3100, batch loss 0.257811, batch error rate 8.000000%\n",
      "At minibatch 3200, batch loss 0.240551, batch error rate 7.000000%\n",
      "At minibatch 3300, batch loss 0.140998, batch error rate 3.000000%\n",
      "At minibatch 3400, batch loss 0.139276, batch error rate 4.000000%\n",
      "At minibatch 3500, batch loss 0.228359, batch error rate 6.000000%\n",
      "After epoch 7: valid_err_rate: 0.055800% currently going ot do 11 epochs\n",
      "At minibatch 3600, batch loss 0.154061, batch error rate 3.000000%\n",
      "At minibatch 3700, batch loss 0.293724, batch error rate 6.000000%\n",
      "At minibatch 3800, batch loss 0.202754, batch error rate 5.000000%\n",
      "At minibatch 3900, batch loss 0.125811, batch error rate 4.000000%\n",
      "At minibatch 4000, batch loss 0.235674, batch error rate 7.000000%\n",
      "After epoch 8: valid_err_rate: 0.051300% currently going ot do 13 epochs\n",
      "At minibatch 4100, batch loss 0.196591, batch error rate 5.000000%\n",
      "At minibatch 4200, batch loss 0.124110, batch error rate 4.000000%\n",
      "At minibatch 4300, batch loss 0.195603, batch error rate 4.000000%\n",
      "At minibatch 4400, batch loss 0.115181, batch error rate 3.000000%\n",
      "At minibatch 4500, batch loss 0.282156, batch error rate 7.000000%\n",
      "After epoch 9: valid_err_rate: 0.043800% currently going ot do 14 epochs\n",
      "At minibatch 4600, batch loss 0.221210, batch error rate 7.000000%\n",
      "At minibatch 4700, batch loss 0.149078, batch error rate 6.000000%\n",
      "At minibatch 4800, batch loss 0.124993, batch error rate 4.000000%\n",
      "At minibatch 4900, batch loss 0.130076, batch error rate 3.000000%\n",
      "At minibatch 5000, batch loss 0.181822, batch error rate 6.000000%\n",
      "After epoch 10: valid_err_rate: 0.042500% currently going ot do 16 epochs\n",
      "At minibatch 5100, batch loss 0.218576, batch error rate 10.000000%\n",
      "At minibatch 5200, batch loss 0.165759, batch error rate 5.000000%\n",
      "At minibatch 5300, batch loss 0.100429, batch error rate 2.000000%\n",
      "At minibatch 5400, batch loss 0.176139, batch error rate 6.000000%\n",
      "At minibatch 5500, batch loss 0.163198, batch error rate 7.000000%\n",
      "After epoch 11: valid_err_rate: 0.040500% currently going ot do 17 epochs\n",
      "At minibatch 5600, batch loss 0.090444, batch error rate 2.000000%\n",
      "At minibatch 5700, batch loss 0.060416, batch error rate 2.000000%\n",
      "At minibatch 5800, batch loss 0.174218, batch error rate 4.000000%\n",
      "At minibatch 5900, batch loss 0.133907, batch error rate 4.000000%\n",
      "At minibatch 6000, batch loss 0.096893, batch error rate 4.000000%\n",
      "After epoch 12: valid_err_rate: 0.036800% currently going ot do 19 epochs\n",
      "At minibatch 6100, batch loss 0.179505, batch error rate 5.000000%\n",
      "At minibatch 6200, batch loss 0.045689, batch error rate 1.000000%\n",
      "At minibatch 6300, batch loss 0.070374, batch error rate 1.000000%\n",
      "At minibatch 6400, batch loss 0.120429, batch error rate 5.000000%\n",
      "At minibatch 6500, batch loss 0.180714, batch error rate 5.000000%\n",
      "After epoch 13: valid_err_rate: 0.035400% currently going ot do 20 epochs\n",
      "At minibatch 6600, batch loss 0.133377, batch error rate 3.000000%\n",
      "At minibatch 6700, batch loss 0.094327, batch error rate 3.000000%\n",
      "At minibatch 6800, batch loss 0.062492, batch error rate 2.000000%\n",
      "At minibatch 6900, batch loss 0.167283, batch error rate 3.000000%\n",
      "At minibatch 7000, batch loss 0.146871, batch error rate 6.000000%\n",
      "After epoch 14: valid_err_rate: 0.034400% currently going ot do 22 epochs\n",
      "At minibatch 7100, batch loss 0.069597, batch error rate 1.000000%\n",
      "At minibatch 7200, batch loss 0.076133, batch error rate 1.000000%\n",
      "At minibatch 7300, batch loss 0.070392, batch error rate 1.000000%\n",
      "At minibatch 7400, batch loss 0.150637, batch error rate 5.000000%\n",
      "At minibatch 7500, batch loss 0.049207, batch error rate 1.000000%\n",
      "After epoch 15: valid_err_rate: 0.034900% currently going ot do 22 epochs\n",
      "At minibatch 7600, batch loss 0.069168, batch error rate 2.000000%\n",
      "At minibatch 7700, batch loss 0.051024, batch error rate 2.000000%\n",
      "At minibatch 7800, batch loss 0.071260, batch error rate 3.000000%\n",
      "At minibatch 7900, batch loss 0.125918, batch error rate 2.000000%\n",
      "At minibatch 8000, batch loss 0.057377, batch error rate 2.000000%\n",
      "After epoch 16: valid_err_rate: 0.034100% currently going ot do 25 epochs\n",
      "At minibatch 8100, batch loss 0.060631, batch error rate 1.000000%\n",
      "At minibatch 8200, batch loss 0.144736, batch error rate 4.000000%\n",
      "At minibatch 8300, batch loss 0.106864, batch error rate 2.000000%\n",
      "At minibatch 8400, batch loss 0.091550, batch error rate 4.000000%\n",
      "At minibatch 8500, batch loss 0.200352, batch error rate 8.000000%\n",
      "After epoch 17: valid_err_rate: 0.033000% currently going ot do 26 epochs\n",
      "At minibatch 8600, batch loss 0.109996, batch error rate 3.000000%\n",
      "At minibatch 8700, batch loss 0.169687, batch error rate 6.000000%\n",
      "At minibatch 8800, batch loss 0.060474, batch error rate 3.000000%\n",
      "At minibatch 8900, batch loss 0.062305, batch error rate 2.000000%\n",
      "At minibatch 9000, batch loss 0.083865, batch error rate 2.000000%\n",
      "After epoch 18: valid_err_rate: 0.030300% currently going ot do 28 epochs\n",
      "At minibatch 9100, batch loss 0.059875, batch error rate 1.000000%\n",
      "At minibatch 9200, batch loss 0.119503, batch error rate 6.000000%\n",
      "At minibatch 9300, batch loss 0.049057, batch error rate 1.000000%\n",
      "At minibatch 9400, batch loss 0.143750, batch error rate 3.000000%\n",
      "At minibatch 9500, batch loss 0.086919, batch error rate 1.000000%\n",
      "After epoch 19: valid_err_rate: 0.029200% currently going ot do 29 epochs\n",
      "At minibatch 9600, batch loss 0.052721, batch error rate 1.000000%\n",
      "At minibatch 9700, batch loss 0.048858, batch error rate 1.000000%\n",
      "At minibatch 9800, batch loss 0.112175, batch error rate 5.000000%\n",
      "At minibatch 9900, batch loss 0.133421, batch error rate 3.000000%\n",
      "At minibatch 10000, batch loss 0.129987, batch error rate 4.000000%\n",
      "After epoch 20: valid_err_rate: 0.030900% currently going ot do 29 epochs\n",
      "At minibatch 10100, batch loss 0.044521, batch error rate 1.000000%\n",
      "At minibatch 10200, batch loss 0.027439, batch error rate 1.000000%\n",
      "At minibatch 10300, batch loss 0.024738, batch error rate 0.000000%\n",
      "At minibatch 10400, batch loss 0.105531, batch error rate 3.000000%\n",
      "At minibatch 10500, batch loss 0.077445, batch error rate 2.000000%\n",
      "After epoch 21: valid_err_rate: 0.027800% currently going ot do 32 epochs\n",
      "At minibatch 10600, batch loss 0.077289, batch error rate 1.000000%\n",
      "At minibatch 10700, batch loss 0.090073, batch error rate 3.000000%\n",
      "At minibatch 10800, batch loss 0.040733, batch error rate 2.000000%\n",
      "At minibatch 10900, batch loss 0.028881, batch error rate 1.000000%\n",
      "At minibatch 11000, batch loss 0.068485, batch error rate 3.000000%\n",
      "After epoch 22: valid_err_rate: 0.028600% currently going ot do 32 epochs\n",
      "At minibatch 11100, batch loss 0.027296, batch error rate 0.000000%\n",
      "At minibatch 11200, batch loss 0.040235, batch error rate 1.000000%\n",
      "At minibatch 11300, batch loss 0.041630, batch error rate 1.000000%\n",
      "At minibatch 11400, batch loss 0.042503, batch error rate 1.000000%\n",
      "At minibatch 11500, batch loss 0.040538, batch error rate 1.000000%\n",
      "After epoch 23: valid_err_rate: 0.027200% currently going ot do 35 epochs\n",
      "At minibatch 11600, batch loss 0.096935, batch error rate 2.000000%\n",
      "At minibatch 11700, batch loss 0.050818, batch error rate 1.000000%\n",
      "At minibatch 11800, batch loss 0.083752, batch error rate 3.000000%\n",
      "At minibatch 11900, batch loss 0.039018, batch error rate 0.000000%\n",
      "At minibatch 12000, batch loss 0.060336, batch error rate 1.000000%\n",
      "After epoch 24: valid_err_rate: 0.028400% currently going ot do 35 epochs\n",
      "At minibatch 12100, batch loss 0.095842, batch error rate 3.000000%\n",
      "At minibatch 12200, batch loss 0.064800, batch error rate 2.000000%\n",
      "At minibatch 12300, batch loss 0.146383, batch error rate 5.000000%\n",
      "At minibatch 12400, batch loss 0.060867, batch error rate 2.000000%\n",
      "At minibatch 12500, batch loss 0.064737, batch error rate 3.000000%\n",
      "After epoch 25: valid_err_rate: 0.028600% currently going ot do 35 epochs\n",
      "At minibatch 12600, batch loss 0.090395, batch error rate 2.000000%\n",
      "At minibatch 12700, batch loss 0.049268, batch error rate 3.000000%\n",
      "At minibatch 12800, batch loss 0.017142, batch error rate 0.000000%\n",
      "At minibatch 12900, batch loss 0.070312, batch error rate 4.000000%\n",
      "At minibatch 13000, batch loss 0.025503, batch error rate 0.000000%\n",
      "After epoch 26: valid_err_rate: 0.027100% currently going ot do 40 epochs\n",
      "At minibatch 13100, batch loss 0.040672, batch error rate 0.000000%\n",
      "At minibatch 13200, batch loss 0.028754, batch error rate 0.000000%\n",
      "At minibatch 13300, batch loss 0.037310, batch error rate 1.000000%\n",
      "At minibatch 13400, batch loss 0.025889, batch error rate 0.000000%\n",
      "At minibatch 13500, batch loss 0.041373, batch error rate 1.000000%\n",
      "After epoch 27: valid_err_rate: 0.026300% currently going ot do 41 epochs\n",
      "At minibatch 13600, batch loss 0.024439, batch error rate 1.000000%\n",
      "At minibatch 13700, batch loss 0.084714, batch error rate 2.000000%\n",
      "At minibatch 13800, batch loss 0.093856, batch error rate 2.000000%\n",
      "At minibatch 13900, batch loss 0.035083, batch error rate 1.000000%\n",
      "At minibatch 14000, batch loss 0.022166, batch error rate 0.000000%\n",
      "After epoch 28: valid_err_rate: 0.025900% currently going ot do 43 epochs\n",
      "At minibatch 14100, batch loss 0.075573, batch error rate 4.000000%\n",
      "At minibatch 14200, batch loss 0.028507, batch error rate 1.000000%\n",
      "At minibatch 14300, batch loss 0.127183, batch error rate 3.000000%\n",
      "At minibatch 14400, batch loss 0.071648, batch error rate 3.000000%\n",
      "At minibatch 14500, batch loss 0.034708, batch error rate 1.000000%\n",
      "After epoch 29: valid_err_rate: 0.027700% currently going ot do 43 epochs\n",
      "At minibatch 14600, batch loss 0.030761, batch error rate 1.000000%\n",
      "At minibatch 14700, batch loss 0.048989, batch error rate 2.000000%\n",
      "At minibatch 14800, batch loss 0.082532, batch error rate 3.000000%\n",
      "At minibatch 14900, batch loss 0.108286, batch error rate 4.000000%\n",
      "At minibatch 15000, batch loss 0.037002, batch error rate 2.000000%\n",
      "After epoch 30: valid_err_rate: 0.024600% currently going ot do 46 epochs\n",
      "At minibatch 15100, batch loss 0.113074, batch error rate 3.000000%\n",
      "At minibatch 15200, batch loss 0.048505, batch error rate 2.000000%\n",
      "At minibatch 15300, batch loss 0.020084, batch error rate 0.000000%\n",
      "At minibatch 15400, batch loss 0.039321, batch error rate 1.000000%\n",
      "At minibatch 15500, batch loss 0.027779, batch error rate 0.000000%\n",
      "After epoch 31: valid_err_rate: 0.025600% currently going ot do 46 epochs\n",
      "At minibatch 15600, batch loss 0.032177, batch error rate 0.000000%\n",
      "At minibatch 15700, batch loss 0.065358, batch error rate 1.000000%\n",
      "At minibatch 15800, batch loss 0.024535, batch error rate 1.000000%\n",
      "At minibatch 15900, batch loss 0.106867, batch error rate 5.000000%\n",
      "At minibatch 16000, batch loss 0.050047, batch error rate 2.000000%\n",
      "After epoch 32: valid_err_rate: 0.026000% currently going ot do 46 epochs\n",
      "At minibatch 16100, batch loss 0.025409, batch error rate 0.000000%\n",
      "At minibatch 16200, batch loss 0.075025, batch error rate 1.000000%\n",
      "At minibatch 16300, batch loss 0.045751, batch error rate 1.000000%\n",
      "At minibatch 16400, batch loss 0.011423, batch error rate 0.000000%\n",
      "At minibatch 16500, batch loss 0.024506, batch error rate 0.000000%\n",
      "After epoch 33: valid_err_rate: 0.025000% currently going ot do 46 epochs\n",
      "At minibatch 16600, batch loss 0.052200, batch error rate 1.000000%\n",
      "At minibatch 16700, batch loss 0.020565, batch error rate 0.000000%\n",
      "At minibatch 16800, batch loss 0.019344, batch error rate 0.000000%\n",
      "At minibatch 16900, batch loss 0.013262, batch error rate 0.000000%\n",
      "At minibatch 17000, batch loss 0.037500, batch error rate 1.000000%\n",
      "After epoch 34: valid_err_rate: 0.025700% currently going ot do 46 epochs\n",
      "At minibatch 17100, batch loss 0.020545, batch error rate 0.000000%\n",
      "At minibatch 17200, batch loss 0.009488, batch error rate 0.000000%\n",
      "At minibatch 17300, batch loss 0.035885, batch error rate 0.000000%\n",
      "At minibatch 17400, batch loss 0.033505, batch error rate 0.000000%\n",
      "At minibatch 17500, batch loss 0.052951, batch error rate 0.000000%\n",
      "After epoch 35: valid_err_rate: 0.025200% currently going ot do 46 epochs\n",
      "At minibatch 17600, batch loss 0.063442, batch error rate 1.000000%\n",
      "At minibatch 17700, batch loss 0.011706, batch error rate 0.000000%\n",
      "At minibatch 17800, batch loss 0.093845, batch error rate 1.000000%\n",
      "At minibatch 17900, batch loss 0.027957, batch error rate 0.000000%\n",
      "At minibatch 18000, batch loss 0.026935, batch error rate 1.000000%\n",
      "After epoch 36: valid_err_rate: 0.025300% currently going ot do 46 epochs\n",
      "At minibatch 18100, batch loss 0.017692, batch error rate 0.000000%\n",
      "At minibatch 18200, batch loss 0.044021, batch error rate 2.000000%\n",
      "At minibatch 18300, batch loss 0.019946, batch error rate 0.000000%\n",
      "At minibatch 18400, batch loss 0.026406, batch error rate 0.000000%\n",
      "At minibatch 18500, batch loss 0.068670, batch error rate 2.000000%\n",
      "After epoch 37: valid_err_rate: 0.025700% currently going ot do 46 epochs\n",
      "At minibatch 18600, batch loss 0.064857, batch error rate 1.000000%\n",
      "At minibatch 18700, batch loss 0.041601, batch error rate 2.000000%\n",
      "At minibatch 18800, batch loss 0.060576, batch error rate 1.000000%\n",
      "At minibatch 18900, batch loss 0.044526, batch error rate 3.000000%\n",
      "At minibatch 19000, batch loss 0.016716, batch error rate 0.000000%\n",
      "After epoch 38: valid_err_rate: 0.025300% currently going ot do 46 epochs\n",
      "At minibatch 19100, batch loss 0.027528, batch error rate 0.000000%\n",
      "At minibatch 19200, batch loss 0.026930, batch error rate 0.000000%\n",
      "At minibatch 19300, batch loss 0.035596, batch error rate 1.000000%\n",
      "At minibatch 19400, batch loss 0.014168, batch error rate 0.000000%\n",
      "At minibatch 19500, batch loss 0.020081, batch error rate 0.000000%\n",
      "After epoch 39: valid_err_rate: 0.023900% currently going ot do 59 epochs\n",
      "At minibatch 19600, batch loss 0.020155, batch error rate 0.000000%\n",
      "At minibatch 19700, batch loss 0.055645, batch error rate 2.000000%\n",
      "At minibatch 19800, batch loss 0.035116, batch error rate 1.000000%\n",
      "At minibatch 19900, batch loss 0.020463, batch error rate 0.000000%\n",
      "At minibatch 20000, batch loss 0.037401, batch error rate 1.000000%\n",
      "After epoch 40: valid_err_rate: 0.024300% currently going ot do 59 epochs\n",
      "At minibatch 20100, batch loss 0.033178, batch error rate 1.000000%\n",
      "At minibatch 20200, batch loss 0.041963, batch error rate 2.000000%\n",
      "At minibatch 20300, batch loss 0.071939, batch error rate 2.000000%\n",
      "At minibatch 20400, batch loss 0.025635, batch error rate 0.000000%\n",
      "At minibatch 20500, batch loss 0.073276, batch error rate 2.000000%\n",
      "After epoch 41: valid_err_rate: 0.026300% currently going ot do 59 epochs\n",
      "At minibatch 20600, batch loss 0.071244, batch error rate 4.000000%\n",
      "At minibatch 20700, batch loss 0.024008, batch error rate 0.000000%\n",
      "At minibatch 20800, batch loss 0.034242, batch error rate 1.000000%\n",
      "At minibatch 20900, batch loss 0.038973, batch error rate 1.000000%\n",
      "At minibatch 21000, batch loss 0.032492, batch error rate 1.000000%\n",
      "After epoch 42: valid_err_rate: 0.024300% currently going ot do 59 epochs\n",
      "At minibatch 21100, batch loss 0.031100, batch error rate 1.000000%\n",
      "At minibatch 21200, batch loss 0.011697, batch error rate 0.000000%\n",
      "At minibatch 21300, batch loss 0.129768, batch error rate 2.000000%\n",
      "At minibatch 21400, batch loss 0.061475, batch error rate 1.000000%\n",
      "At minibatch 21500, batch loss 0.021059, batch error rate 0.000000%\n",
      "After epoch 43: valid_err_rate: 0.024700% currently going ot do 59 epochs\n",
      "At minibatch 21600, batch loss 0.068700, batch error rate 3.000000%\n",
      "At minibatch 21700, batch loss 0.024139, batch error rate 1.000000%\n",
      "At minibatch 21800, batch loss 0.080100, batch error rate 2.000000%\n",
      "At minibatch 21900, batch loss 0.011642, batch error rate 0.000000%\n",
      "At minibatch 22000, batch loss 0.048727, batch error rate 1.000000%\n",
      "After epoch 44: valid_err_rate: 0.024800% currently going ot do 59 epochs\n",
      "At minibatch 22100, batch loss 0.044135, batch error rate 1.000000%\n",
      "At minibatch 22200, batch loss 0.073248, batch error rate 1.000000%\n",
      "At minibatch 22300, batch loss 0.059274, batch error rate 1.000000%\n",
      "At minibatch 22400, batch loss 0.014452, batch error rate 0.000000%\n",
      "At minibatch 22500, batch loss 0.015214, batch error rate 0.000000%\n",
      "After epoch 45: valid_err_rate: 0.023800% currently going ot do 68 epochs\n",
      "At minibatch 22600, batch loss 0.010469, batch error rate 0.000000%\n",
      "At minibatch 22700, batch loss 0.035595, batch error rate 1.000000%\n",
      "At minibatch 22800, batch loss 0.088325, batch error rate 2.000000%\n",
      "At minibatch 22900, batch loss 0.019950, batch error rate 0.000000%\n",
      "At minibatch 23000, batch loss 0.040824, batch error rate 1.000000%\n",
      "After epoch 46: valid_err_rate: 0.023000% currently going ot do 70 epochs\n",
      "At minibatch 23100, batch loss 0.015906, batch error rate 0.000000%\n",
      "At minibatch 23200, batch loss 0.017163, batch error rate 0.000000%\n",
      "At minibatch 23300, batch loss 0.008068, batch error rate 0.000000%\n",
      "At minibatch 23400, batch loss 0.053578, batch error rate 1.000000%\n",
      "At minibatch 23500, batch loss 0.014013, batch error rate 0.000000%\n",
      "After epoch 47: valid_err_rate: 0.023300% currently going ot do 70 epochs\n",
      "At minibatch 23600, batch loss 0.054490, batch error rate 2.000000%\n",
      "At minibatch 23700, batch loss 0.009670, batch error rate 0.000000%\n",
      "At minibatch 23800, batch loss 0.032073, batch error rate 1.000000%\n",
      "At minibatch 23900, batch loss 0.010843, batch error rate 0.000000%\n",
      "At minibatch 24000, batch loss 0.021278, batch error rate 0.000000%\n",
      "After epoch 48: valid_err_rate: 0.022900% currently going ot do 73 epochs\n",
      "At minibatch 24100, batch loss 0.033088, batch error rate 1.000000%\n",
      "At minibatch 24200, batch loss 0.038625, batch error rate 1.000000%\n",
      "At minibatch 24300, batch loss 0.020327, batch error rate 0.000000%\n",
      "At minibatch 24400, batch loss 0.039821, batch error rate 1.000000%\n",
      "At minibatch 24500, batch loss 0.015028, batch error rate 0.000000%\n",
      "After epoch 49: valid_err_rate: 0.023000% currently going ot do 73 epochs\n",
      "At minibatch 24600, batch loss 0.027292, batch error rate 0.000000%\n",
      "At minibatch 24700, batch loss 0.026115, batch error rate 0.000000%\n",
      "At minibatch 24800, batch loss 0.039858, batch error rate 2.000000%\n",
      "At minibatch 24900, batch loss 0.056287, batch error rate 2.000000%\n",
      "At minibatch 25000, batch loss 0.016231, batch error rate 0.000000%\n",
      "After epoch 50: valid_err_rate: 0.023800% currently going ot do 73 epochs\n",
      "At minibatch 25100, batch loss 0.011299, batch error rate 0.000000%\n",
      "At minibatch 25200, batch loss 0.012678, batch error rate 0.000000%\n",
      "At minibatch 25300, batch loss 0.013596, batch error rate 0.000000%\n",
      "At minibatch 25400, batch loss 0.034951, batch error rate 1.000000%\n",
      "At minibatch 25500, batch loss 0.013609, batch error rate 0.000000%\n",
      "After epoch 51: valid_err_rate: 0.023600% currently going ot do 73 epochs\n",
      "At minibatch 25600, batch loss 0.100909, batch error rate 2.000000%\n",
      "At minibatch 25700, batch loss 0.014052, batch error rate 0.000000%\n",
      "At minibatch 25800, batch loss 0.022152, batch error rate 0.000000%\n",
      "At minibatch 25900, batch loss 0.013817, batch error rate 0.000000%\n",
      "At minibatch 26000, batch loss 0.008831, batch error rate 0.000000%\n",
      "After epoch 52: valid_err_rate: 0.024900% currently going ot do 73 epochs\n",
      "At minibatch 26100, batch loss 0.009119, batch error rate 0.000000%\n",
      "At minibatch 26200, batch loss 0.016889, batch error rate 0.000000%\n",
      "At minibatch 26300, batch loss 0.023610, batch error rate 1.000000%\n",
      "At minibatch 26400, batch loss 0.009243, batch error rate 0.000000%\n",
      "At minibatch 26500, batch loss 0.091986, batch error rate 1.000000%\n",
      "After epoch 53: valid_err_rate: 0.024300% currently going ot do 73 epochs\n",
      "At minibatch 26600, batch loss 0.004645, batch error rate 0.000000%\n",
      "At minibatch 26700, batch loss 0.013960, batch error rate 0.000000%\n",
      "At minibatch 26800, batch loss 0.010948, batch error rate 0.000000%\n",
      "At minibatch 26900, batch loss 0.037237, batch error rate 2.000000%\n",
      "At minibatch 27000, batch loss 0.019158, batch error rate 1.000000%\n",
      "After epoch 54: valid_err_rate: 0.024000% currently going ot do 73 epochs\n",
      "At minibatch 27100, batch loss 0.019974, batch error rate 0.000000%\n",
      "At minibatch 27200, batch loss 0.039232, batch error rate 1.000000%\n",
      "At minibatch 27300, batch loss 0.014847, batch error rate 0.000000%\n",
      "At minibatch 27400, batch loss 0.012482, batch error rate 0.000000%\n",
      "At minibatch 27500, batch loss 0.016070, batch error rate 1.000000%\n",
      "After epoch 55: valid_err_rate: 0.024300% currently going ot do 73 epochs\n",
      "At minibatch 27600, batch loss 0.021312, batch error rate 0.000000%\n",
      "At minibatch 27700, batch loss 0.020143, batch error rate 0.000000%\n",
      "At minibatch 27800, batch loss 0.028096, batch error rate 1.000000%\n",
      "At minibatch 27900, batch loss 0.008835, batch error rate 0.000000%\n",
      "At minibatch 28000, batch loss 0.016348, batch error rate 0.000000%\n",
      "After epoch 56: valid_err_rate: 0.023400% currently going ot do 73 epochs\n",
      "At minibatch 28100, batch loss 0.010335, batch error rate 0.000000%\n",
      "At minibatch 28200, batch loss 0.064809, batch error rate 1.000000%\n",
      "At minibatch 28300, batch loss 0.022363, batch error rate 1.000000%\n",
      "At minibatch 28400, batch loss 0.012908, batch error rate 0.000000%\n",
      "At minibatch 28500, batch loss 0.041419, batch error rate 1.000000%\n",
      "After epoch 57: valid_err_rate: 0.025600% currently going ot do 73 epochs\n",
      "At minibatch 28600, batch loss 0.016282, batch error rate 1.000000%\n",
      "At minibatch 28700, batch loss 0.014408, batch error rate 0.000000%\n",
      "At minibatch 28800, batch loss 0.027041, batch error rate 1.000000%\n",
      "At minibatch 28900, batch loss 0.010641, batch error rate 0.000000%\n",
      "At minibatch 29000, batch loss 0.015663, batch error rate 0.000000%\n",
      "After epoch 58: valid_err_rate: 0.023500% currently going ot do 73 epochs\n",
      "At minibatch 29100, batch loss 0.033175, batch error rate 1.000000%\n",
      "At minibatch 29200, batch loss 0.042700, batch error rate 2.000000%\n",
      "At minibatch 29300, batch loss 0.017396, batch error rate 0.000000%\n",
      "At minibatch 29400, batch loss 0.023416, batch error rate 1.000000%\n",
      "At minibatch 29500, batch loss 0.007061, batch error rate 0.000000%\n",
      "After epoch 59: valid_err_rate: 0.024000% currently going ot do 73 epochs\n",
      "At minibatch 29600, batch loss 0.041286, batch error rate 1.000000%\n",
      "At minibatch 29700, batch loss 0.009993, batch error rate 0.000000%\n",
      "At minibatch 29800, batch loss 0.014199, batch error rate 0.000000%\n",
      "At minibatch 29900, batch loss 0.010901, batch error rate 0.000000%\n",
      "At minibatch 30000, batch loss 0.008286, batch error rate 0.000000%\n",
      "After epoch 60: valid_err_rate: 0.022500% currently going ot do 91 epochs\n",
      "At minibatch 30100, batch loss 0.006165, batch error rate 0.000000%\n",
      "At minibatch 30200, batch loss 0.031829, batch error rate 1.000000%\n",
      "At minibatch 30300, batch loss 0.018482, batch error rate 0.000000%\n",
      "At minibatch 30400, batch loss 0.023862, batch error rate 1.000000%\n",
      "At minibatch 30500, batch loss 0.004423, batch error rate 0.000000%\n",
      "After epoch 61: valid_err_rate: 0.022000% currently going ot do 92 epochs\n",
      "At minibatch 30600, batch loss 0.015099, batch error rate 1.000000%\n",
      "At minibatch 30700, batch loss 0.012097, batch error rate 0.000000%\n",
      "At minibatch 30800, batch loss 0.015039, batch error rate 0.000000%\n",
      "At minibatch 30900, batch loss 0.005021, batch error rate 0.000000%\n",
      "At minibatch 31000, batch loss 0.012459, batch error rate 0.000000%\n",
      "After epoch 62: valid_err_rate: 0.023800% currently going ot do 92 epochs\n",
      "At minibatch 31100, batch loss 0.077534, batch error rate 1.000000%\n",
      "At minibatch 31200, batch loss 0.008307, batch error rate 0.000000%\n",
      "At minibatch 31300, batch loss 0.005216, batch error rate 0.000000%\n",
      "At minibatch 31400, batch loss 0.005137, batch error rate 0.000000%\n",
      "At minibatch 31500, batch loss 0.022269, batch error rate 0.000000%\n",
      "After epoch 63: valid_err_rate: 0.021800% currently going ot do 95 epochs\n",
      "At minibatch 31600, batch loss 0.010446, batch error rate 0.000000%\n",
      "At minibatch 31700, batch loss 0.036519, batch error rate 1.000000%\n",
      "At minibatch 31800, batch loss 0.008391, batch error rate 0.000000%\n",
      "At minibatch 31900, batch loss 0.010862, batch error rate 0.000000%\n",
      "At minibatch 32000, batch loss 0.014785, batch error rate 1.000000%\n",
      "After epoch 64: valid_err_rate: 0.022300% currently going ot do 95 epochs\n",
      "At minibatch 32100, batch loss 0.014568, batch error rate 0.000000%\n",
      "At minibatch 32200, batch loss 0.014982, batch error rate 0.000000%\n",
      "At minibatch 32300, batch loss 0.013142, batch error rate 0.000000%\n",
      "At minibatch 32400, batch loss 0.007345, batch error rate 0.000000%\n",
      "At minibatch 32500, batch loss 0.017100, batch error rate 0.000000%\n",
      "After epoch 65: valid_err_rate: 0.022700% currently going ot do 95 epochs\n",
      "At minibatch 32600, batch loss 0.012499, batch error rate 1.000000%\n",
      "At minibatch 32700, batch loss 0.009886, batch error rate 0.000000%\n",
      "At minibatch 32800, batch loss 0.011135, batch error rate 0.000000%\n",
      "At minibatch 32900, batch loss 0.003854, batch error rate 0.000000%\n",
      "At minibatch 33000, batch loss 0.006122, batch error rate 0.000000%\n",
      "After epoch 66: valid_err_rate: 0.021800% currently going ot do 95 epochs\n",
      "At minibatch 33100, batch loss 0.013098, batch error rate 0.000000%\n",
      "At minibatch 33200, batch loss 0.033025, batch error rate 1.000000%\n",
      "At minibatch 33300, batch loss 0.016561, batch error rate 0.000000%\n",
      "At minibatch 33400, batch loss 0.014186, batch error rate 0.000000%\n",
      "At minibatch 33500, batch loss 0.017940, batch error rate 0.000000%\n",
      "After epoch 67: valid_err_rate: 0.023500% currently going ot do 95 epochs\n",
      "At minibatch 33600, batch loss 0.006276, batch error rate 0.000000%\n",
      "At minibatch 33700, batch loss 0.013398, batch error rate 1.000000%\n",
      "At minibatch 33800, batch loss 0.009510, batch error rate 0.000000%\n",
      "At minibatch 33900, batch loss 0.011944, batch error rate 0.000000%\n",
      "At minibatch 34000, batch loss 0.016322, batch error rate 1.000000%\n",
      "After epoch 68: valid_err_rate: 0.024100% currently going ot do 95 epochs\n",
      "At minibatch 34100, batch loss 0.022989, batch error rate 1.000000%\n",
      "At minibatch 34200, batch loss 0.014306, batch error rate 0.000000%\n",
      "At minibatch 34300, batch loss 0.013414, batch error rate 0.000000%\n",
      "At minibatch 34400, batch loss 0.018680, batch error rate 0.000000%\n",
      "At minibatch 34500, batch loss 0.010569, batch error rate 0.000000%\n",
      "After epoch 69: valid_err_rate: 0.021500% currently going ot do 104 epochs\n",
      "At minibatch 34600, batch loss 0.005584, batch error rate 0.000000%\n",
      "At minibatch 34700, batch loss 0.013217, batch error rate 0.000000%\n",
      "At minibatch 34800, batch loss 0.011895, batch error rate 0.000000%\n",
      "At minibatch 34900, batch loss 0.023723, batch error rate 1.000000%\n",
      "At minibatch 35000, batch loss 0.018541, batch error rate 0.000000%\n",
      "After epoch 70: valid_err_rate: 0.022400% currently going ot do 104 epochs\n",
      "At minibatch 35100, batch loss 0.005602, batch error rate 0.000000%\n",
      "At minibatch 35200, batch loss 0.012499, batch error rate 0.000000%\n",
      "At minibatch 35300, batch loss 0.007261, batch error rate 0.000000%\n",
      "At minibatch 35400, batch loss 0.009011, batch error rate 0.000000%\n",
      "At minibatch 35500, batch loss 0.011011, batch error rate 0.000000%\n",
      "After epoch 71: valid_err_rate: 0.021800% currently going ot do 104 epochs\n",
      "At minibatch 35600, batch loss 0.006813, batch error rate 0.000000%\n",
      "At minibatch 35700, batch loss 0.007987, batch error rate 0.000000%\n",
      "At minibatch 35800, batch loss 0.009717, batch error rate 0.000000%\n",
      "At minibatch 35900, batch loss 0.020582, batch error rate 1.000000%\n",
      "At minibatch 36000, batch loss 0.007162, batch error rate 0.000000%\n",
      "After epoch 72: valid_err_rate: 0.022600% currently going ot do 104 epochs\n",
      "At minibatch 36100, batch loss 0.014372, batch error rate 0.000000%\n",
      "At minibatch 36200, batch loss 0.006494, batch error rate 0.000000%\n",
      "At minibatch 36300, batch loss 0.006695, batch error rate 0.000000%\n",
      "At minibatch 36400, batch loss 0.017462, batch error rate 0.000000%\n",
      "At minibatch 36500, batch loss 0.003783, batch error rate 0.000000%\n",
      "After epoch 73: valid_err_rate: 0.022900% currently going ot do 104 epochs\n",
      "At minibatch 36600, batch loss 0.008452, batch error rate 0.000000%\n",
      "At minibatch 36700, batch loss 0.015126, batch error rate 0.000000%\n",
      "At minibatch 36800, batch loss 0.005280, batch error rate 0.000000%\n",
      "At minibatch 36900, batch loss 0.006761, batch error rate 0.000000%\n",
      "At minibatch 37000, batch loss 0.004915, batch error rate 0.000000%\n",
      "After epoch 74: valid_err_rate: 0.022600% currently going ot do 104 epochs\n",
      "At minibatch 37100, batch loss 0.008498, batch error rate 0.000000%\n",
      "At minibatch 37200, batch loss 0.009068, batch error rate 0.000000%\n",
      "At minibatch 37300, batch loss 0.005896, batch error rate 0.000000%\n",
      "At minibatch 37400, batch loss 0.024757, batch error rate 0.000000%\n",
      "At minibatch 37500, batch loss 0.013369, batch error rate 0.000000%\n",
      "After epoch 75: valid_err_rate: 0.022600% currently going ot do 104 epochs\n",
      "At minibatch 37600, batch loss 0.010221, batch error rate 0.000000%\n",
      "At minibatch 37700, batch loss 0.072180, batch error rate 1.000000%\n",
      "At minibatch 37800, batch loss 0.005769, batch error rate 0.000000%\n",
      "At minibatch 37900, batch loss 0.005297, batch error rate 0.000000%\n",
      "At minibatch 38000, batch loss 0.006372, batch error rate 0.000000%\n",
      "After epoch 76: valid_err_rate: 0.021600% currently going ot do 104 epochs\n",
      "At minibatch 38100, batch loss 0.040595, batch error rate 2.000000%\n",
      "At minibatch 38200, batch loss 0.006669, batch error rate 0.000000%\n",
      "At minibatch 38300, batch loss 0.006576, batch error rate 0.000000%\n",
      "At minibatch 38400, batch loss 0.015688, batch error rate 0.000000%\n",
      "At minibatch 38500, batch loss 0.009570, batch error rate 0.000000%\n",
      "After epoch 77: valid_err_rate: 0.023400% currently going ot do 104 epochs\n",
      "At minibatch 38600, batch loss 0.006252, batch error rate 0.000000%\n",
      "At minibatch 38700, batch loss 0.010833, batch error rate 0.000000%\n",
      "At minibatch 38800, batch loss 0.021050, batch error rate 1.000000%\n",
      "At minibatch 38900, batch loss 0.011306, batch error rate 0.000000%\n",
      "At minibatch 39000, batch loss 0.011312, batch error rate 0.000000%\n",
      "After epoch 78: valid_err_rate: 0.021600% currently going ot do 104 epochs\n",
      "At minibatch 39100, batch loss 0.029065, batch error rate 1.000000%\n",
      "At minibatch 39200, batch loss 0.006018, batch error rate 0.000000%\n",
      "At minibatch 39300, batch loss 0.007040, batch error rate 0.000000%\n",
      "At minibatch 39400, batch loss 0.009074, batch error rate 0.000000%\n",
      "At minibatch 39500, batch loss 0.004623, batch error rate 0.000000%\n",
      "After epoch 79: valid_err_rate: 0.021200% currently going ot do 119 epochs\n",
      "At minibatch 39600, batch loss 0.004563, batch error rate 0.000000%\n",
      "At minibatch 39700, batch loss 0.015697, batch error rate 0.000000%\n",
      "At minibatch 39800, batch loss 0.012663, batch error rate 0.000000%\n",
      "At minibatch 39900, batch loss 0.010310, batch error rate 0.000000%\n",
      "At minibatch 40000, batch loss 0.010477, batch error rate 0.000000%\n",
      "After epoch 80: valid_err_rate: 0.021100% currently going ot do 121 epochs\n",
      "At minibatch 40100, batch loss 0.006277, batch error rate 0.000000%\n",
      "At minibatch 40200, batch loss 0.006958, batch error rate 0.000000%\n",
      "At minibatch 40300, batch loss 0.004249, batch error rate 0.000000%\n",
      "At minibatch 40400, batch loss 0.021506, batch error rate 0.000000%\n",
      "At minibatch 40500, batch loss 0.005264, batch error rate 0.000000%\n",
      "After epoch 81: valid_err_rate: 0.021100% currently going ot do 121 epochs\n",
      "At minibatch 40600, batch loss 0.007621, batch error rate 0.000000%\n",
      "At minibatch 40700, batch loss 0.005701, batch error rate 0.000000%\n",
      "At minibatch 40800, batch loss 0.009130, batch error rate 0.000000%\n",
      "At minibatch 40900, batch loss 0.008052, batch error rate 0.000000%\n",
      "At minibatch 41000, batch loss 0.009328, batch error rate 0.000000%\n",
      "After epoch 82: valid_err_rate: 0.020400% currently going ot do 124 epochs\n",
      "At minibatch 41100, batch loss 0.004056, batch error rate 0.000000%\n",
      "At minibatch 41200, batch loss 0.010256, batch error rate 0.000000%\n",
      "At minibatch 41300, batch loss 0.007471, batch error rate 0.000000%\n",
      "At minibatch 41400, batch loss 0.007194, batch error rate 0.000000%\n",
      "At minibatch 41500, batch loss 0.003888, batch error rate 0.000000%\n",
      "After epoch 83: valid_err_rate: 0.021500% currently going ot do 124 epochs\n",
      "At minibatch 41600, batch loss 0.013165, batch error rate 0.000000%\n",
      "At minibatch 41700, batch loss 0.007746, batch error rate 0.000000%\n",
      "At minibatch 41800, batch loss 0.009129, batch error rate 0.000000%\n",
      "At minibatch 41900, batch loss 0.006119, batch error rate 0.000000%\n",
      "At minibatch 42000, batch loss 0.003003, batch error rate 0.000000%\n",
      "After epoch 84: valid_err_rate: 0.022400% currently going ot do 124 epochs\n",
      "At minibatch 42100, batch loss 0.006562, batch error rate 0.000000%\n",
      "At minibatch 42200, batch loss 0.014713, batch error rate 0.000000%\n",
      "At minibatch 42300, batch loss 0.008314, batch error rate 0.000000%\n",
      "At minibatch 42400, batch loss 0.003794, batch error rate 0.000000%\n",
      "At minibatch 42500, batch loss 0.006878, batch error rate 0.000000%\n",
      "After epoch 85: valid_err_rate: 0.021300% currently going ot do 124 epochs\n",
      "At minibatch 42600, batch loss 0.010395, batch error rate 0.000000%\n",
      "At minibatch 42700, batch loss 0.016489, batch error rate 0.000000%\n",
      "At minibatch 42800, batch loss 0.005163, batch error rate 0.000000%\n",
      "At minibatch 42900, batch loss 0.007536, batch error rate 0.000000%\n",
      "At minibatch 43000, batch loss 0.003308, batch error rate 0.000000%\n",
      "After epoch 86: valid_err_rate: 0.021700% currently going ot do 124 epochs\n",
      "At minibatch 43100, batch loss 0.007390, batch error rate 0.000000%\n",
      "At minibatch 43200, batch loss 0.003167, batch error rate 0.000000%\n",
      "At minibatch 43300, batch loss 0.008186, batch error rate 0.000000%\n",
      "At minibatch 43400, batch loss 0.008131, batch error rate 0.000000%\n",
      "At minibatch 43500, batch loss 0.019941, batch error rate 1.000000%\n",
      "After epoch 87: valid_err_rate: 0.021900% currently going ot do 124 epochs\n",
      "At minibatch 43600, batch loss 0.004628, batch error rate 0.000000%\n",
      "At minibatch 43700, batch loss 0.018075, batch error rate 0.000000%\n",
      "At minibatch 43800, batch loss 0.013574, batch error rate 1.000000%\n",
      "At minibatch 43900, batch loss 0.001948, batch error rate 0.000000%\n",
      "At minibatch 44000, batch loss 0.009221, batch error rate 0.000000%\n",
      "After epoch 88: valid_err_rate: 0.021200% currently going ot do 124 epochs\n",
      "At minibatch 44100, batch loss 0.004767, batch error rate 0.000000%\n",
      "At minibatch 44200, batch loss 0.005197, batch error rate 0.000000%\n",
      "At minibatch 44300, batch loss 0.010069, batch error rate 1.000000%\n",
      "At minibatch 44400, batch loss 0.011024, batch error rate 0.000000%\n",
      "At minibatch 44500, batch loss 0.009681, batch error rate 0.000000%\n",
      "After epoch 89: valid_err_rate: 0.021500% currently going ot do 124 epochs\n",
      "At minibatch 44600, batch loss 0.006488, batch error rate 0.000000%\n",
      "At minibatch 44700, batch loss 0.004540, batch error rate 0.000000%\n",
      "At minibatch 44800, batch loss 0.011635, batch error rate 0.000000%\n",
      "At minibatch 44900, batch loss 0.006189, batch error rate 0.000000%\n",
      "At minibatch 45000, batch loss 0.008184, batch error rate 0.000000%\n",
      "After epoch 90: valid_err_rate: 0.020900% currently going ot do 124 epochs\n",
      "At minibatch 45100, batch loss 0.006124, batch error rate 0.000000%\n",
      "At minibatch 45200, batch loss 0.014042, batch error rate 0.000000%\n",
      "At minibatch 45300, batch loss 0.003433, batch error rate 0.000000%\n",
      "At minibatch 45400, batch loss 0.008238, batch error rate 0.000000%\n",
      "At minibatch 45500, batch loss 0.011703, batch error rate 0.000000%\n",
      "After epoch 91: valid_err_rate: 0.021300% currently going ot do 124 epochs\n",
      "At minibatch 45600, batch loss 0.022644, batch error rate 1.000000%\n",
      "At minibatch 45700, batch loss 0.005714, batch error rate 0.000000%\n",
      "At minibatch 45800, batch loss 0.008942, batch error rate 0.000000%\n",
      "At minibatch 45900, batch loss 0.005814, batch error rate 0.000000%\n",
      "At minibatch 46000, batch loss 0.006051, batch error rate 0.000000%\n",
      "After epoch 92: valid_err_rate: 0.020800% currently going ot do 124 epochs\n",
      "At minibatch 46100, batch loss 0.003335, batch error rate 0.000000%\n",
      "At minibatch 46200, batch loss 0.010397, batch error rate 0.000000%\n",
      "At minibatch 46300, batch loss 0.003181, batch error rate 0.000000%\n",
      "At minibatch 46400, batch loss 0.002210, batch error rate 0.000000%\n",
      "At minibatch 46500, batch loss 0.002200, batch error rate 0.000000%\n",
      "After epoch 93: valid_err_rate: 0.021200% currently going ot do 124 epochs\n",
      "At minibatch 46600, batch loss 0.005434, batch error rate 0.000000%\n",
      "At minibatch 46700, batch loss 0.002231, batch error rate 0.000000%\n",
      "At minibatch 46800, batch loss 0.003412, batch error rate 0.000000%\n",
      "At minibatch 46900, batch loss 0.002082, batch error rate 0.000000%\n",
      "At minibatch 47000, batch loss 0.001760, batch error rate 0.000000%\n",
      "After epoch 94: valid_err_rate: 0.021000% currently going ot do 124 epochs\n",
      "At minibatch 47100, batch loss 0.002096, batch error rate 0.000000%\n",
      "At minibatch 47200, batch loss 0.004746, batch error rate 0.000000%\n",
      "At minibatch 47300, batch loss 0.005464, batch error rate 0.000000%\n",
      "At minibatch 47400, batch loss 0.003913, batch error rate 0.000000%\n",
      "At minibatch 47500, batch loss 0.007219, batch error rate 0.000000%\n",
      "After epoch 95: valid_err_rate: 0.021500% currently going ot do 124 epochs\n",
      "At minibatch 47600, batch loss 0.008794, batch error rate 0.000000%\n",
      "At minibatch 47700, batch loss 0.004646, batch error rate 0.000000%\n",
      "At minibatch 47800, batch loss 0.003258, batch error rate 0.000000%\n",
      "At minibatch 47900, batch loss 0.006826, batch error rate 0.000000%\n",
      "At minibatch 48000, batch loss 0.007644, batch error rate 0.000000%\n",
      "After epoch 96: valid_err_rate: 0.020900% currently going ot do 124 epochs\n",
      "At minibatch 48100, batch loss 0.003112, batch error rate 0.000000%\n",
      "At minibatch 48200, batch loss 0.003267, batch error rate 0.000000%\n",
      "At minibatch 48300, batch loss 0.005223, batch error rate 0.000000%\n",
      "At minibatch 48400, batch loss 0.003707, batch error rate 0.000000%\n",
      "At minibatch 48500, batch loss 0.006109, batch error rate 0.000000%\n",
      "After epoch 97: valid_err_rate: 0.021100% currently going ot do 124 epochs\n",
      "At minibatch 48600, batch loss 0.012339, batch error rate 0.000000%\n",
      "At minibatch 48700, batch loss 0.006379, batch error rate 0.000000%\n",
      "At minibatch 48800, batch loss 0.008574, batch error rate 0.000000%\n",
      "At minibatch 48900, batch loss 0.004582, batch error rate 0.000000%\n",
      "At minibatch 49000, batch loss 0.002499, batch error rate 0.000000%\n",
      "After epoch 98: valid_err_rate: 0.020700% currently going ot do 124 epochs\n",
      "At minibatch 49100, batch loss 0.005797, batch error rate 0.000000%\n",
      "At minibatch 49200, batch loss 0.006750, batch error rate 0.000000%\n",
      "At minibatch 49300, batch loss 0.005603, batch error rate 0.000000%\n",
      "At minibatch 49400, batch loss 0.004499, batch error rate 0.000000%\n",
      "At minibatch 49500, batch loss 0.002556, batch error rate 0.000000%\n",
      "After epoch 99: valid_err_rate: 0.021100% currently going ot do 124 epochs\n",
      "At minibatch 49600, batch loss 0.005160, batch error rate 0.000000%\n",
      "At minibatch 49700, batch loss 0.005322, batch error rate 0.000000%\n",
      "At minibatch 49800, batch loss 0.004315, batch error rate 0.000000%\n",
      "At minibatch 49900, batch loss 0.014303, batch error rate 1.000000%\n",
      "At minibatch 50000, batch loss 0.005313, batch error rate 0.000000%\n",
      "After epoch 100: valid_err_rate: 0.020700% currently going ot do 124 epochs\n",
      "At minibatch 50100, batch loss 0.004589, batch error rate 0.000000%\n",
      "At minibatch 50200, batch loss 0.004450, batch error rate 0.000000%\n",
      "At minibatch 50300, batch loss 0.006207, batch error rate 0.000000%\n",
      "At minibatch 50400, batch loss 0.032712, batch error rate 1.000000%\n",
      "At minibatch 50500, batch loss 0.004425, batch error rate 0.000000%\n",
      "After epoch 101: valid_err_rate: 0.020700% currently going ot do 124 epochs\n",
      "At minibatch 50600, batch loss 0.009659, batch error rate 0.000000%\n",
      "At minibatch 50700, batch loss 0.002830, batch error rate 0.000000%\n",
      "At minibatch 50800, batch loss 0.007966, batch error rate 0.000000%\n",
      "At minibatch 50900, batch loss 0.003297, batch error rate 0.000000%\n",
      "At minibatch 51000, batch loss 0.006273, batch error rate 0.000000%\n",
      "After epoch 102: valid_err_rate: 0.020800% currently going ot do 124 epochs\n",
      "At minibatch 51100, batch loss 0.004174, batch error rate 0.000000%\n",
      "At minibatch 51200, batch loss 0.003970, batch error rate 0.000000%\n",
      "At minibatch 51300, batch loss 0.007552, batch error rate 0.000000%\n",
      "At minibatch 51400, batch loss 0.006138, batch error rate 0.000000%\n",
      "At minibatch 51500, batch loss 0.005475, batch error rate 0.000000%\n",
      "After epoch 103: valid_err_rate: 0.021400% currently going ot do 124 epochs\n",
      "At minibatch 51600, batch loss 0.002259, batch error rate 0.000000%\n",
      "At minibatch 51700, batch loss 0.007273, batch error rate 0.000000%\n",
      "At minibatch 51800, batch loss 0.003005, batch error rate 0.000000%\n",
      "At minibatch 51900, batch loss 0.003862, batch error rate 0.000000%\n",
      "At minibatch 52000, batch loss 0.003791, batch error rate 0.000000%\n",
      "After epoch 104: valid_err_rate: 0.021000% currently going ot do 124 epochs\n",
      "At minibatch 52100, batch loss 0.006024, batch error rate 0.000000%\n",
      "At minibatch 52200, batch loss 0.007540, batch error rate 0.000000%\n",
      "At minibatch 52300, batch loss 0.004316, batch error rate 0.000000%\n",
      "At minibatch 52400, batch loss 0.001906, batch error rate 0.000000%\n",
      "At minibatch 52500, batch loss 0.012497, batch error rate 0.000000%\n",
      "After epoch 105: valid_err_rate: 0.021900% currently going ot do 124 epochs\n",
      "At minibatch 52600, batch loss 0.004787, batch error rate 0.000000%\n",
      "At minibatch 52700, batch loss 0.006131, batch error rate 0.000000%\n",
      "At minibatch 52800, batch loss 0.002466, batch error rate 0.000000%\n",
      "At minibatch 52900, batch loss 0.002673, batch error rate 0.000000%\n",
      "At minibatch 53000, batch loss 0.006246, batch error rate 0.000000%\n",
      "After epoch 106: valid_err_rate: 0.021200% currently going ot do 124 epochs\n",
      "At minibatch 53100, batch loss 0.010567, batch error rate 1.000000%\n",
      "At minibatch 53200, batch loss 0.009914, batch error rate 0.000000%\n",
      "At minibatch 53300, batch loss 0.002008, batch error rate 0.000000%\n",
      "At minibatch 53400, batch loss 0.006299, batch error rate 0.000000%\n",
      "At minibatch 53500, batch loss 0.004446, batch error rate 0.000000%\n",
      "After epoch 107: valid_err_rate: 0.020000% currently going ot do 161 epochs\n",
      "At minibatch 53600, batch loss 0.006346, batch error rate 0.000000%\n",
      "At minibatch 53700, batch loss 0.007568, batch error rate 0.000000%\n",
      "At minibatch 53800, batch loss 0.003884, batch error rate 0.000000%\n",
      "At minibatch 53900, batch loss 0.002157, batch error rate 0.000000%\n",
      "At minibatch 54000, batch loss 0.003225, batch error rate 0.000000%\n",
      "After epoch 108: valid_err_rate: 0.021300% currently going ot do 161 epochs\n",
      "At minibatch 54100, batch loss 0.008057, batch error rate 0.000000%\n",
      "At minibatch 54200, batch loss 0.002114, batch error rate 0.000000%\n",
      "At minibatch 54300, batch loss 0.007866, batch error rate 0.000000%\n",
      "At minibatch 54400, batch loss 0.004297, batch error rate 0.000000%\n",
      "At minibatch 54500, batch loss 0.002611, batch error rate 0.000000%\n",
      "After epoch 109: valid_err_rate: 0.020100% currently going ot do 161 epochs\n",
      "At minibatch 54600, batch loss 0.003600, batch error rate 0.000000%\n",
      "At minibatch 54700, batch loss 0.003247, batch error rate 0.000000%\n",
      "At minibatch 54800, batch loss 0.005221, batch error rate 0.000000%\n",
      "At minibatch 54900, batch loss 0.002580, batch error rate 0.000000%\n",
      "At minibatch 55000, batch loss 0.006210, batch error rate 0.000000%\n",
      "After epoch 110: valid_err_rate: 0.020400% currently going ot do 161 epochs\n",
      "At minibatch 55100, batch loss 0.003431, batch error rate 0.000000%\n",
      "At minibatch 55200, batch loss 0.006058, batch error rate 0.000000%\n",
      "At minibatch 55300, batch loss 0.001971, batch error rate 0.000000%\n",
      "At minibatch 55400, batch loss 0.002336, batch error rate 0.000000%\n",
      "At minibatch 55500, batch loss 0.003005, batch error rate 0.000000%\n",
      "After epoch 111: valid_err_rate: 0.020800% currently going ot do 161 epochs\n",
      "At minibatch 55600, batch loss 0.001727, batch error rate 0.000000%\n",
      "At minibatch 55700, batch loss 0.002479, batch error rate 0.000000%\n",
      "At minibatch 55800, batch loss 0.004309, batch error rate 0.000000%\n",
      "At minibatch 55900, batch loss 0.002226, batch error rate 0.000000%\n",
      "At minibatch 56000, batch loss 0.003694, batch error rate 0.000000%\n",
      "After epoch 112: valid_err_rate: 0.020500% currently going ot do 161 epochs\n",
      "At minibatch 56100, batch loss 0.004809, batch error rate 0.000000%\n",
      "At minibatch 56200, batch loss 0.004408, batch error rate 0.000000%\n",
      "At minibatch 56300, batch loss 0.003370, batch error rate 0.000000%\n",
      "At minibatch 56400, batch loss 0.004597, batch error rate 0.000000%\n",
      "At minibatch 56500, batch loss 0.003305, batch error rate 0.000000%\n",
      "After epoch 113: valid_err_rate: 0.021300% currently going ot do 161 epochs\n",
      "At minibatch 56600, batch loss 0.003054, batch error rate 0.000000%\n",
      "At minibatch 56700, batch loss 0.004031, batch error rate 0.000000%\n",
      "At minibatch 56800, batch loss 0.004489, batch error rate 0.000000%\n",
      "At minibatch 56900, batch loss 0.007037, batch error rate 0.000000%\n",
      "At minibatch 57000, batch loss 0.002628, batch error rate 0.000000%\n",
      "After epoch 114: valid_err_rate: 0.021200% currently going ot do 161 epochs\n",
      "At minibatch 57100, batch loss 0.004812, batch error rate 0.000000%\n",
      "At minibatch 57200, batch loss 0.006247, batch error rate 0.000000%\n",
      "At minibatch 57300, batch loss 0.003055, batch error rate 0.000000%\n",
      "At minibatch 57400, batch loss 0.005366, batch error rate 0.000000%\n",
      "At minibatch 57500, batch loss 0.005641, batch error rate 0.000000%\n",
      "After epoch 115: valid_err_rate: 0.020800% currently going ot do 161 epochs\n",
      "At minibatch 57600, batch loss 0.004024, batch error rate 0.000000%\n",
      "At minibatch 57700, batch loss 0.001721, batch error rate 0.000000%\n",
      "At minibatch 57800, batch loss 0.006341, batch error rate 0.000000%\n",
      "At minibatch 57900, batch loss 0.002704, batch error rate 0.000000%\n",
      "At minibatch 58000, batch loss 0.004565, batch error rate 0.000000%\n",
      "After epoch 116: valid_err_rate: 0.020800% currently going ot do 161 epochs\n",
      "At minibatch 58100, batch loss 0.003192, batch error rate 0.000000%\n",
      "At minibatch 58200, batch loss 0.003325, batch error rate 0.000000%\n",
      "At minibatch 58300, batch loss 0.002267, batch error rate 0.000000%\n",
      "At minibatch 58400, batch loss 0.002377, batch error rate 0.000000%\n",
      "At minibatch 58500, batch loss 0.004601, batch error rate 0.000000%\n",
      "After epoch 117: valid_err_rate: 0.020200% currently going ot do 161 epochs\n",
      "At minibatch 58600, batch loss 0.003491, batch error rate 0.000000%\n",
      "At minibatch 58700, batch loss 0.003257, batch error rate 0.000000%\n",
      "At minibatch 58800, batch loss 0.008894, batch error rate 0.000000%\n",
      "At minibatch 58900, batch loss 0.003285, batch error rate 0.000000%\n",
      "At minibatch 59000, batch loss 0.003563, batch error rate 0.000000%\n",
      "After epoch 118: valid_err_rate: 0.021400% currently going ot do 161 epochs\n",
      "At minibatch 59100, batch loss 0.002249, batch error rate 0.000000%\n",
      "At minibatch 59200, batch loss 0.001456, batch error rate 0.000000%\n",
      "At minibatch 59300, batch loss 0.002425, batch error rate 0.000000%\n",
      "At minibatch 59400, batch loss 0.005835, batch error rate 0.000000%\n",
      "At minibatch 59500, batch loss 0.002878, batch error rate 0.000000%\n",
      "After epoch 119: valid_err_rate: 0.021000% currently going ot do 161 epochs\n",
      "At minibatch 59600, batch loss 0.004079, batch error rate 0.000000%\n",
      "At minibatch 59700, batch loss 0.009252, batch error rate 0.000000%\n",
      "At minibatch 59800, batch loss 0.002777, batch error rate 0.000000%\n",
      "At minibatch 59900, batch loss 0.005281, batch error rate 0.000000%\n",
      "At minibatch 60000, batch loss 0.002491, batch error rate 0.000000%\n",
      "After epoch 120: valid_err_rate: 0.020800% currently going ot do 161 epochs\n",
      "At minibatch 60100, batch loss 0.004253, batch error rate 0.000000%\n",
      "At minibatch 60200, batch loss 0.003303, batch error rate 0.000000%\n",
      "At minibatch 60300, batch loss 0.004155, batch error rate 0.000000%\n",
      "At minibatch 60400, batch loss 0.003804, batch error rate 0.000000%\n",
      "At minibatch 60500, batch loss 0.002863, batch error rate 0.000000%\n",
      "After epoch 121: valid_err_rate: 0.020900% currently going ot do 161 epochs\n",
      "At minibatch 60600, batch loss 0.011192, batch error rate 0.000000%\n",
      "At minibatch 60700, batch loss 0.002246, batch error rate 0.000000%\n",
      "At minibatch 60800, batch loss 0.007638, batch error rate 0.000000%\n",
      "At minibatch 60900, batch loss 0.002379, batch error rate 0.000000%\n",
      "At minibatch 61000, batch loss 0.009794, batch error rate 0.000000%\n",
      "After epoch 122: valid_err_rate: 0.021400% currently going ot do 161 epochs\n",
      "At minibatch 61100, batch loss 0.006728, batch error rate 0.000000%\n",
      "At minibatch 61200, batch loss 0.002217, batch error rate 0.000000%\n",
      "At minibatch 61300, batch loss 0.003865, batch error rate 0.000000%\n",
      "At minibatch 61400, batch loss 0.002042, batch error rate 0.000000%\n",
      "At minibatch 61500, batch loss 0.002908, batch error rate 0.000000%\n",
      "After epoch 123: valid_err_rate: 0.020900% currently going ot do 161 epochs\n",
      "At minibatch 61600, batch loss 0.001112, batch error rate 0.000000%\n",
      "At minibatch 61700, batch loss 0.001734, batch error rate 0.000000%\n",
      "At minibatch 61800, batch loss 0.001513, batch error rate 0.000000%\n",
      "At minibatch 61900, batch loss 0.003913, batch error rate 0.000000%\n",
      "At minibatch 62000, batch loss 0.004778, batch error rate 0.000000%\n",
      "After epoch 124: valid_err_rate: 0.020600% currently going ot do 161 epochs\n",
      "At minibatch 62100, batch loss 0.002823, batch error rate 0.000000%\n",
      "At minibatch 62200, batch loss 0.001860, batch error rate 0.000000%\n",
      "At minibatch 62300, batch loss 0.003004, batch error rate 0.000000%\n",
      "At minibatch 62400, batch loss 0.002381, batch error rate 0.000000%\n",
      "At minibatch 62500, batch loss 0.005720, batch error rate 0.000000%\n",
      "After epoch 125: valid_err_rate: 0.020300% currently going ot do 161 epochs\n",
      "At minibatch 62600, batch loss 0.001456, batch error rate 0.000000%\n",
      "At minibatch 62700, batch loss 0.001245, batch error rate 0.000000%\n",
      "At minibatch 62800, batch loss 0.002314, batch error rate 0.000000%\n",
      "At minibatch 62900, batch loss 0.002224, batch error rate 0.000000%\n",
      "At minibatch 63000, batch loss 0.005028, batch error rate 0.000000%\n",
      "After epoch 126: valid_err_rate: 0.020700% currently going ot do 161 epochs\n",
      "At minibatch 63100, batch loss 0.003400, batch error rate 0.000000%\n",
      "At minibatch 63200, batch loss 0.002204, batch error rate 0.000000%\n",
      "At minibatch 63300, batch loss 0.001210, batch error rate 0.000000%\n",
      "At minibatch 63400, batch loss 0.001829, batch error rate 0.000000%\n",
      "At minibatch 63500, batch loss 0.002771, batch error rate 0.000000%\n",
      "After epoch 127: valid_err_rate: 0.020200% currently going ot do 161 epochs\n",
      "At minibatch 63600, batch loss 0.002617, batch error rate 0.000000%\n",
      "At minibatch 63700, batch loss 0.003304, batch error rate 0.000000%\n",
      "At minibatch 63800, batch loss 0.002659, batch error rate 0.000000%\n",
      "At minibatch 63900, batch loss 0.003825, batch error rate 0.000000%\n",
      "At minibatch 64000, batch loss 0.002446, batch error rate 0.000000%\n",
      "After epoch 128: valid_err_rate: 0.021000% currently going ot do 161 epochs\n",
      "At minibatch 64100, batch loss 0.001640, batch error rate 0.000000%\n",
      "At minibatch 64200, batch loss 0.001642, batch error rate 0.000000%\n",
      "At minibatch 64300, batch loss 0.001836, batch error rate 0.000000%\n",
      "At minibatch 64400, batch loss 0.004180, batch error rate 0.000000%\n",
      "At minibatch 64500, batch loss 0.002557, batch error rate 0.000000%\n",
      "After epoch 129: valid_err_rate: 0.020300% currently going ot do 161 epochs\n",
      "At minibatch 64600, batch loss 0.002787, batch error rate 0.000000%\n",
      "At minibatch 64700, batch loss 0.003078, batch error rate 0.000000%\n",
      "At minibatch 64800, batch loss 0.001679, batch error rate 0.000000%\n",
      "At minibatch 64900, batch loss 0.005429, batch error rate 0.000000%\n",
      "At minibatch 65000, batch loss 0.004664, batch error rate 0.000000%\n",
      "After epoch 130: valid_err_rate: 0.020700% currently going ot do 161 epochs\n",
      "At minibatch 65100, batch loss 0.001905, batch error rate 0.000000%\n",
      "At minibatch 65200, batch loss 0.003057, batch error rate 0.000000%\n",
      "At minibatch 65300, batch loss 0.001002, batch error rate 0.000000%\n",
      "At minibatch 65400, batch loss 0.005859, batch error rate 0.000000%\n",
      "At minibatch 65500, batch loss 0.005088, batch error rate 0.000000%\n",
      "After epoch 131: valid_err_rate: 0.020900% currently going ot do 161 epochs\n",
      "At minibatch 65600, batch loss 0.002269, batch error rate 0.000000%\n",
      "At minibatch 65700, batch loss 0.001733, batch error rate 0.000000%\n",
      "At minibatch 65800, batch loss 0.001574, batch error rate 0.000000%\n",
      "At minibatch 65900, batch loss 0.001171, batch error rate 0.000000%\n",
      "At minibatch 66000, batch loss 0.002510, batch error rate 0.000000%\n",
      "After epoch 132: valid_err_rate: 0.020000% currently going ot do 161 epochs\n",
      "At minibatch 66100, batch loss 0.001602, batch error rate 0.000000%\n",
      "At minibatch 66200, batch loss 0.002976, batch error rate 0.000000%\n",
      "At minibatch 66300, batch loss 0.001939, batch error rate 0.000000%\n",
      "At minibatch 66400, batch loss 0.003540, batch error rate 0.000000%\n",
      "At minibatch 66500, batch loss 0.003503, batch error rate 0.000000%\n",
      "After epoch 133: valid_err_rate: 0.020900% currently going ot do 161 epochs\n",
      "At minibatch 66600, batch loss 0.004048, batch error rate 0.000000%\n",
      "At minibatch 66700, batch loss 0.002055, batch error rate 0.000000%\n",
      "At minibatch 66800, batch loss 0.004464, batch error rate 0.000000%\n",
      "At minibatch 66900, batch loss 0.001555, batch error rate 0.000000%\n",
      "At minibatch 67000, batch loss 0.003394, batch error rate 0.000000%\n",
      "After epoch 134: valid_err_rate: 0.020600% currently going ot do 161 epochs\n",
      "At minibatch 67100, batch loss 0.006077, batch error rate 0.000000%\n",
      "At minibatch 67200, batch loss 0.001492, batch error rate 0.000000%\n",
      "At minibatch 67300, batch loss 0.003168, batch error rate 0.000000%\n",
      "At minibatch 67400, batch loss 0.003894, batch error rate 0.000000%\n",
      "At minibatch 67500, batch loss 0.004248, batch error rate 0.000000%\n",
      "After epoch 135: valid_err_rate: 0.020000% currently going ot do 161 epochs\n",
      "At minibatch 67600, batch loss 0.002373, batch error rate 0.000000%\n",
      "At minibatch 67700, batch loss 0.002642, batch error rate 0.000000%\n",
      "At minibatch 67800, batch loss 0.002879, batch error rate 0.000000%\n",
      "At minibatch 67900, batch loss 0.004067, batch error rate 0.000000%\n",
      "At minibatch 68000, batch loss 0.001457, batch error rate 0.000000%\n",
      "After epoch 136: valid_err_rate: 0.020500% currently going ot do 161 epochs\n",
      "At minibatch 68100, batch loss 0.002766, batch error rate 0.000000%\n",
      "At minibatch 68200, batch loss 0.002633, batch error rate 0.000000%\n",
      "At minibatch 68300, batch loss 0.001397, batch error rate 0.000000%\n",
      "At minibatch 68400, batch loss 0.004792, batch error rate 0.000000%\n",
      "At minibatch 68500, batch loss 0.003147, batch error rate 0.000000%\n",
      "After epoch 137: valid_err_rate: 0.020600% currently going ot do 161 epochs\n",
      "At minibatch 68600, batch loss 0.003278, batch error rate 0.000000%\n",
      "At minibatch 68700, batch loss 0.003318, batch error rate 0.000000%\n",
      "At minibatch 68800, batch loss 0.003358, batch error rate 0.000000%\n",
      "At minibatch 68900, batch loss 0.002705, batch error rate 0.000000%\n",
      "At minibatch 69000, batch loss 0.002440, batch error rate 0.000000%\n",
      "After epoch 138: valid_err_rate: 0.020900% currently going ot do 161 epochs\n",
      "At minibatch 69100, batch loss 0.001609, batch error rate 0.000000%\n",
      "At minibatch 69200, batch loss 0.003595, batch error rate 0.000000%\n",
      "At minibatch 69300, batch loss 0.001873, batch error rate 0.000000%\n",
      "At minibatch 69400, batch loss 0.004143, batch error rate 0.000000%\n",
      "At minibatch 69500, batch loss 0.002349, batch error rate 0.000000%\n",
      "After epoch 139: valid_err_rate: 0.020600% currently going ot do 161 epochs\n",
      "At minibatch 69600, batch loss 0.002731, batch error rate 0.000000%\n",
      "At minibatch 69700, batch loss 0.002614, batch error rate 0.000000%\n",
      "At minibatch 69800, batch loss 0.006992, batch error rate 0.000000%\n",
      "At minibatch 69900, batch loss 0.001289, batch error rate 0.000000%\n",
      "At minibatch 70000, batch loss 0.002076, batch error rate 0.000000%\n",
      "After epoch 140: valid_err_rate: 0.021000% currently going ot do 161 epochs\n",
      "At minibatch 70100, batch loss 0.000895, batch error rate 0.000000%\n",
      "At minibatch 70200, batch loss 0.002118, batch error rate 0.000000%\n",
      "At minibatch 70300, batch loss 0.001503, batch error rate 0.000000%\n",
      "At minibatch 70400, batch loss 0.003802, batch error rate 0.000000%\n",
      "At minibatch 70500, batch loss 0.001230, batch error rate 0.000000%\n",
      "After epoch 141: valid_err_rate: 0.019900% currently going ot do 212 epochs\n",
      "At minibatch 70600, batch loss 0.002002, batch error rate 0.000000%\n",
      "At minibatch 70700, batch loss 0.065826, batch error rate 1.000000%\n",
      "At minibatch 70800, batch loss 0.001302, batch error rate 0.000000%\n",
      "At minibatch 70900, batch loss 0.001139, batch error rate 0.000000%\n",
      "At minibatch 71000, batch loss 0.001956, batch error rate 0.000000%\n",
      "After epoch 142: valid_err_rate: 0.019800% currently going ot do 214 epochs\n",
      "At minibatch 71100, batch loss 0.003503, batch error rate 0.000000%\n",
      "At minibatch 71200, batch loss 0.001585, batch error rate 0.000000%\n",
      "At minibatch 71300, batch loss 0.004826, batch error rate 0.000000%\n",
      "At minibatch 71400, batch loss 0.004895, batch error rate 0.000000%\n",
      "At minibatch 71500, batch loss 0.002662, batch error rate 0.000000%\n",
      "After epoch 143: valid_err_rate: 0.020700% currently going ot do 214 epochs\n",
      "At minibatch 71600, batch loss 0.001363, batch error rate 0.000000%\n",
      "At minibatch 71700, batch loss 0.003468, batch error rate 0.000000%\n",
      "At minibatch 71800, batch loss 0.003773, batch error rate 0.000000%\n",
      "At minibatch 71900, batch loss 0.003722, batch error rate 0.000000%\n",
      "At minibatch 72000, batch loss 0.002982, batch error rate 0.000000%\n",
      "After epoch 144: valid_err_rate: 0.020800% currently going ot do 214 epochs\n",
      "At minibatch 72100, batch loss 0.001104, batch error rate 0.000000%\n",
      "At minibatch 72200, batch loss 0.001095, batch error rate 0.000000%\n",
      "At minibatch 72300, batch loss 0.003091, batch error rate 0.000000%\n",
      "At minibatch 72400, batch loss 0.003839, batch error rate 0.000000%\n",
      "At minibatch 72500, batch loss 0.001572, batch error rate 0.000000%\n",
      "After epoch 145: valid_err_rate: 0.020400% currently going ot do 214 epochs\n",
      "At minibatch 72600, batch loss 0.002996, batch error rate 0.000000%\n",
      "At minibatch 72700, batch loss 0.002019, batch error rate 0.000000%\n",
      "At minibatch 72800, batch loss 0.006179, batch error rate 0.000000%\n",
      "At minibatch 72900, batch loss 0.002560, batch error rate 0.000000%\n",
      "At minibatch 73000, batch loss 0.003830, batch error rate 0.000000%\n",
      "After epoch 146: valid_err_rate: 0.020600% currently going ot do 214 epochs\n",
      "At minibatch 73100, batch loss 0.001399, batch error rate 0.000000%\n",
      "At minibatch 73200, batch loss 0.002896, batch error rate 0.000000%\n",
      "At minibatch 73300, batch loss 0.001610, batch error rate 0.000000%\n",
      "At minibatch 73400, batch loss 0.002860, batch error rate 0.000000%\n",
      "At minibatch 73500, batch loss 0.002746, batch error rate 0.000000%\n",
      "After epoch 147: valid_err_rate: 0.020400% currently going ot do 214 epochs\n",
      "At minibatch 73600, batch loss 0.001901, batch error rate 0.000000%\n",
      "At minibatch 73700, batch loss 0.002107, batch error rate 0.000000%\n",
      "At minibatch 73800, batch loss 0.002882, batch error rate 0.000000%\n",
      "At minibatch 73900, batch loss 0.000982, batch error rate 0.000000%\n",
      "At minibatch 74000, batch loss 0.002261, batch error rate 0.000000%\n",
      "After epoch 148: valid_err_rate: 0.020700% currently going ot do 214 epochs\n",
      "At minibatch 74100, batch loss 0.001146, batch error rate 0.000000%\n",
      "At minibatch 74200, batch loss 0.002018, batch error rate 0.000000%\n",
      "At minibatch 74300, batch loss 0.002248, batch error rate 0.000000%\n",
      "At minibatch 74400, batch loss 0.000910, batch error rate 0.000000%\n",
      "At minibatch 74500, batch loss 0.002033, batch error rate 0.000000%\n",
      "After epoch 149: valid_err_rate: 0.020300% currently going ot do 214 epochs\n",
      "At minibatch 74600, batch loss 0.001201, batch error rate 0.000000%\n",
      "At minibatch 74700, batch loss 0.004147, batch error rate 0.000000%\n",
      "At minibatch 74800, batch loss 0.001379, batch error rate 0.000000%\n",
      "At minibatch 74900, batch loss 0.002760, batch error rate 0.000000%\n",
      "At minibatch 75000, batch loss 0.002041, batch error rate 0.000000%\n",
      "After epoch 150: valid_err_rate: 0.020500% currently going ot do 214 epochs\n",
      "At minibatch 75100, batch loss 0.003210, batch error rate 0.000000%\n",
      "At minibatch 75200, batch loss 0.002722, batch error rate 0.000000%\n",
      "At minibatch 75300, batch loss 0.003464, batch error rate 0.000000%\n",
      "At minibatch 75400, batch loss 0.002835, batch error rate 0.000000%\n",
      "At minibatch 75500, batch loss 0.003341, batch error rate 0.000000%\n",
      "After epoch 151: valid_err_rate: 0.020800% currently going ot do 214 epochs\n",
      "At minibatch 75600, batch loss 0.002918, batch error rate 0.000000%\n",
      "At minibatch 75700, batch loss 0.001628, batch error rate 0.000000%\n",
      "At minibatch 75800, batch loss 0.002229, batch error rate 0.000000%\n",
      "At minibatch 75900, batch loss 0.001684, batch error rate 0.000000%\n",
      "At minibatch 76000, batch loss 0.002125, batch error rate 0.000000%\n",
      "After epoch 152: valid_err_rate: 0.021100% currently going ot do 214 epochs\n",
      "At minibatch 76100, batch loss 0.002812, batch error rate 0.000000%\n",
      "At minibatch 76200, batch loss 0.002598, batch error rate 0.000000%\n",
      "At minibatch 76300, batch loss 0.001707, batch error rate 0.000000%\n",
      "At minibatch 76400, batch loss 0.001649, batch error rate 0.000000%\n",
      "At minibatch 76500, batch loss 0.001918, batch error rate 0.000000%\n",
      "After epoch 153: valid_err_rate: 0.020500% currently going ot do 214 epochs\n",
      "At minibatch 76600, batch loss 0.002316, batch error rate 0.000000%\n",
      "At minibatch 76700, batch loss 0.002717, batch error rate 0.000000%\n",
      "At minibatch 76800, batch loss 0.001974, batch error rate 0.000000%\n",
      "At minibatch 76900, batch loss 0.002281, batch error rate 0.000000%\n",
      "At minibatch 77000, batch loss 0.001781, batch error rate 0.000000%\n",
      "After epoch 154: valid_err_rate: 0.021000% currently going ot do 214 epochs\n",
      "At minibatch 77100, batch loss 0.002558, batch error rate 0.000000%\n",
      "At minibatch 77200, batch loss 0.001333, batch error rate 0.000000%\n",
      "At minibatch 77300, batch loss 0.001566, batch error rate 0.000000%\n",
      "At minibatch 77400, batch loss 0.001786, batch error rate 0.000000%\n",
      "At minibatch 77500, batch loss 0.001974, batch error rate 0.000000%\n",
      "After epoch 155: valid_err_rate: 0.020400% currently going ot do 214 epochs\n",
      "At minibatch 77600, batch loss 0.002015, batch error rate 0.000000%\n",
      "At minibatch 77700, batch loss 0.001950, batch error rate 0.000000%\n",
      "At minibatch 77800, batch loss 0.000894, batch error rate 0.000000%\n",
      "At minibatch 77900, batch loss 0.003708, batch error rate 0.000000%\n",
      "At minibatch 78000, batch loss 0.002416, batch error rate 0.000000%\n",
      "After epoch 156: valid_err_rate: 0.020500% currently going ot do 214 epochs\n",
      "At minibatch 78100, batch loss 0.002763, batch error rate 0.000000%\n",
      "At minibatch 78200, batch loss 0.002003, batch error rate 0.000000%\n",
      "At minibatch 78300, batch loss 0.002989, batch error rate 0.000000%\n",
      "At minibatch 78400, batch loss 0.001982, batch error rate 0.000000%\n",
      "At minibatch 78500, batch loss 0.004240, batch error rate 0.000000%\n",
      "After epoch 157: valid_err_rate: 0.021000% currently going ot do 214 epochs\n",
      "At minibatch 78600, batch loss 0.001620, batch error rate 0.000000%\n",
      "At minibatch 78700, batch loss 0.002346, batch error rate 0.000000%\n",
      "At minibatch 78800, batch loss 0.002389, batch error rate 0.000000%\n",
      "At minibatch 78900, batch loss 0.001999, batch error rate 0.000000%\n",
      "At minibatch 79000, batch loss 0.001129, batch error rate 0.000000%\n",
      "After epoch 158: valid_err_rate: 0.021200% currently going ot do 214 epochs\n",
      "At minibatch 79100, batch loss 0.001938, batch error rate 0.000000%\n",
      "At minibatch 79200, batch loss 0.001545, batch error rate 0.000000%\n",
      "At minibatch 79300, batch loss 0.002073, batch error rate 0.000000%\n",
      "At minibatch 79400, batch loss 0.001444, batch error rate 0.000000%\n",
      "At minibatch 79500, batch loss 0.000848, batch error rate 0.000000%\n",
      "After epoch 159: valid_err_rate: 0.020500% currently going ot do 214 epochs\n",
      "At minibatch 79600, batch loss 0.003084, batch error rate 0.000000%\n",
      "At minibatch 79700, batch loss 0.002040, batch error rate 0.000000%\n",
      "At minibatch 79800, batch loss 0.001278, batch error rate 0.000000%\n",
      "At minibatch 79900, batch loss 0.001138, batch error rate 0.000000%\n",
      "At minibatch 80000, batch loss 0.001588, batch error rate 0.000000%\n",
      "After epoch 160: valid_err_rate: 0.020600% currently going ot do 214 epochs\n",
      "At minibatch 80100, batch loss 0.003046, batch error rate 0.000000%\n",
      "At minibatch 80200, batch loss 0.002449, batch error rate 0.000000%\n",
      "At minibatch 80300, batch loss 0.001560, batch error rate 0.000000%\n",
      "At minibatch 80400, batch loss 0.003600, batch error rate 0.000000%\n",
      "At minibatch 80500, batch loss 0.000994, batch error rate 0.000000%\n",
      "After epoch 161: valid_err_rate: 0.021300% currently going ot do 214 epochs\n",
      "At minibatch 80600, batch loss 0.002325, batch error rate 0.000000%\n",
      "At minibatch 80700, batch loss 0.002120, batch error rate 0.000000%\n",
      "At minibatch 80800, batch loss 0.003162, batch error rate 0.000000%\n",
      "At minibatch 80900, batch loss 0.001442, batch error rate 0.000000%\n",
      "At minibatch 81000, batch loss 0.002779, batch error rate 0.000000%\n",
      "After epoch 162: valid_err_rate: 0.020500% currently going ot do 214 epochs\n",
      "At minibatch 81100, batch loss 0.004796, batch error rate 0.000000%\n",
      "At minibatch 81200, batch loss 0.001743, batch error rate 0.000000%\n",
      "At minibatch 81300, batch loss 0.002103, batch error rate 0.000000%\n",
      "At minibatch 81400, batch loss 0.001823, batch error rate 0.000000%\n",
      "At minibatch 81500, batch loss 0.001055, batch error rate 0.000000%\n",
      "After epoch 163: valid_err_rate: 0.020700% currently going ot do 214 epochs\n",
      "At minibatch 81600, batch loss 0.000901, batch error rate 0.000000%\n",
      "At minibatch 81700, batch loss 0.003851, batch error rate 0.000000%\n",
      "At minibatch 81800, batch loss 0.001881, batch error rate 0.000000%\n",
      "At minibatch 81900, batch loss 0.001005, batch error rate 0.000000%\n",
      "Setting network parameters from after epoch 142\n",
      "Test error rate: 0.022300\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f99ee138490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# TODO: pick a network architecture here. The one below is just \n",
    "# softmax regression\n",
    "#\n",
    "\n",
    "net = FeedForwardNet([\n",
    "        AffineLayer(784,100),\n",
    "        ReLULayer(),\n",
    "        TanhLayer(100,20),\n",
    "        TanhLayer(),\n",
    "        AffineLayer(20,10),\n",
    "        SoftMaxLayer()\n",
    "        ])\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream, 0.0001)\n",
    "\n",
    "print \"Test error rate: %f\" % (compute_error_rate(net, mnist_test_stream), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0223"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "compute_error_rate(net, mnist_test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
